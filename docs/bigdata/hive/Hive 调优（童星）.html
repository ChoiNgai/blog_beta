<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Hive 调优 | 大数据技术文档</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="./favicon.ico">
    <meta name="description" content="从入门到入土">
    
    <link rel="preload" href="./assets/css/0.styles.8b017a1a.css" as="style"><link rel="preload" href="./assets/js/app.dbe5dc78.js" as="script"><link rel="preload" href="./assets/js/2.fa5f1a4a.js" as="script"><link rel="preload" href="./assets/js/19.b1f41278.js" as="script"><link rel="prefetch" href="./assets/js/10.af7e6d14.js"><link rel="prefetch" href="./assets/js/100.f98d6b7c.js"><link rel="prefetch" href="./assets/js/101.3ebb2df2.js"><link rel="prefetch" href="./assets/js/102.bddf26fd.js"><link rel="prefetch" href="./assets/js/103.3c13d131.js"><link rel="prefetch" href="./assets/js/104.76e7b64f.js"><link rel="prefetch" href="./assets/js/105.b8907b13.js"><link rel="prefetch" href="./assets/js/106.26f679d0.js"><link rel="prefetch" href="./assets/js/107.7670ff83.js"><link rel="prefetch" href="./assets/js/108.c620cb38.js"><link rel="prefetch" href="./assets/js/109.394140cc.js"><link rel="prefetch" href="./assets/js/11.99d58157.js"><link rel="prefetch" href="./assets/js/110.8a03d1ab.js"><link rel="prefetch" href="./assets/js/111.709e74df.js"><link rel="prefetch" href="./assets/js/112.e7c7c661.js"><link rel="prefetch" href="./assets/js/113.b9c7ba9e.js"><link rel="prefetch" href="./assets/js/114.45aede8c.js"><link rel="prefetch" href="./assets/js/115.1ad69003.js"><link rel="prefetch" href="./assets/js/116.0f4944de.js"><link rel="prefetch" href="./assets/js/117.338b1f46.js"><link rel="prefetch" href="./assets/js/118.56fa4871.js"><link rel="prefetch" href="./assets/js/119.1374f6e2.js"><link rel="prefetch" href="./assets/js/12.2e190b00.js"><link rel="prefetch" href="./assets/js/120.7e3bde42.js"><link rel="prefetch" href="./assets/js/121.bf4c105b.js"><link rel="prefetch" href="./assets/js/122.9678e246.js"><link rel="prefetch" href="./assets/js/123.4f9c51b8.js"><link rel="prefetch" href="./assets/js/124.6ed32d31.js"><link rel="prefetch" href="./assets/js/125.c1c1f240.js"><link rel="prefetch" href="./assets/js/126.f7dae67d.js"><link rel="prefetch" href="./assets/js/127.aa5de251.js"><link rel="prefetch" href="./assets/js/128.7a097fcd.js"><link rel="prefetch" href="./assets/js/129.62b823c3.js"><link rel="prefetch" href="./assets/js/13.5d4714fa.js"><link rel="prefetch" href="./assets/js/130.04125268.js"><link rel="prefetch" href="./assets/js/131.f93632b5.js"><link rel="prefetch" href="./assets/js/132.8efcb3ab.js"><link rel="prefetch" href="./assets/js/133.045ade7e.js"><link rel="prefetch" href="./assets/js/134.82af04a6.js"><link rel="prefetch" href="./assets/js/135.06de65c0.js"><link rel="prefetch" href="./assets/js/136.3fc5ce45.js"><link rel="prefetch" href="./assets/js/137.ca4e288e.js"><link rel="prefetch" href="./assets/js/138.f8e9409a.js"><link rel="prefetch" href="./assets/js/139.51cc74b3.js"><link rel="prefetch" href="./assets/js/14.cd95effa.js"><link rel="prefetch" href="./assets/js/15.d7a053dc.js"><link rel="prefetch" href="./assets/js/16.241383de.js"><link rel="prefetch" href="./assets/js/17.cdf37f90.js"><link rel="prefetch" href="./assets/js/18.54a8e3ff.js"><link rel="prefetch" href="./assets/js/20.00809aa4.js"><link rel="prefetch" href="./assets/js/21.ed4af20e.js"><link rel="prefetch" href="./assets/js/22.d70e789f.js"><link rel="prefetch" href="./assets/js/23.4bd4acfc.js"><link rel="prefetch" href="./assets/js/24.b68c0ece.js"><link rel="prefetch" href="./assets/js/25.56ed8a43.js"><link rel="prefetch" href="./assets/js/26.6bcb257f.js"><link rel="prefetch" href="./assets/js/27.c00845f2.js"><link rel="prefetch" href="./assets/js/28.6badc057.js"><link rel="prefetch" href="./assets/js/29.9230aae3.js"><link rel="prefetch" href="./assets/js/3.b52d03f5.js"><link rel="prefetch" href="./assets/js/30.24beb61e.js"><link rel="prefetch" href="./assets/js/31.f2a94727.js"><link rel="prefetch" href="./assets/js/32.a3ba2747.js"><link rel="prefetch" href="./assets/js/33.c524bae4.js"><link rel="prefetch" href="./assets/js/34.163f85b3.js"><link rel="prefetch" href="./assets/js/35.c42df8de.js"><link rel="prefetch" href="./assets/js/36.03d4748a.js"><link rel="prefetch" href="./assets/js/37.07cd39b4.js"><link rel="prefetch" href="./assets/js/38.18db04a1.js"><link rel="prefetch" href="./assets/js/39.c1633a4e.js"><link rel="prefetch" href="./assets/js/4.1d0a3544.js"><link rel="prefetch" href="./assets/js/40.a49e9f5c.js"><link rel="prefetch" href="./assets/js/41.0490672b.js"><link rel="prefetch" href="./assets/js/42.03ff0a41.js"><link rel="prefetch" href="./assets/js/43.7567d202.js"><link rel="prefetch" href="./assets/js/44.e36ca589.js"><link rel="prefetch" href="./assets/js/45.5b0f064d.js"><link rel="prefetch" href="./assets/js/46.11d921c5.js"><link rel="prefetch" href="./assets/js/47.f961d914.js"><link rel="prefetch" href="./assets/js/48.ba14e041.js"><link rel="prefetch" href="./assets/js/49.566ddc43.js"><link rel="prefetch" href="./assets/js/5.cc194294.js"><link rel="prefetch" href="./assets/js/50.b70961ac.js"><link rel="prefetch" href="./assets/js/51.3c084f92.js"><link rel="prefetch" href="./assets/js/52.14662c12.js"><link rel="prefetch" href="./assets/js/53.c40aa86f.js"><link rel="prefetch" href="./assets/js/54.300e2058.js"><link rel="prefetch" href="./assets/js/55.3b06c210.js"><link rel="prefetch" href="./assets/js/56.227ea682.js"><link rel="prefetch" href="./assets/js/57.19a5d767.js"><link rel="prefetch" href="./assets/js/58.7fc8ce3f.js"><link rel="prefetch" href="./assets/js/59.9abc1a7f.js"><link rel="prefetch" href="./assets/js/6.a67b19a4.js"><link rel="prefetch" href="./assets/js/60.21e470b2.js"><link rel="prefetch" href="./assets/js/61.342c1882.js"><link rel="prefetch" href="./assets/js/62.b8d083e4.js"><link rel="prefetch" href="./assets/js/63.d98056bc.js"><link rel="prefetch" href="./assets/js/64.abb48239.js"><link rel="prefetch" href="./assets/js/65.06d2edcf.js"><link rel="prefetch" href="./assets/js/66.11f32f94.js"><link rel="prefetch" href="./assets/js/67.55fb49ed.js"><link rel="prefetch" href="./assets/js/68.6a9fb92c.js"><link rel="prefetch" href="./assets/js/69.ea206161.js"><link rel="prefetch" href="./assets/js/7.4db7fac4.js"><link rel="prefetch" href="./assets/js/70.19b21c67.js"><link rel="prefetch" href="./assets/js/71.6d30915f.js"><link rel="prefetch" href="./assets/js/72.c663480f.js"><link rel="prefetch" href="./assets/js/73.c64a8bc9.js"><link rel="prefetch" href="./assets/js/74.b4853538.js"><link rel="prefetch" href="./assets/js/75.ed723be5.js"><link rel="prefetch" href="./assets/js/76.bd2efea1.js"><link rel="prefetch" href="./assets/js/77.8e8632d7.js"><link rel="prefetch" href="./assets/js/78.69ef8849.js"><link rel="prefetch" href="./assets/js/79.9785ef8d.js"><link rel="prefetch" href="./assets/js/8.eace717d.js"><link rel="prefetch" href="./assets/js/80.5365bfc6.js"><link rel="prefetch" href="./assets/js/81.69b36a86.js"><link rel="prefetch" href="./assets/js/82.aad515c0.js"><link rel="prefetch" href="./assets/js/83.050ac97d.js"><link rel="prefetch" href="./assets/js/84.78bb8c43.js"><link rel="prefetch" href="./assets/js/85.feeffda7.js"><link rel="prefetch" href="./assets/js/86.f38af2cb.js"><link rel="prefetch" href="./assets/js/87.3c875b53.js"><link rel="prefetch" href="./assets/js/88.ccc36d1b.js"><link rel="prefetch" href="./assets/js/89.44e2573e.js"><link rel="prefetch" href="./assets/js/9.3a163afb.js"><link rel="prefetch" href="./assets/js/90.816482fc.js"><link rel="prefetch" href="./assets/js/91.9072f45b.js"><link rel="prefetch" href="./assets/js/92.858235e2.js"><link rel="prefetch" href="./assets/js/93.ef35de87.js"><link rel="prefetch" href="./assets/js/94.2701ba5d.js"><link rel="prefetch" href="./assets/js/95.666433f2.js"><link rel="prefetch" href="./assets/js/96.f2b93c96.js"><link rel="prefetch" href="./assets/js/97.cb6eb699.js"><link rel="prefetch" href="./assets/js/98.8f172db0.js"><link rel="prefetch" href="./assets/js/99.26797ef8.js">
    <link rel="stylesheet" href="./assets/css/0.styles.8b017a1a.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/./" class="home-link router-link-active"><!----> <span class="site-name">大数据技术文档</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/./" class="nav-link">
  首页
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="编程基础" class="dropdown-title"><span class="title">编程基础</span> <span class="arrow down"></span></button> <button type="button" aria-label="编程基础" class="mobile-dropdown-title"><span class="title">编程基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>
          Java基础
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/java基础语法/" class="nav-link">
  Java基础语法
</a></li><li class="dropdown-subitem"><a href="/./coding-base/" class="nav-link">
  Java基础实战
</a></li></ul></li><li class="dropdown-item"><h4>
          Java进阶(选学)
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/java并发编程/java并发编程.html" class="nav-link">
  Java并发编程
</a></li><li class="dropdown-subitem"><a href="/./coding-base/" class="nav-link">
  Java网络编程
</a></li><li class="dropdown-subitem"><a href="/./coding-base/java集合/Java集合（永盛）.html" class="nav-link">
  Java集合
</a></li><li class="dropdown-subitem"><a href="/./coding-base/java虚拟机/" class="nav-link">
  Java虚拟机
</a></li></ul></li><li class="dropdown-item"><h4>
          计算机基础
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/linux/" class="nav-link">
  Linux
</a></li><li class="dropdown-subitem"><a href="/./coding-base/数据结构与算法/" class="nav-link">
  数据结构（重要）
</a></li><li class="dropdown-subitem"><a href="/./coding-base/计算机网络/计算机网络（双祥）.html" class="nav-link">
  计算机网络
</a></li><li class="dropdown-subitem"><a href="/./coding-base/操作系统/" class="nav-link">
  操作系统
</a></li></ul></li><li class="dropdown-item"><h4>
          Python（选学）
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/Python/python基础/" class="nav-link">
  Python基础语法
</a></li><li class="dropdown-subitem"><a href="/./coding-base/Python/python库/" class="nav-link">
  Python数据科学库
</a></li></ul></li><li class="dropdown-item"><h4>
          框架（选学）
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/框架/sprin系列/" class="nav-link">
  Spring系列
</a></li><li class="dropdown-subitem"><a href="/./coding-base/框架/flask/falsk.html" class="nav-link">
  Flask
</a></li><li class="dropdown-subitem"><a href="/./coding-base/框架/vue/flask.html" class="nav-link">
  Vue
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据库" class="dropdown-title"><span class="title">数据库</span> <span class="arrow down"></span></button> <button type="button" aria-label="数据库" class="mobile-dropdown-title"><span class="title">数据库</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/./database/mysql/" class="nav-link">
  MySQL
</a></li><li class="dropdown-item"><!----> <a href="/./database/hbase/" class="nav-link">
  HBase
</a></li><li class="dropdown-item"><!----> <a href="/./database/tidb/" class="nav-link">
  TiDB
</a></li><li class="dropdown-item"><!----> <a href="/./database/clickhouse/" class="nav-link">
  ClickHouse
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据仓库" class="dropdown-title"><span class="title">数据仓库</span> <span class="arrow down"></span></button> <button type="button" aria-label="数据仓库" class="mobile-dropdown-title"><span class="title">数据仓库</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/./datahouse/sql/" class="nav-link">
  SQL
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/大数据基础/bigdata-base.html" class="nav-link">
  大数据基础
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/离线数仓/" class="nav-link">
  离线数仓
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/实时数仓/" class="nav-link">
  实时数仓
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/商业化技术/" class="nav-link">
  商业化技术
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/电商业务/" class="nav-link">
  电商业务
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="大数据框架及组件" class="dropdown-title"><span class="title">大数据框架及组件</span> <span class="arrow down"></span></button> <button type="button" aria-label="大数据框架及组件" class="mobile-dropdown-title"><span class="title">大数据框架及组件</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/./bigdata/hadoop/" class="nav-link">
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/hive/" class="nav-link router-link-active">
  Hive
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/zookeeper/" class="nav-link">
  Zookeeper
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/kafka/" class="nav-link">
  kafka
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/spark/" class="nav-link">
  Spark
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/flink/" class="nav-link">
  Flink
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="其他" class="dropdown-title"><span class="title">其他</span> <span class="arrow down"></span></button> <button type="button" aria-label="其他" class="mobile-dropdown-title"><span class="title">其他</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/./other/面经/" class="nav-link">
  面经
</a></li><li class="dropdown-item"><!----> <a href="/./other/机器学习/" class="nav-link">
  机器学习
</a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/./" class="nav-link">
  首页
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="编程基础" class="dropdown-title"><span class="title">编程基础</span> <span class="arrow down"></span></button> <button type="button" aria-label="编程基础" class="mobile-dropdown-title"><span class="title">编程基础</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>
          Java基础
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/java基础语法/" class="nav-link">
  Java基础语法
</a></li><li class="dropdown-subitem"><a href="/./coding-base/" class="nav-link">
  Java基础实战
</a></li></ul></li><li class="dropdown-item"><h4>
          Java进阶(选学)
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/java并发编程/java并发编程.html" class="nav-link">
  Java并发编程
</a></li><li class="dropdown-subitem"><a href="/./coding-base/" class="nav-link">
  Java网络编程
</a></li><li class="dropdown-subitem"><a href="/./coding-base/java集合/Java集合（永盛）.html" class="nav-link">
  Java集合
</a></li><li class="dropdown-subitem"><a href="/./coding-base/java虚拟机/" class="nav-link">
  Java虚拟机
</a></li></ul></li><li class="dropdown-item"><h4>
          计算机基础
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/linux/" class="nav-link">
  Linux
</a></li><li class="dropdown-subitem"><a href="/./coding-base/数据结构与算法/" class="nav-link">
  数据结构（重要）
</a></li><li class="dropdown-subitem"><a href="/./coding-base/计算机网络/计算机网络（双祥）.html" class="nav-link">
  计算机网络
</a></li><li class="dropdown-subitem"><a href="/./coding-base/操作系统/" class="nav-link">
  操作系统
</a></li></ul></li><li class="dropdown-item"><h4>
          Python（选学）
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/Python/python基础/" class="nav-link">
  Python基础语法
</a></li><li class="dropdown-subitem"><a href="/./coding-base/Python/python库/" class="nav-link">
  Python数据科学库
</a></li></ul></li><li class="dropdown-item"><h4>
          框架（选学）
        </h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/./coding-base/框架/sprin系列/" class="nav-link">
  Spring系列
</a></li><li class="dropdown-subitem"><a href="/./coding-base/框架/flask/falsk.html" class="nav-link">
  Flask
</a></li><li class="dropdown-subitem"><a href="/./coding-base/框架/vue/flask.html" class="nav-link">
  Vue
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据库" class="dropdown-title"><span class="title">数据库</span> <span class="arrow down"></span></button> <button type="button" aria-label="数据库" class="mobile-dropdown-title"><span class="title">数据库</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/./database/mysql/" class="nav-link">
  MySQL
</a></li><li class="dropdown-item"><!----> <a href="/./database/hbase/" class="nav-link">
  HBase
</a></li><li class="dropdown-item"><!----> <a href="/./database/tidb/" class="nav-link">
  TiDB
</a></li><li class="dropdown-item"><!----> <a href="/./database/clickhouse/" class="nav-link">
  ClickHouse
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="数据仓库" class="dropdown-title"><span class="title">数据仓库</span> <span class="arrow down"></span></button> <button type="button" aria-label="数据仓库" class="mobile-dropdown-title"><span class="title">数据仓库</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/./datahouse/sql/" class="nav-link">
  SQL
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/大数据基础/bigdata-base.html" class="nav-link">
  大数据基础
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/离线数仓/" class="nav-link">
  离线数仓
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/实时数仓/" class="nav-link">
  实时数仓
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/商业化技术/" class="nav-link">
  商业化技术
</a></li><li class="dropdown-item"><!----> <a href="/./datahouse/电商业务/" class="nav-link">
  电商业务
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="大数据框架及组件" class="dropdown-title"><span class="title">大数据框架及组件</span> <span class="arrow down"></span></button> <button type="button" aria-label="大数据框架及组件" class="mobile-dropdown-title"><span class="title">大数据框架及组件</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/./bigdata/hadoop/" class="nav-link">
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/hive/" class="nav-link router-link-active">
  Hive
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/zookeeper/" class="nav-link">
  Zookeeper
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/kafka/" class="nav-link">
  kafka
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/spark/" class="nav-link">
  Spark
</a></li><li class="dropdown-item"><!----> <a href="/./bigdata/flink/" class="nav-link">
  Flink
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="其他" class="dropdown-title"><span class="title">其他</span> <span class="arrow down"></span></button> <button type="button" aria-label="其他" class="mobile-dropdown-title"><span class="title">其他</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/./other/面经/" class="nav-link">
  面经
</a></li><li class="dropdown-item"><!----> <a href="/./other/机器学习/" class="nav-link">
  机器学习
</a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Hive</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/./bigdata/hive/" aria-current="page" class="sidebar-link">目录</a></li><li><a href="/./bigdata/hive/Hive全面讲解.html" class="sidebar-link">Hive全面讲解</a></li><li><a href="/./bigdata/hive/Hive 调优（童星）.html" class="active sidebar-link">Hive 调优</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/./bigdata/hive/Hive 调优（童星）.html#hive-调优" class="sidebar-link">Hive 调优</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/./bigdata/hive/Hive 调优（童星）.html#分区表" class="sidebar-link">分区表</a></li><li class="sidebar-sub-header"><a href="/./bigdata/hive/Hive 调优（童星）.html#分桶表" class="sidebar-link">分桶表</a></li><li class="sidebar-sub-header"><a href="/./bigdata/hive/Hive 调优（童星）.html#索引设计" class="sidebar-link">索引设计</a></li><li class="sidebar-sub-header"><a href="/./bigdata/hive/Hive 调优（童星）.html#hive表数据优化" class="sidebar-link">Hive表数据优化</a></li><li class="sidebar-sub-header"><a href="/./bigdata/hive/Hive 调优（童星）.html#存储优化" class="sidebar-link">存储优化</a></li><li class="sidebar-sub-header"><a href="/./bigdata/hive/Hive 调优（童星）.html#计算-job-执行优化" class="sidebar-link">计算 Job 执行优化</a></li><li class="sidebar-sub-header"><a href="/./bigdata/hive/Hive 调优（童星）.html#mapreduce-属性优化" class="sidebar-link">MapReduce 属性优化：</a></li><li class="sidebar-sub-header"><a href="/./bigdata/hive/Hive 调优（童星）.html#数据倾斜" class="sidebar-link">数据倾斜：</a></li></ul></li></ul></li><li><a href="/./bigdata/hive/Flink1.10和Hive集成一些需要注意的点.html" class="sidebar-link">Flink1.10和Hive集成一些需要注意的点</a></li><li><a href="/./bigdata/hive/Hive  4万字性能调优面试总结.html" class="sidebar-link">Hive | 4万字性能调优面试总结</a></li><li><a href="/./bigdata/hive/Hive - ORC 文件存储格式详细解析.html" class="sidebar-link">Hive - ORC 文件存储格式详细解析</a></li><li><a href="/./bigdata/hive/Hive SQL三个经典优化案例.html" class="sidebar-link">Hive SQL三个经典优化案例</a></li><li><a href="/./bigdata/hive/Hive SQL内置函数大全.html" class="sidebar-link">Hive SQL内置函数大全</a></li><li><a href="/./bigdata/hive/Hive SQL底层执行过程详细剖析.html" class="sidebar-link">Hive SQL底层执行过程详细剖析</a></li><li><a href="/./bigdata/hive/Hive 中的四种排序详解.html" class="sidebar-link">Hive 中的四种排序详解</a></li><li><a href="/./bigdata/hive/Hive 分析函数进阶指南.html" class="sidebar-link">Hive 分析函数进阶指南</a></li><li><a href="/./bigdata/hive/Hive 复杂数据类型.html" class="sidebar-link">Hive 复杂数据类型</a></li><li><a href="/./bigdata/hive/Hive 高频考点讲解.html" class="sidebar-link">Hive 高频考点讲解</a></li><li><a href="/./bigdata/hive/HiveHadoop高频面试点小集合.html" class="sidebar-link">Hive/Hadoop高频面试点小集合</a></li><li><a href="/./bigdata/hive/Hive中的count(distinct)优化.html" class="sidebar-link">Hive中的count(distinct)优化</a></li><li><a href="/./bigdata/hive/Hive优化-大表join大表优化.html" class="sidebar-link">Hive优化-大表join大表优化</a></li><li><a href="/./bigdata/hive/Hive使用必知必会系列.html" class="sidebar-link">Hive使用必知必会系列</a></li><li><a href="/./bigdata/hive/Hive和Hbase的各自适用场景.html" class="sidebar-link">Hive和Hbase的各自适用场景</a></li><li><a href="/./bigdata/hive/Hive小文件合并与数据压缩.html" class="sidebar-link">Hive小文件合并与数据压缩</a></li><li><a href="/./bigdata/hive/Hive小知识之分桶抽样.html" class="sidebar-link">Hive小知识之分桶抽样</a></li><li><a href="/./bigdata/hive/Hive常用参数调优十二式.html" class="sidebar-link">Hive常用参数调优十二式</a></li><li><a href="/./bigdata/hive/Hive常用的函数总结.html" class="sidebar-link">Hive常用的函数总结</a></li><li><a href="/./bigdata/hive/Hive底层原理：explain执行计划详解.html" class="sidebar-link">Hive底层原理：explain执行计划详解</a></li><li><a href="/./bigdata/hive/Hive性能调优  并行执行严格模式JVM重用推测执行.html" class="sidebar-link">Hive性能调优 | 并行执行/严格模式/JVM重用/推测执行</a></li><li><a href="/./bigdata/hive/Hive性能调优  数据倾斜.html" class="sidebar-link">Hive性能调优 | 数据倾斜</a></li><li><a href="/./bigdata/hive/Hive技术原理.html" class="sidebar-link">Hive技术原理</a></li><li><a href="/./bigdata/hive/Hive窗口函数总结与实践.html" class="sidebar-link">Hive窗口函数总结与实践</a></li><li><a href="/./bigdata/hive/Hive计算绝对值同环比.html" class="sidebar-link">Hive计算绝对值同环比</a></li><li><a href="/./bigdata/hive/Hive调优全方位指南.html" class="sidebar-link">Hive调优全方位指南</a></li><li><a href="/./bigdata/hive/Impala一文详解及与Hive简单对比.html" class="sidebar-link">Impala一文详解及与Hive简单对比</a></li><li><a href="/./bigdata/hive/一文搞定Hive函数.html" class="sidebar-link">一文搞定Hive函数</a></li><li><a href="/./bigdata/hive/一篇文章了解Hive调优.html" class="sidebar-link">一篇文章了解Hive调优</a></li><li><a href="/./bigdata/hive/基于Hadoop的数据仓库Hive基础知识.html" class="sidebar-link">基于Hadoop的数据仓库Hive基础知识</a></li><li><a href="/./bigdata/hive/快手数仓Hive MetaStore挑战与优化.html" class="sidebar-link">快手数仓Hive MetaStore挑战与优化</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="hive-调优"><a href="#hive-调优" class="header-anchor">#</a> Hive 调优</h2> <h3 id="分区表"><a href="#分区表" class="header-anchor">#</a> 分区表</h3> <h5 id="hive-查询基本原理"><a href="#hive-查询基本原理" class="header-anchor">#</a> Hive 查询基本原理：</h5> <blockquote><p>Hive的设计思想是通过元数据将HDFS上的文件映射成表，基本的查询原理是当用户通过HQL语句对Hive中的表进行复杂数据处理和计算时，默认将其转换为分布式计算MapReduce程序对HDFS中的数据进行读取处理的过程。</p></blockquote> <hr> <p>==普通表结构：==</p> <p>​	默认的普通表结构中，表的最后一级目录就是表的目录，而底层的计算会使用表的最后一级目录作为Input进行计算，这种场景下，我们就会遇到一个问题，如果表的数据很多，而我们需要被处理的数据很少，只是其中一小部分，这样就会导致大量不必要的数据被程序加载，在程序中被过滤，导致大量不必要的计算资源的浪费。</p> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer//img/image-20211120225439252.png" alt="image-20211120225439252" style="zoom:50%;"> <p>==分区表设计思想：==</p> <p>​	针对上面的问题，Hive提供了一种特殊的表结构来解决——分区表结构。分区表结构的设计思想是：根据查询的需求，将数据按照查询的条件<strong>一般都以时间</strong>进行划分分区存储，将不同分区的数据单独使用一个HDFS目录来进行存储，当底层实现计算时，根据查询的条件，只读取对应分区的数据作为输入，减少不必要的数据加载，提高程序的性能。</p> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer//img/image-20211120225608249.png" alt="image-20211120225608249" style="zoom:50%;"> <p>==代码测试==：</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">--创建表</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> tb_login_part<span class="token punctuation">(</span>
  userid string
<span class="token punctuation">)</span> 
partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span>logindate string<span class="token punctuation">)</span>
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>

<span class="token comment">--开启动态分区</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token operator">=</span>nonstrict<span class="token punctuation">;</span>
<span class="token comment">--按登录日期分区</span>
<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> tb_login_part <span class="token keyword">partition</span><span class="token punctuation">(</span>logindate<span class="token punctuation">)</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> tb_login<span class="token punctuation">;</span>

</code></pre></div><p>我们可以通过<code>Explain Extend</code>来查看具体的执行计划：</p> <ul><li>不做分区的表格:</li></ul> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token keyword">explain</span> <span class="token keyword">extended</span>
<span class="token keyword">select</span>
  logindate<span class="token punctuation">,</span>
  <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">as</span> cnt
<span class="token keyword">from</span> tb_login
<span class="token keyword">where</span> logindate <span class="token operator">=</span> <span class="token string">'2021-03-23'</span> <span class="token operator">or</span> logindate <span class="token operator">=</span> <span class="token string">'2021-03-24'</span>
<span class="token keyword">group</span> <span class="token keyword">by</span> logindate<span class="token punctuation">;</span>
</code></pre></div><p>具体如下所示：</p> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer//img/image-20211120230120281.png" alt="zo" style="zoom:150%;"> <ul><li>做了分区的表格：</li></ul> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token keyword">explain</span> <span class="token keyword">extended</span>
<span class="token keyword">select</span>
  logindate<span class="token punctuation">,</span>
  <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">as</span> cnt
<span class="token keyword">from</span> tb_login_part
<span class="token keyword">where</span> logindate <span class="token operator">=</span> <span class="token string">'2021-03-23'</span> <span class="token operator">or</span> logindate <span class="token operator">=</span> <span class="token string">'2021-03-24'</span>
<span class="token keyword">group</span> <span class="token keyword">by</span> logindate<span class="token punctuation">;</span>
</code></pre></div><p>具体如下所示：</p> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120230234579.png" alt="image-20211120230234579" style="zoom:150%;"> <p>从图中可以看到根据<code>logindate</code>做了分区后，查询时文件也做了分离。</p> <h3 id="分桶表"><a href="#分桶表" class="header-anchor">#</a> 分桶表</h3> <h5 id="主要解决-hive-中的-join-问题"><a href="#主要解决-hive-中的-join-问题" class="header-anchor">#</a> 主要解决 Hive 中的 Join 问题</h5> <blockquote><p>表的Join是数据分析处理过程中必不可少的操作，Hive同样支持Join的语法，Hive Join的底层还是通过MapReduce来实现的，但是Hive实现Join时面临一个问题：如果有两张非常大的表要进行Join，两张表的数据量都很大，Hive底层通过MapReduce实现时，无法使用MapJoin提高Join的性能，只能走默认的ReduceJoin，而ReduceJoin必须经过Shuffle过程，相对性能比较差，而且容易产生数据倾斜，如何解决这个问题？</p></blockquote> <p>==分桶表设计思想：==</p> <p>​	针对以上的问题，Hive中提供了另外一种表的结构——分桶表结构。分桶表的设计有别于分区表的设计，分区表是将数据划分不同的目录进行存储，而<em>分桶表是将数据划分不同的文件进行存储</em>。分桶表的设计是按照一定的规则<strong>通过MapReduce中的多个Reduce来实现</strong>将数据划分到不同的文件中进行存储，构建分桶表。</p> <p>​	如果有两张表按照相同的划分规则<strong>按照Join的关联字段</strong>将各自的数据进行划分，在Join时，就可以实现Bucket与Bucket的Join，避免不必要的比较。</p> <p>==举个例子：==</p> <blockquote><p>​	当前有两张表，订单表有1000万条，用户表有10万条，两张表的关联字段是userid，现在要实现两张表的Join。</p> <p>​	我们将订单表按照userid划分为3个桶，1000万条数据按照userid的hash取余存储在对应的Bucket中。同理，我们再将用户表按照相同的规则，存储在3个桶中。</p> <p>​	在Join时，只需要将两张表的Bucket0与Bucket0进行Join，Bucket1与Bucket1进行Join，Bucket2与Bucket2进行Join即可，不用让所有的数据挨个比较，降低了比较次数，提高了Join的性能。</p> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120231013932.png" alt=""></p></blockquote> <p>==代码：==</p> <ul><li>创建两张表格</li></ul> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">#  构建分桶dept表</span>
<span class="token keyword">use</span> db_emp<span class="token punctuation">;</span>
<span class="token comment">-- 创建分桶表</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> tb_dept02<span class="token punctuation">(</span>
    deptno string<span class="token punctuation">,</span>
    dname string<span class="token punctuation">,</span>
    loc string
<span class="token punctuation">)</span>
<span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>deptno<span class="token punctuation">)</span> sorted <span class="token keyword">by</span> <span class="token punctuation">(</span>deptno <span class="token keyword">asc</span><span class="token punctuation">)</span> <span class="token keyword">into</span> <span class="token number">3</span> buckets
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">','</span><span class="token punctuation">;</span>
<span class="token comment">--写入分桶表</span>
<span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> tb_dept02
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> tb_dept01<span class="token punctuation">;</span>

<span class="token comment"># 	构建分桶emp表</span>
<span class="token keyword">use</span> db_emp<span class="token punctuation">;</span>
<span class="token comment">-- 创建分桶表</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> tb_emp02<span class="token punctuation">(</span>
   empno string<span class="token punctuation">,</span>
   ename string<span class="token punctuation">,</span>
   job string<span class="token punctuation">,</span>
   managerid string<span class="token punctuation">,</span>
   hiredate string<span class="token punctuation">,</span>
   salary <span class="token keyword">double</span><span class="token punctuation">,</span>
   jiangjin <span class="token keyword">double</span><span class="token punctuation">,</span>
   deptno string
<span class="token punctuation">)</span>
<span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>deptno<span class="token punctuation">)</span> sorted <span class="token keyword">by</span> <span class="token punctuation">(</span>deptno <span class="token keyword">asc</span><span class="token punctuation">)</span> <span class="token keyword">into</span> <span class="token number">3</span> buckets
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>
<span class="token comment">-- 写入分桶表</span>
<span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> tb_emp02
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> tb_emp01<span class="token punctuation">;</span>
</code></pre></div><ul><li>基于emp 和 dept 表格来join</li></ul> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment"># 开启分桶SMB join</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>bucketmapjoin <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span>sortmerge<span class="token punctuation">.</span><span class="token keyword">join</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>bucketmapjoin<span class="token punctuation">.</span>sortedmerge <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>

<span class="token comment">-- 查看执行计划</span>
<span class="token keyword">explain</span>
<span class="token keyword">select</span>
  a<span class="token punctuation">.</span>empno<span class="token punctuation">,</span>
  a<span class="token punctuation">.</span>ename<span class="token punctuation">,</span>
  a<span class="token punctuation">.</span>salary<span class="token punctuation">,</span>
  b<span class="token punctuation">.</span>deptno<span class="token punctuation">,</span>
  b<span class="token punctuation">.</span>dname
<span class="token keyword">from</span> tb_emp02 a <span class="token keyword">join</span> tb_dept02 b <span class="token keyword">on</span> a<span class="token punctuation">.</span>deptno <span class="token operator">=</span> b<span class="token punctuation">.</span>deptno<span class="token punctuation">;</span>
</code></pre></div><p>执行计划如图所示：</p> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120231522028.png" alt="image-20211120231522028" style="zoom:150%;"> <p>对比普通表下使用join的执行计划：</p> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120231604745.png" alt="image-20211120231604745" style="zoom:150%;"> <h3 id="索引设计"><a href="#索引设计" class="header-anchor">#</a> 索引设计</h3> <h5 id="hive-中的索引"><a href="#hive-中的索引" class="header-anchor">#</a> Hive 中的索引</h5> <blockquote><p>​	在传统的关系型数据库例如MySQL、Oracle等数据库中，为了提高数据的查询效率，可以为表中的字段单独构建索引，查询时，可以基于字段的索引快速的实现查询、过滤等操作。</p> <p>​	Hive中也同样提供了索引的设计，允许用户为字段构建索引，提高数据的查询效率。但是Hive的索引与关系型数据库中的索引并不相同，比如，Hive不支持主键或者外键。Hive索引可以建立在表中的某些列上，以提升一些操作的效率，例如减少MapReduce任务中需要读取的数据块的数量。</p> <p>​	在可以预见到分区数据非常庞大的情况下，分桶和索引常常是优于分区的。而分桶由于SMB Join对关联键要求严格，所以并不是总能生效。</p></blockquote> <p>==应用问题==</p> <p>​	由于Hive的索引设计过于繁琐，所以从Hive3.0版本开始，取消了对Hive Index的支持及使用，不过如果使用的是Hive1.x或者Hive2.x在特定的场景下依旧可以使用Hive Index来提高性能。</p> <p>​	实际工作场景中，一般不推荐使用Hive Index，推荐使用ORC文件格式中的索引来代替Hive Index提高查询性能。</p> <h3 id="hive表数据优化"><a href="#hive表数据优化" class="header-anchor">#</a> Hive表数据优化</h3> <h5 id="文件格式"><a href="#文件格式" class="header-anchor">#</a> 文件格式：</h5> <blockquote><p>​	Hive数据存储的本质还是HDFS，所有的数据读写都基于HDFS的文件来实现，为了提高对HDFS文件读写的性能，Hive中提供了多种文件存储格式：TextFile、SequenceFile、RCFile、ORC、Parquet等。不同的文件存储格式具有不同的存储特点，有的可以降低存储空间，有的可以提高查询性能等，可以用来实现不同场景下的数据存储，以提高对于数据文件的读写效率。</p></blockquote> <p>各个格式的具体介绍（来自官方文档）：</p> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120232203172.png" alt="image-20211120232203172" style="zoom:150%;"> <h5 id="textfile"><a href="#textfile" class="header-anchor">#</a> TextFile:</h5> <blockquote><p>​		TextFIle是Hive中默认的文件格式，存储形式为按行存储。工作中最常见的数据文件格式就是TextFile文件，几乎所有的原始数据生成都是TextFile格式，所以Hive设计时考虑到为了避免各种编码及数据错乱的问题，选用了TextFile作为默认的格式。建表时不指定存储格式即为textfile，导入数据时把数据文件拷贝至hdfs不进行处理。</p> <ul><li><strong>TextFile的优点</strong></li></ul> <p>最简单的数据格式，不需要经过处理，可以直接cat查看</p> <p>可以使用任意的分隔符进行分割</p> <p>便于和其他工具（Pig, grep, sed, awk）共享数据</p> <p>可以搭配Gzip、Bzip2、Snappy等压缩一起使用</p> <ul><li><strong>TextFile的缺点</strong></li></ul> <p>耗费存储空间，I/O性能较低</p> <p>结合压缩时Hive不进行数据切分合并，不能进行并行操作，查询效率低</p> <p>按行存储，读取列的性能差</p> <ul><li><strong>TextFile的应用场景</strong></li></ul> <p>适合于小量数据的存储查询</p> <p>一般用于做第一层数据加载和测试使用</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 创建textfile数据表</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> tb_sogou_text<span class="token punctuation">(</span>
  stime string<span class="token punctuation">,</span>
  userid string<span class="token punctuation">,</span>
  keyword string<span class="token punctuation">,</span>
  clickorder string<span class="token punctuation">,</span>
  url string
<span class="token punctuation">)</span>
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span>
stored <span class="token keyword">as</span> textfile<span class="token punctuation">;</span>
</code></pre></div><h5 id="sequencefile"><a href="#sequencefile" class="header-anchor">#</a> SequenceFile:</h5> <blockquote><p>​		SequenceFile是Hadoop里用来存储序列化的键值对即二进制的一种文件格式。SequenceFile文件也可以作为MapReduce作业的输入和输出，hive也支持这种格式。</p> <ul><li><strong>SequenceFIle的优点</strong></li></ul> <p>以二进制的KV形式存储数据，与底层交互更加友好，性能更快</p> <p>可压缩、可分割，优化磁盘利用率和I/O</p> <p>可并行操作数据，查询效率高</p> <p>SequenceFile也可以用于存储多个小文件</p> <ul><li><strong>SequenceFIle的缺点</strong></li></ul> <p>存储空间消耗最大</p> <p>与非Hadoop生态系统之外的工具不兼容</p> <p>构建SequenceFile需要通过TextFile文件转化加载。</p> <ul><li><strong>SequenceFIle的应用</strong></li></ul> <p>适合于小量数据，但是查询列比较多的场景</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 创建SequenceFile数据表</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> tb_sogou_seq<span class="token punctuation">(</span>
  stime string<span class="token punctuation">,</span>
  userid string<span class="token punctuation">,</span>
  keyword string<span class="token punctuation">,</span>
  clickorder string<span class="token punctuation">,</span>
  url string
<span class="token punctuation">)</span>
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span>
stored <span class="token keyword">as</span> sequencefile<span class="token punctuation">;</span>
</code></pre></div><h5 id="parquet"><a href="#parquet" class="header-anchor">#</a> Parquet:</h5> <blockquote><p>​		Parquet是一种支持嵌套结构的<strong>列式存储</strong>文件格式，最早是由Twitter和Cloudera合作开发，2015年5月从Apache孵化器里毕业成为Apache顶级项目。是一种支持嵌套数据模型对的列式存储系统，作为大数据系统中OLAP查询的优化方案，它已经被多种查询引擎原生支持，并且部分高性能引擎将其作为默认的文件存储格式。通过数据编码和压缩，以及映射下推和谓词下推功能，Parquet的性能也较之其它文件格式有所提升。</p> <p>​		Parquet 是与语言无关的，而且不与任何一种数据处理框架绑定在一起，适配多种语言和组件，能够与 Parquet 适配的查询引擎包括 Hive, Impala, Pig, Presto, Drill, Tajo, HAWQ, IBM Big SQL等，计算框架包括 MapReduce, Spark, Cascading, Crunch, Scalding, Kite 等</p> <p>​		Parquet是Hadoop生态圈中主流的列式存储格式，并且行业内流行这样一句话流传：如果说HDFS是大数据时代文件系统的事实标准，Parquet 就是大数据时代存储格式的事实标准。Hive中也同样支持使用Parquet格式来实现数据的存储，并且是工作中主要使用的存储格式之一。</p> <ul><li><strong>Parquet的优点</strong></li></ul> <p>更高效的压缩和编码</p> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120232912397.png" alt=""></p> <p>可用于多种数据处理框架</p> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120232946607.png" alt=""></p> <ul><li><strong>Parquet的缺点</strong></li></ul> <p>不支持update, insert, delete, ACID</p> <ul><li><strong>Parquet的应用</strong></li></ul> <p>适用于字段数非常多，无更新，只取部分列的查询。</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 创建Parquet数据表</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> tb_sogou_parquet<span class="token punctuation">(</span>
  stime string<span class="token punctuation">,</span>
  userid string<span class="token punctuation">,</span>
  keyword string<span class="token punctuation">,</span>
  clickorder string<span class="token punctuation">,</span>
  url string
<span class="token punctuation">)</span>
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span>
stored <span class="token keyword">as</span> parquet<span class="token punctuation">;</span>
</code></pre></div><h5 id="orc"><a href="#orc" class="header-anchor">#</a> ORC：</h5> <blockquote><p>​		<strong>ORC（OptimizedRC File）<strong>文件格式也是一种Hadoop生态圈中的</strong>列式存储</strong>格式，源自于RC（RecordColumnar File），用于降低Hadoop数据存储空间和加速Hive查询速度。它并不是一个单纯的列式存储格式，仍然是首先根据行组分割整个表，在每一个行组内进行<strong>按列存储</strong>。ORC文件是自描述的，它的元数据使用Protocol Buffers序列化，并且文件中的数据尽可能的压缩以降低存储空间的消耗，目前也被Hive、Spark SQL、Presto等查询引擎支持。</p> <p>​	ORC文件也是以二进制方式存储的，所以是不可以直接读取，ORC文件也是自解析的，它包含许多的元数据，这些元数据都是同构ProtoBuffer进行序列化的。其中涉及到如下的概念：</p> <ul><li><p>ORC文件：保存在文件系统上的普通二进制文件，一个ORC文件中可以包含多个stripe，每一stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。</p></li> <li><p>文件级元数据：包括文件的描述信息PostScript、文件meta信息（包括整个文件的统计信息）、所有stripe的信息和文件schema信息。</p></li> <li><p>stripe：一组行形成一个stripe，每次读取文件是以行组为单位的，一般为HDFS的块大小，保存了每一列的索引和数据。</p></li> <li><p>stripe元数据：保存stripe的位置、每一个列的在该stripe的统计信息以及所有的stream类型和位置。</p></li> <li><p>row group：索引的最小单位，一个stripe中包含多个row group，默认为10000个值组成。</p></li> <li><p>stream：一个stream表示文件中一段有效的数据，包括索引和数据两类。索引stream保存每一个row group的位置和统计信息，数据stream包括多种类型的数据，具体需要哪几种是由该列类型和编码方式决定。</p></li></ul></blockquote> <blockquote><ul><li><strong>ORC的优点</strong></li></ul> <p>列式存储，存储效率非常高</p> <p>可压缩，高效的列存取</p> <p>查询效率较高，支持索引</p> <p>支持矢量化查询</p> <ul><li><p>ORC的缺点**</p> <p>加载时性能消耗较大</p> <p>需要通过text文件转化生成</p> <p>读取全量数据时性能较差</p></li> <li><p><strong>ORC的应用</strong></p></li></ul> <p>适用于Hive中大型的存储、查询</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 创建ORC数据表</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> tb_sogou_orc<span class="token punctuation">(</span>
  stime string<span class="token punctuation">,</span>
  userid string<span class="token punctuation">,</span>
  keyword string<span class="token punctuation">,</span>
  clickorder string<span class="token punctuation">,</span>
  url string
<span class="token punctuation">)</span>
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span>
stored <span class="token keyword">as</span> orc<span class="token punctuation">;</span>
</code></pre></div><h3 id="存储优化"><a href="#存储优化" class="header-anchor">#</a> 存储优化</h3> <h5 id="避免小文件生成"><a href="#避免小文件生成" class="header-anchor">#</a> 避免小文件生成：</h5> <blockquote><p>​		Hive的存储本质还是HDFS，HDFS是不利于小文件存储的，因为每个小文件会产生一条元数据信息，并且不利用MapReduce的处理，MapReduce中每个小文件会启动一个MapTask计算处理，导致资源的浪费，所以在使用Hive进行处理分析时，要尽量避免小文件的生成。</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 如果hive的程序，只有maptask，将MapTask产生的所有小文件进行合并</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapfiles<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">-- 如果hive的程序，有Map和ReduceTask,将ReduceTask产生的所有小文件进行合并</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapredfiles<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">-- 每一个合并的文件的大小</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>size<span class="token punctuation">.</span>per<span class="token punctuation">.</span>task<span class="token operator">=</span><span class="token number">256000000</span><span class="token punctuation">;</span>
<span class="token comment">-- 平均每个文件的大小，如果小于这个值就会进行合并</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>smallfiles<span class="token punctuation">.</span>avgsize<span class="token operator">=</span><span class="token number">16000000</span><span class="token punctuation">;</span>
</code></pre></div><p><strong>读取小文件：</strong></p> <p>​	我们总会遇到数据处理的中间结果是小文件的情况，例如每个小时的分区数据中，大多数小时的数据都比较多，但是个别几个小时，如凌晨的2点~6点等等，数据量比较小，下一步进行处理时就必须对多个小文件进行处理，那么这种场景下怎么解决呢？</p> <p>​	类似于MapReduce中的解决方案，Hive中也提供一种输入类CombineHiveInputFormat，用于将小文件合并以后，再进行处理。</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 设置Hive中底层MapReduce读取数据的输入类：将所有文件合并为一个大文件作为输入</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>input<span class="token punctuation">.</span>format<span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>io<span class="token punctuation">.</span>CombineHiveInputFormat<span class="token punctuation">;</span>
</code></pre></div><h5 id="orc文件索引"><a href="#orc文件索引" class="header-anchor">#</a> ORC文件索引：</h5> <blockquote><p>​	在使用ORC文件时，为了加快读取ORC文件中的数据内容，ORC提供了两种索引机制：Row Group Index 和 Bloom Filter Index可以帮助提高查询ORC文件的性能，当用户写入数据时，可以指定构建索引，当用户查询数据时，可以根据索引提前对数据进行过滤，避免不必要的数据扫描。</p></blockquote> <h6 id="row-group-index"><a href="#row-group-index" class="header-anchor">#</a> ==Row Group Index:==</h6> <p>​		一个ORC文件包含一个或多个stripes(groups of row data)，每个stripe中包含了每个column的min/max值的索引数据，<strong>当查询中有&lt;,&gt;,=的操作时，会根据min/max值，跳过扫描不包含的stripes</strong>。而其中为<strong>每个stripe建立的包含min/max值的索引，就称为Row Group Index行组索引</strong>，也叫min-max Index大小对比索引，或者Storage Index。</p> <p>​		在建立ORC格式表时，指定表参数’orc.create.index’=’true’之后，便会建立Row Group Index，需要注意的是，为了使Row Group Index有效利用，向表中加载数据时，<strong>必须对需要使用索引的字段进行排序</strong>，否则，min/max会失去意义。另外，<strong>这种索引主要用于数值型字段的范围查询过滤优化上</strong>。</p> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121103847578.png" alt=""></p> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span><span class="token keyword">index</span><span class="token punctuation">.</span>filter<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">-- 永久生效，请配置在hive-site.xml中</span>

<span class="token keyword">create</span> <span class="token keyword">table</span> tb_sogou_orc_index
stored <span class="token keyword">as</span> orc tblproperties <span class="token punctuation">(</span><span class="token string">&quot;orc.create.index&quot;</span><span class="token operator">=</span><span class="token string">&quot;true&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">as</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> tb_sogou_source
distribute <span class="token keyword">by</span> stime
sort <span class="token keyword">by</span> stime<span class="token punctuation">;</span>
</code></pre></div><h6 id="bloom-filter-index"><a href="#bloom-filter-index" class="header-anchor">#</a> ==Bloom Filter Index:==</h6> <p>​		建表时候，通过表参数”orc.bloom.filter.columns”=”columnName……”来指定为哪些字段建立BloomFilter索引，这样，在生成数据的时候，会在每个stripe中，为该字段建立BloomFilter的数据结构，当查<strong>询条件中包含对该字段的=号过滤时候</strong>，先从BloomFilter中获取以下是否包含该值，如果不包含，则跳过该stripe。</p> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 创建表，并指定构建索引</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> tb_sogou_orc_bloom
stored <span class="token keyword">as</span> orc tblproperties 
<span class="token comment">-- (&quot;orc.create.index&quot;=&quot;true&quot;,orc.bloom.filter.columns&quot;=&quot;stime,userid&quot;) 配置代码</span>
<span class="token keyword">as</span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> tb_sogou_source
distribute <span class="token keyword">by</span> stime
sort <span class="token keyword">by</span> stime<span class="token punctuation">;</span>

<span class="token comment">-- stime的范围过滤可以走row group index，userid的过滤可以走bloom filter index</span>
<span class="token keyword">select</span>
  <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span>
<span class="token keyword">from</span> tb_sogou_orc_index
<span class="token keyword">where</span> stime <span class="token operator">&gt;</span> <span class="token string">'12:00:00'</span> <span class="token operator">and</span> stime <span class="token operator">&lt;</span> <span class="token string">'18:00:00'</span>
<span class="token operator">and</span> userid <span class="token operator">=</span> <span class="token string">'3933365481995287'</span> <span class="token punctuation">;</span>
</code></pre></div><h6 id="orc矢量化查询"><a href="#orc矢量化查询" class="header-anchor">#</a> ==ORC矢量化查询:==</h6> <p>​		Hive的默认查询执行引擎一次处理一行，而矢量化查询执行是一种Hive针对ORC文件操作的特性，目的是按照每批1024行读取数据，并且一次性对整个记录整合（而不是对单条记录）应用操作，提升了像过滤, 联合, 聚合等等操作的性能。</p> <p>​		注意：要使用矢量化查询执行，就必须以<strong>ORC格式</strong>存储数据。</p> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token keyword">set</span> hive<span class="token punctuation">.</span>vectorized<span class="token punctuation">.</span>execution<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>vectorized<span class="token punctuation">.</span>execution<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>enabled <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
</code></pre></div><h3 id="计算-job-执行优化"><a href="#计算-job-执行优化" class="header-anchor">#</a> 计算 Job 执行优化</h3> <blockquote><p>​		HiveQL是一种类SQL的语言，从编程语言规范来说是一种声明式语言，用户会根据查询需求提交声明式的HQL查询，而Hive会根据底层计算引擎将其转化成Mapreduce/Tez/Spark的 job。</p> <p>​		explain会解析HQL语句，将整个HQL语句的实现步骤、依赖关系、实现过程都会进行解析返回，可以帮助更好的了解一条HQL语句在底层是如何实现数据的查询及处理的过程，这样可以辅助用户对Hive进行优化。</p> <p>​		官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain<img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121104815958.png" alt="image-20211121104815958" style="zoom:150%;"></p></blockquote> <p>==语法解释：==</p> <p><code>EXPLAIN [FORMATTED|EXTENDED|DEPENDENCY|AUTHORIZATION|] query</code></p> <p>	FORMATTED：对执行计划进行格式化，返回JSON格式的执行计划
	EXTENDED：提供一些额外的信息，比如文件的路径信息
	DEPENDENCY：以JSON格式返回查询所依赖的表和分区的列表
	AUTHORIZATION：列出需要被授权的条目，包括输入与输出</p> <p>==解析执行：==</p> <p>解析后的执行计划一般由三个部分构成，分别是：</p> <ul><li><strong>The Abstract Syntax Tree for the query</strong></li></ul> <p>抽象语法树：Hive使用Antlr解析生成器，可以自动地将HQL生成为抽象语法树</p> <ul><li><strong>The dependencies between the different stages of the plan</strong></li></ul> <p>Stage依赖关系：会列出运行查询所有的依赖以及stage的数量</p> <ul><li><strong>The description of each of the stages</strong></li></ul> <p>Stage内容：包含了非常重要的信息，比如运行时的operator和sort orders等具体的信息</p> <p>==示例1：过滤==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token keyword">explain</span> <span class="token keyword">select</span> <span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">as</span> cnt <span class="token keyword">from</span> tb_emp <span class="token keyword">where</span> deptno <span class="token operator">=</span> <span class="token string">'10'</span><span class="token punctuation">;</span>
</code></pre></div><ul><li>组成：</li></ul> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121105109366.png" alt="image-20211121105109366" style="zoom:150%;"> <ul><li>解释：</li></ul> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121105228837.png" alt="image-20211121105228837" style="zoom:150%;"> <p>==示例2:分组排序==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token keyword">explain</span>
<span class="token keyword">select</span>
   deptno<span class="token punctuation">,</span><span class="token function">count</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">as</span> cnt
<span class="token keyword">from</span> tb_emp <span class="token keyword">where</span> salary <span class="token operator">&gt;</span> <span class="token number">2000</span>
<span class="token keyword">group</span> <span class="token keyword">by</span> deptno <span class="token keyword">order</span> <span class="token keyword">by</span> cnt <span class="token keyword">desc</span><span class="token punctuation">;</span>
</code></pre></div><ul><li>==解释：==</li></ul> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121105432180.png" alt=""></p> <h3 id="mapreduce-属性优化"><a href="#mapreduce-属性优化" class="header-anchor">#</a> MapReduce 属性优化：</h3> <h5 id="jvm重用"><a href="#jvm重用" class="header-anchor">#</a> JVM重用：</h5> <blockquote><p>​		JVM正常指代一个Java进程，Hadoop默认使用派生的JVM来执行map-reducer，如果一个MapReduce程序中有100个Map，10个Reduce，Hadoop默认会为每个Task启动一个JVM来运行，那么就会启动100个JVM来运行MapTask，在JVM启动时内存开销大，尤其是Job大数据量情况，如果单个Task数据量比较小，也会申请JVM资源，这就导致了资源紧张及浪费的情况。</p> <p>​		为了解决上述问题，MapReduce中提供了JVM重用机制来解决，JVM重用可以使得JVM实例在同一个job中重新使用N次，当一个Task运行结束以后，JVM不会进行释放，而是继续供下一个Task运行，直到运行了N个Task以后，就会释放，N的值可以在Hadoop的mapred-site.xml文件中进行配置，通常在10-20之间。</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- Hadoop3之前的配置，在mapred-site.xml中添加以下参数</span>
<span class="token comment">-- Hadoop3中已不再支持该选项</span>
mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>jvm<span class="token punctuation">.</span>numtasks<span class="token operator">=</span><span class="token number">10</span> 
</code></pre></div><h5 id="并行执行"><a href="#并行执行" class="header-anchor">#</a> 并行执行：</h5> <blockquote><p>​		Hive在实现HQL计算运行时，会解析为多个Stage，有时候Stage彼此之间有依赖关系，只能挨个执行，但是在一些别的场景下，很多的Stage之间是没有依赖关系的，例如Union语句，Join语句等等，这些Stage没有依赖关系，但是Hive依旧默认挨个执行每个Stage，这样会导致性能非常差，我们可以通过修改参数，开启并行执行，当多个Stage之间没有依赖关系时，允许多个Stage并行执行，提高性能。</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 开启Stage并行化，默认为false</span>
<span class="token keyword">SET</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">-- 指定并行化线程数，默认为8</span>
<span class="token keyword">SET</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>thread<span class="token punctuation">.</span>number<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">;</span> 
</code></pre></div><p><strong>-- 注意：线程数越多，程序运行速度越快，但同样更消耗CPU资源</strong></p> <h5 id="join-优化"><a href="#join-优化" class="header-anchor">#</a> Join 优化：</h5> <blockquote><p>​		表的Join是数据分析处理过程中必不可少的操作，Hive同样支持Join的语法，Hive Join的底层还是通过MapReduce来实现的，Hive实现Join时，为了提高MapReduce的性能，提供了多种Join方案来实现，例如适合小表Join大表的Map Join，大表Join大表的Reduce Join，以及大表Join的优化方案Bucket Join等。</p></blockquote> <h6 id="map-join"><a href="#map-join" class="header-anchor">#</a> ==Map Join:==</h6> <blockquote><ul><li><strong>应用场景</strong></li></ul> <p>适合于小表join大表或者小表Join小表</p> <ul><li><p><strong>原理</strong></p> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121110059681.png" alt=""></p></li></ul> <p>将小的那份数据给每个MapTask的内存都放一份完整的数据，大的数据每个部分都可以与小数据的完整数据进行join</p> <p>底层不需要经过shuffle，需要占用内存空间存放小的数据文件</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 默认已经开启了Map Join</span>
hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span><span class="token keyword">join</span><span class="token operator">=</span><span class="token boolean">true</span>
</code></pre></div><p><strong>Hive中判断哪张表是小表及限制</strong></p> <ul><li><p>LEFT OUTER JOIN的左表必须是大表</p></li> <li><p>RIGHT OUTER JOIN的右表必须是大表</p></li> <li><p>INNER JOIN左表或右表均可以作为大表</p></li> <li><p>FULL OUTER JOIN不能使用MAPJOIN</p></li> <li><p>MAPJOIN支持小表为子查询</p></li> <li><p>使用MAPJOIN时需要引用小表或是子查询时，需要引用别名</p></li> <li><p>在MAPJOIN中，可以使用不等值连接或者使用OR连接多个条件</p></li> <li><p>在MAPJOIN中最多支持指定6张小表，否则报语法错误</p></li></ul> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 2.0版本开始由以下参数控制</span>
hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span><span class="token keyword">join</span><span class="token punctuation">.</span>noconditionaltask<span class="token punctuation">.</span>size<span class="token operator">=</span><span class="token number">512000000</span>
</code></pre></div><h6 id="reduce-join"><a href="#reduce-join" class="header-anchor">#</a> ==Reduce Join:==</h6> <blockquote><ul><li><strong>应用场景</strong></li></ul> <p>适合于大表Join大表</p> <ul><li><strong>原理</strong></li></ul> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121110600183.png" alt=""></p> <p>将两张表的数据在shuffle阶段利用shuffle的分组来将数据按照关联字段进行合并</p> <p>必须经过shuffle，利用Shuffle过程中的分组来实现关联</p> <ul><li>使用：</li></ul> <p>Hive会自动判断是否满足Map Join，如果不满足Map Join，则自动执行Reduce Join</p></blockquote> <h6 id="bucket-join"><a href="#bucket-join" class="header-anchor">#</a> ==Bucket Join:==</h6> <blockquote><ul><li><strong>应用场景</strong></li></ul> <p>适合于大表Join大表</p> <ul><li><strong>原理</strong></li></ul> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121110809984.png" alt=""></p> <p>将两张表按照相同的规则将数据划分，根据对应的规则的数据进行join，减少了比较次数，提高了性能</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 开启分桶join</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>bucketmapjoin <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>

<span class="token comment">-- Sort Merge Bucket Join（SMB）：基于有序的数据Join</span>
<span class="token comment">-- 开启分桶SMB join</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>bucketmapjoin <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span>sortmerge<span class="token punctuation">.</span><span class="token keyword">join</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>bucketmapjoin<span class="token punctuation">.</span>sortedmerge <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>auto<span class="token punctuation">.</span><span class="token keyword">convert</span><span class="token punctuation">.</span>sortmerge<span class="token punctuation">.</span><span class="token keyword">join</span><span class="token punctuation">.</span>noconditionaltask<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>

</code></pre></div><p><strong>-- 分桶字段 = Join字段 = 排序字段 ，桶的个数相等或者成倍数</strong></p> <h5 id="优化器"><a href="#优化器" class="header-anchor">#</a> 优化器：</h5> <blockquote><p>​		在使用Hive的过程中经常会遇到一些特殊的问题，例如当一个程序中如果有一些操作彼此之间有关联性，是可以放在一个MapReduce中实现的，但是Hive会不智能的选择，Hive会使用两个MapReduce来完成这两个操作。</p></blockquote> <p>==示例：==</p> <p>当我们执行以下SQL语句：</p> <p><code>select …… from table group by id order by id desc;</code></p> <p>该SQL语句转换为MapReduce时，我们可以有两种方案来实现：</p> <ul><li><strong>方案一</strong></li></ul> <p>第一个MapReduce做group by，经过shuffle阶段对id做分组</p> <p>第二个MapReduce对第一个MapReduce的结果做order by，经过shuffle阶段对id进行排序</p> <ul><li><strong>方案二</strong></li></ul> <p>因为都是对id处理，可以使用一个MapReduce的shuffle既可以做分组也可以排序</p> <p>在这种场景下，Hive会默认选择用第一种方案来实现，这样会导致性能相对较差，我们可以在Hive中开启关联优化，对有关联关系的操作进行解析时，可以尽量放在同一个MapReduce中实现。</p> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>correlation<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
</code></pre></div><h5 id="cbo-优化器引擎"><a href="#cbo-优化器引擎" class="header-anchor">#</a> CBO 优化器引擎：</h5> <blockquote><p>​		在使用MySQL或者Hive等工具时，我们经常会遇到一个问题，默认的优化器在底层解析一些聚合统计类的处理的时候，底层解析的方案有时候不是最佳的方案。</p> <p>​		例如：当前有一张表【共1000条数据】，id构建了索引，id =100值有900条，我们现在的需求是查询所有id = 100的数据，所以SQL语句为：select * from table where id = 100;</p> <p>​		由于id这一列构建了索引，索引默认的优化器引擎RBO，会选择先从索引中查询id = 100的值所在的位置，再根据索引记录位置去读取对应的数据，但是这并不是最佳的执行方案。有id=100的值有900条，占了总数据的90%，这时候是没有必要检索索引以后再检索数据的，可以直接检索数据返回，这样的效率会更高，更节省资源，这种方式就是CBO优化器引擎会选择的方案。</p> <p>​		使用Hive时，Hive中也支持RBO与CBO这两种引擎，默认使用的是RBO优化器引擎。</p> <ul><li><strong>RBO</strong></li></ul> <p>rule basic optimise：基于规则的优化器</p> <p>根据设定好的规则来对程序进行优化</p> <ul><li><strong>CBO</strong></li></ul> <p>cost basic optimise：基于代价的优化器</p> <p>根据不同场景所需要付出的代价来合适选择优化的方案</p> <p>对数据的分布的信息【数值出现的次数，条数，分布】来综合判断用哪种处理的方案是最佳方案</p> <p><strong>很明显CBO引擎更加智能，所以在使用Hive时，我们可以配置底层的优化器引擎为CBO引擎。</strong></p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token keyword">set</span> hive<span class="token punctuation">.</span>cbo<span class="token punctuation">.</span><span class="token keyword">enable</span><span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">compute</span><span class="token punctuation">.</span>query<span class="token punctuation">.</span><span class="token keyword">using</span><span class="token punctuation">.</span>stats<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>stats<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span><span class="token keyword">column</span><span class="token punctuation">.</span>stats<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>stats<span class="token punctuation">.</span><span class="token keyword">fetch</span><span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span>stats<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
</code></pre></div><ul><li><strong>要求</strong></li></ul> <p>要想使用CBO引擎，必须构建数据的元数据【表行数、列的信息、分区的信息……】</p> <p>提前获取这些信息，CBO才能基于代价选择合适的处理计划</p> <p>所以CBO引擎一般搭配analyze分析优化器一起使用</p> <h5 id="analyze分析优化器"><a href="#analyze分析优化器" class="header-anchor">#</a> Analyze分析优化器:</h5> <blockquote><ul><li>功能</li></ul> <p>用于提前运行一个MapReduce程序将表或者分区的信息构建一些元数据 <code>表的信息、分区信息、列的信息</code>，搭配CBO引擎一起使用</p></blockquote> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 构建分区信息元数据</span>
<span class="token keyword">ANALYZE</span> <span class="token keyword">TABLE</span> tablename
  <span class="token punctuation">[</span><span class="token keyword">PARTITION</span><span class="token punctuation">(</span>partcol1<span class="token punctuation">[</span><span class="token operator">=</span>val1<span class="token punctuation">]</span><span class="token punctuation">,</span> partcol2<span class="token punctuation">[</span><span class="token operator">=</span>val2<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
  <span class="token keyword">COMPUTE</span> <span class="token keyword">STATISTICS</span> <span class="token punctuation">[</span>noscan<span class="token punctuation">]</span><span class="token punctuation">;</span>

<span class="token comment">-- 构建列的元数据</span>
<span class="token keyword">ANALYZE</span> <span class="token keyword">TABLE</span> tablename
  <span class="token punctuation">[</span><span class="token keyword">PARTITION</span><span class="token punctuation">(</span>partcol1<span class="token punctuation">[</span><span class="token operator">=</span>val1<span class="token punctuation">]</span><span class="token punctuation">,</span> partcol2<span class="token punctuation">[</span><span class="token operator">=</span>val2<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
  <span class="token keyword">COMPUTE</span> <span class="token keyword">STATISTICS</span> <span class="token keyword">FOR</span> <span class="token keyword">COLUMNS</span> <span class="token punctuation">(</span> <span class="token keyword">columns</span> name1<span class="token punctuation">,</span> <span class="token keyword">columns</span> name2<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token punctuation">[</span>noscan<span class="token punctuation">]</span><span class="token punctuation">;</span>

<span class="token comment">-- 查看元数据</span>
<span class="token keyword">DESC</span> FORMATTED <span class="token punctuation">[</span>tablename<span class="token punctuation">]</span> <span class="token punctuation">[</span>columnname<span class="token punctuation">]</span><span class="token punctuation">;</span>
</code></pre></div><h5 id="谓词下推-pdd"><a href="#谓词下推-pdd" class="header-anchor">#</a> 谓词下推 PDD :</h5> <blockquote><ul><li>基本思想：</li></ul> <p>谓词下推 Predicate Pushdown（PPD）的思想简单点说就是在不影响最终结果的情况下，尽量将过滤条件提前执行。谓词下推后，过滤条件在map端执行，减少了map端的输出，降低了数据在集群上传输的量，降低了Reduce端的数据负载，节约了集群的资源，也提升了任务的性能。</p> <ul><li>开启参数：(<em>默认自动开启谓词下推</em>)</li></ul> <p><code>hive.optimize.ppd=**true**;</code></p> <ul><li>基本规则：</li></ul> <p>不同Join场景下的Where谓词下推测试</p> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121112210279.png" alt=""></p> <ul><li>试验结论：</li></ul> <img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121112400823.png" alt="image-20211121112400823" style="zoom:150%;"> <p>Inner Join和Full outer Join，条件写在on后面，还是where后面，性能上面没有区别</p> <p>Left outer Join时 ，右侧的表写在on后面，左侧的表写在where后面，性能上有提高</p> <p>Right outer Join时，左侧的表写在on后面、右侧的表写在where后面，性能上有提高</p> <p>如果SQL语句中出现不确定结果的函数，也无法实现下推</p></blockquote> <h3 id="数据倾斜"><a href="#数据倾斜" class="header-anchor">#</a> 数据倾斜：</h3> <blockquote><ul><li><p>现象：</p> <p>分布式计算中最常见的，最容易遇到的问题就是数据倾斜，数据倾斜的现象是，当我们提交运行一个程序时，我们通过监控发现，这个程序的大多数的Task都已经运行结束了，只有某一个Task一直在运行，迟迟不能结束，导致整体的进度卡在99%或者100%，这时候我们就可以判定程序出现了数据倾斜的问题。</p></li> <li><p>原因：</p> <p>第一：数据本身就是倾斜的，数据中某种数据出现的次数过多。</p> <p>第二：分区规则导致这些相同的数据都分配给了同一个Task，导致这个Task拿到了大量的数据，而其他Task拿到的数据比较少，所以运行起来相比较于其他Task就比较慢一些。</p> <p><strong>产生数据倾斜的根本原因在于分区规则。</strong></p></li></ul></blockquote> <h6 id="group-by的数据倾斜"><a href="#group-by的数据倾斜" class="header-anchor">#</a> ==group By的数据倾斜:==</h6> <p>​		当程序中出现group by或者count（distinct）等分组聚合的场景时，如果数据本身是倾斜的根据MapReduce的Hash分区规则，肯定会出现数据倾斜的现象。根本原因是因为分区规则导致的，所以我们可以通过以下几种方案来解决group by导致的数据倾斜的问题。</p> <ul><li><strong>方案一：开启Map端聚合</strong></li></ul> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 开启Map端聚合：Combiner</span>
hive<span class="token punctuation">.</span>map<span class="token punctuation">.</span>aggr<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
</code></pre></div><p><strong>通过减少Reduce的输入量，避免每个Task数据差异过大导致数据倾斜</strong></p> <ul><li><strong>方案二：实现随机分区</strong></li></ul> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- SQL中避免数据倾斜，构建随机分区</span>
<span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> <span class="token keyword">table</span> distribute <span class="token keyword">by</span> rand<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div><ol><li><p>distribute by用于指定底层的MapReduce按照哪个字段作为Key实现分区、分组等</p></li> <li><p>默认由Hive自己选择，我们可以通过distribute by自己指定，通过rank函数随机值实现随机分区，避免数据倾斜</p></li></ol> <ul><li><strong>方案三：自动构建随机分区并自动聚合</strong></li></ul> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 开启随机分区，走两个MapReduce </span>
hive<span class="token punctuation">.</span>groupby<span class="token punctuation">.</span>skewindata<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
</code></pre></div><ol><li><p>开启该参数以后，当前程序会自动通过两个MapReduce来运行</p></li> <li><p>第一个MapReduce自动进行随机分区，然后实现聚合</p></li> <li><p>第二个MapReduce将聚合的结果再按照业务进行处理，得到结果</p></li></ol> <h6 id="join-的数据倾斜"><a href="#join-的数据倾斜" class="header-anchor">#</a> ==Join 的数据倾斜:==</h6> <blockquote><p>​		实际业务需求中往往需要构建两张表的Join实现，如果两张表比较大，无法实现Map Join，只能走Reduce Join，那么当关联字段中某一种值过多的时候依旧会导致数据倾斜的问题，面对Join产生的数据倾斜，我们核心的思想是尽量避免Reduce Join的产生，优先使用Map Join来实现，但往往很多的Join场景不满足Map Join的需求。</p></blockquote> <p>那么我们可以以下几种方案来解决Join产生的数据倾斜问题：</p> <ul><li><strong>方案一：提前过滤，将大数据变成小数据，实现Map Join:</strong></li></ul> <p>实现两张表的Join时，我们要尽量考虑是否可以使用Map Join来实现Join过程**。**有些场景下看起来是大表Join大表，但是我们可以通过转换将大表Join大表变成大表Join小表，来实现Map Join。</p> <p>==例子：==现在有两张表订单表A与用户表B，需要实现查询今天所有订单的用户信息。</p> <p>​		A表：今天的订单，1000万条，字段：orderId,userId,produceId,price等</p> <p>​		B表：用户信息表，100万条，字段：userid,username,age,phone等</p> <p>​		关联字段为 userid</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 将下了订单的用户的数据过滤出来，再Join</span>
<span class="token keyword">select</span> a<span class="token punctuation">.</span><span class="token operator">*</span><span class="token punctuation">,</span>d<span class="token punctuation">.</span><span class="token operator">*</span>
<span class="token keyword">from</span> <span class="token punctuation">(</span>
  <span class="token comment">-- 获取所有下订单的用户信息</span>
  <span class="token keyword">select</span>
    b<span class="token punctuation">.</span><span class="token operator">*</span>
  <span class="token keyword">from</span>
  <span class="token comment">-- 获取所有下订单的userid</span>
  <span class="token punctuation">(</span> <span class="token keyword">select</span> <span class="token keyword">distinct</span> a<span class="token punctuation">.</span>userid <span class="token keyword">from</span> A a <span class="token punctuation">)</span> c <span class="token keyword">join</span> B b <span class="token keyword">on</span> c<span class="token punctuation">.</span>userid <span class="token operator">=</span> b<span class="token punctuation">.</span>userid <span class="token punctuation">)</span> d 
<span class="token keyword">join</span>
  A a <span class="token keyword">on</span> d<span class="token punctuation">.</span>userid <span class="token operator">=</span> a<span class="token punctuation">.</span>userid<span class="token punctuation">;</span>
</code></pre></div><ol><li><p>100万个用户中，在今天下订单的人数可能只有一小部分，大量数据是不会Join成功的</p></li> <li><p>可以提前将订单表中的userid去重，获取所有下订单的用户id</p></li> <li><p>再使用所有下订单的用户id关联用户表，得到所有下订单的用户的信息</p></li> <li><p>最后再使用下订单的用户信息关联订单表</p></li> <li><p>通过多次Map Join来代替Reduce Join，性能更好也可以避免数据倾斜</p></li></ol> <p>​</p> <ul><li><strong>方案二：使用Bucket Join</strong></li></ul> <ol><li><p>如果使用方案一来避免Reduce Join ，有些场景下依旧无法满足，例如过滤后的数据依旧是一张大表，那么最后的Join依旧是一个Reduce Join</p></li> <li><p>这种场景下，我们可以将两张表的数据构建为桶表，实现Bucket Map Join，避免数据倾斜</p></li></ol> <ul><li><p><strong>方案三：使用Skew Join</strong></p> <blockquote><p>​		Skew Join是Hive中一种专门为了避免数据倾斜而设计的特殊的Join过程，这种Join的原理是将Map Join和Reduce Join进行合并，如果某个值出现了数据倾斜，就会将产生数据倾斜的数据单独使用Map Join来实现，其他没有产生数据倾斜的数据由Reduce Join来实现，这样就避免了Reduce Join中产生数据倾斜的问题，最终将Map Join的结果和Reduce Join的结果进行Union合并。</p></blockquote></li></ul> <p>==代码：==</p> <div class="language-sql extra-class"><pre class="language-sql"><code><span class="token comment">-- 开启运行过程中skewjoin</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>skewjoin<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">-- 如果这个key的出现的次数超过这个范围</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>skewjoin<span class="token punctuation">.</span><span class="token keyword">key</span><span class="token operator">=</span><span class="token number">100000</span><span class="token punctuation">;</span>
<span class="token comment">-- 在编译时判断是否会产生数据倾斜</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span>skewjoin<span class="token punctuation">.</span>compiletime<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">-- 不合并，提升性能</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">optimize</span><span class="token punctuation">.</span><span class="token keyword">union</span><span class="token punctuation">.</span>remove<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">-- 如果Hive的底层走的是MapReduce，必须开启这个属性，才能实现不合并</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>input<span class="token punctuation">.</span>fileinputformat<span class="token punctuation">.</span>input<span class="token punctuation">.</span>dir<span class="token punctuation">.</span>recursive<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
</code></pre></div><p>==原理：==</p> <p><img src="https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121114042570.png" alt=""></p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/./bigdata/hive/Hive全面讲解.html" class="prev">
        Hive全面讲解
      </a></span> <span class="next"><a href="/./bigdata/hive/Flink1.10和Hive集成一些需要注意的点.html">
        Flink1.10和Hive集成一些需要注意的点
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="./assets/js/app.dbe5dc78.js" defer></script><script src="./assets/js/2.fa5f1a4a.js" defer></script><script src="./assets/js/19.b1f41278.js" defer></script>
  </body>
</html>
