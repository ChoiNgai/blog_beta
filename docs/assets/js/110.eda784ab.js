(window.webpackJsonp=window.webpackJsonp||[]).push([[110],{494:function(t,s,a){"use strict";a.r(s);var n=a(30),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"用户画像-标签数据存储"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#用户画像-标签数据存储"}},[t._v("#")]),t._v(" 用户画像：标签数据存储")]),t._v(" "),a("p",[t._v("在用户画像简介 中提到过一个通用的用户画像建设架构，也就是一个典型的Lambda架构。这里参考用户画像工程化解决方案，在整个工程化方案中，系统依赖的基础设施包括"),a("strong",[t._v("Spark、Hive、HBase、Airflow、MySQL、Redis、Elasticsearch")]),t._v("。除去基础设施外，系统主体还包括 "),a("strong",[t._v("Flink、ETL、产品端")]),t._v(" 3个重要组成部分。图 2-1 所示是用户画像数仓架构图，下面对其进行详细介绍。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/joeyooa/data-images/raw/master/note/2021/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303131393030353731303936352e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c.png",alt:""}})]),t._v(" "),a("p",[t._v("下方虚线框中为常见的"),a("strong",[t._v("数据仓库ETL加工流程")]),t._v("，也就是将"),a("strong",[t._v("每日的业务数据、日志数据、埋点数据等经过ETL过程，加工到数据仓库对应的ODS层、DW层、DM层中")]),t._v("。")]),t._v(" "),a("p",[t._v("​    中间的虚线框即为用户画像建模的主要环节，"),a("strong",[t._v("用户画像不是产生数据的源头，而是对基于数据仓库ODS层、DW层、DM层中与用户相关数据的二次建模加工")]),t._v("。在ETL过程中将用户标签计算结果写入"),a("strong",[t._v("Hive")]),t._v("，由于不同数据库有不同的应用场景，后续需要进一步将数据同步到"),a("code",[t._v("MySQL")]),t._v("、"),a("code",[t._v("HBase")]),t._v("、"),a("code",[t._v("Elasticsearch")]),t._v(" 等数据库中。")]),t._v(" "),a("ul",[a("li",[t._v("Hive："),a("strong",[t._v("存储用户标签计算结果")]),t._v("、"),a("strong",[t._v("用户人群计算结果")]),t._v("、"),a("strong",[t._v("用户特征库计算结果")])]),t._v(" "),a("li",[t._v("MySQL："),a("strong",[t._v("存储标签元数据")]),t._v("，"),a("strong",[t._v("监控相关数据")]),t._v("，"),a("strong",[t._v("导出到业务系统的数据")])]),t._v(" "),a("li",[t._v("HBase："),a("strong",[t._v("存储线上接口实时调用类数据")])]),t._v(" "),a("li",[t._v("Elasticserch："),a("strong",[t._v("支持海量数据的实时查询分析")]),t._v("，"),a("strong",[t._v("用于存储用户人群计算、用户群透视分析所需的用户标签数据")]),t._v("（由于用户人群计算、用户群透视分析的条件转化成的SQL语句多条件嵌套较为复杂，使用 Impala 执行也需花费大量时间）")])]),t._v(" "),a("p",[t._v("​    "),a("strong",[t._v("用户标签数据在Hive中加工完成后，部分标签通过Sqoop同步到MySQL数据库，提供用于BI报表展示的数据、多维透视分析数据、圈人服务数据；另一部分标签同步到HBase数据库用于产品的线上个性化推荐")])]),t._v(" "),a("h2",{attrs:{id:"_1-标签数据存储之hive"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-标签数据存储之hive"}},[t._v("#")]),t._v(" 1. 标签数据存储之Hive")]),t._v(" "),a("h3",{attrs:{id:"数据存储建模"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据存储建模"}},[t._v("#")]),t._v(" 数据存储建模")]),t._v(" "),a("p",[t._v("站在数仓开发的角度来看，即是将数仓中建模好的数据，包括业务数据与用户埋点数据，基于一定的统计、规则、算法将数仓中的数据加工成用户标签树。数仓建模、分层的方法这里不做多的介绍，按照规范，用户画像的输入数据为Hive中DWD表，输出为DWS层。重点考虑标签表的设计。Hive存储标签相关的数据涉及到的一些表：")]),t._v(" "),a("ul",[a("li",[t._v("用户标签表")]),t._v(" "),a("li",[t._v("标签聚合的表")]),t._v(" "),a("li",[t._v("人群计算的表")])]),t._v(" "),a("h4",{attrs:{id:"宽表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#宽表"}},[t._v("#")]),t._v(" 宽表")]),t._v(" "),a("p",[t._v("如果将用户标签开发成一张大的宽表，以Hive为例，我们最常用的就是宽表，也就是一个 key，跟上它的所有标签。比如下面是一个简单的宽表。")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"left"}},[t._v("用户ID")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("性别")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("年龄")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("学历")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("职业")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("月薪")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("月消费能力")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"left"}},[t._v("001")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("男")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("28")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("本科")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("程序员")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("10k-20k")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("1k-2k")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"left"}},[t._v("002")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("女")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("23")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("大专")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("销售")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("不详")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("100-200")])])])]),t._v(" "),a("p",[t._v("那么用宽表有什么问题吗？")]),t._v(" "),a("ul",[a("li",[t._v("由于用户的标签会非常多，而且随着用户画像的深入，会有很多细分领域的标签，这就意味着标签的数量会随时增加，而且可能会很频繁。")]),t._v(" "),a("li",[t._v("不同的标签计算频率不同，比如说学历一周计算一次都是可以接收的，但是APP登录活跃情况却可能需要每天都要计算。")]),t._v(" "),a("li",[t._v("计算完成时间不同，如果是以宽表的形式存储，那么最终需要把各个小表的计算结果合并，此时如果出现了一部分结果早上3点计算完成，一部分要早上10点才能计算完成，那么宽表最终的生成时间就要很晚。")]),t._v(" "),a("li",[t._v("大量空缺的标签会导致存储稀疏，有一些标签会有很多的缺失，这在用户画像中很常见。")])]),t._v(" "),a("p",[t._v("当标签数量较多的时候，我们必须考虑以上问题，且增加ETL任务的时间，维护困难。")]),t._v(" "),a("h4",{attrs:{id:"竖表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#竖表"}},[t._v("#")]),t._v(" 竖表")]),t._v(" "),a("p",[t._v("竖表即是将userId + 标签ID作为分组的key存储。竖表其实就是将标签都拆开，一个用户有多少标签，那么在这里面就会有几条数据。")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"left"}},[t._v("用户ID")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("标签名")]),t._v(" "),a("th",{staticStyle:{"text-align":"left"}},[t._v("标签值")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"left"}},[t._v("001")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("sex")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("男")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"left"}},[t._v("001")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("salary_month")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("10k-20k")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"left"}},[t._v("002")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("sex")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("女")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"left"}},[t._v("002")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("age")]),t._v(" "),a("td",{staticStyle:{"text-align":"left"}},[t._v("23")])])])]),t._v(" "),a("p",[t._v("竖表能比较好地解决上面宽表的问题。但是它也会带来了新的问题，比如说多标签组合的查询需求：“我们想看年龄在23-30之间，月薪在10-20k之间，喜欢听古典音乐的女性”，这种多标签查询条件组合情况在竖表中就不太容易支持。")]),t._v(" "),a("h4",{attrs:{id:"横表-竖表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#横表-竖表"}},[t._v("#")]),t._v(" 横表+竖表")]),t._v(" "),a("p",[t._v("如前面所分析，竖表和横表各有所长和所短，那么能不能两者结合呢？")]),t._v(" "),a("p",[t._v("这其实也要考虑横表和竖表的特性，整体来讲就是竖表对计算层支持的好，横表对查询层支持的好。那么设计的化就可以这样：")]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://gitee.com/joeyooa/data-images/raw/master/note/2021/image-20210605155208457.png",alt:"image-20210605155208457"}}),t._v(" "),a("h4",{attrs:{id:"分区存储"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分区存储"}},[t._v("#")]),t._v(" 分区存储")]),t._v(" "),a("p",[t._v("如果将用户标签开发成一张大的宽表，在这张宽表下放几十种类型标签，那么每天该画像宽表的ETL作业将会花费很长时间，而且不便于向这张宽表中新增标签类型。")]),t._v(" "),a("p",[t._v("​    要解决这种ETL花费时间较长的问题，可以从以下几个方面着手：")]),t._v(" "),a("ul",[a("li",[t._v("将数据分区存储，分别执行作业；")]),t._v(" "),a("li",[t._v("标签脚本性能调优；")]),t._v(" "),a("li",[t._v("基于一些标签共同的数据来源开发中间表。")])]),t._v(" "),a("p",[t._v("​    下面介绍一种用户标签分表、分区存储的解决方案。")]),t._v(" "),a("p",[t._v("​    根据标签指标体系的人口属性、行为属性、用户消费、风险控制、社交属性等维度分别建立对应的标签表进行分表存储对应的标签数据。如下图所示。")]),t._v(" "),a("ul",[a("li",[t._v("人口属性表：dw.userprofile_attritube_all；")]),t._v(" "),a("li",[t._v("行为属性表：dw.userprofile_action_all；")]),t._v(" "),a("li",[t._v("用户消费表：dw.userprofile_consume_all；")]),t._v(" "),a("li",[t._v("风险控制表：dw.userprofile_riskmanage_all；")]),t._v(" "),a("li",[t._v("社交属性表：dw.userprofile_social_all")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/joeyooa/data-images/raw/master/note/2021/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313039353131393637332e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c.png",alt:"68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313039353131393637332e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c"}})]),t._v(" "),a("p",[t._v("用户的人口属性宽表：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/joeyooa/data-images/raw/master/note/2021/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313039353233323138312e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c.png",alt:"68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313039353233323138312e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c"}})]),t._v(" "),a("ul",[a("li",[t._v("其他id维度（如cookieid、deviceid、registerid等）的标签数据存储，也可以使用上面案例中的表结构。")])]),t._v(" "),a("p",[t._v("​    在上面的创建中通过设立人口属性维度的宽表开发相关的用户标签，为了提高数据的插入和查询效率，在Hive中可以使用分区表的方式，将数据存储在不同的目录中。在Hive使用select查询时一般会扫描整个表中所有数据，将会花费很多时间扫描不是当前要查询的数据，为了扫描表中关心的一部分数据，在建表时引入了partition的概念。在查询时，可以通过Hive的分区机制来控制一次遍历的数据量。")]),t._v(" "),a("p",[t._v("​\t所以Hive中的各个维度的用户标签相关的表结构采用竖表的设计：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("IF")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("NOT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("EXISTS")]),t._v(" dws_user_profile_attritube\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    user_id STRING "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'用户id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("value")]),t._v("   STRING "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'标签权重'")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'人口属性维度用户标签表'")]),t._v("\nPARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    dt      STRING "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'日期分区'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    tag_id  STRING "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'标签id'")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h3",{attrs:{id:"标签汇聚"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#标签汇聚"}},[t._v("#")]),t._v(" 标签汇聚")]),t._v(" "),a("p",[t._v("在上面一节提到的案例中，用户的每个标签都插入到相应的分区下面，"),a("strong",[t._v("但是对一个用户来说，打在他身上的全部标签存储在不同的分区下面。为了方便分析和查询，需要将用户身上的标签做聚合处理")]),t._v("。标签汇聚后将一个每个用户身上的全量标签汇聚到一个字段中，表结构设计如下：")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("CREATE TABLE `dw.userprofile_userlabel_map_all`\n(\n    `userid`     string COMMENT 'userid',\n    `userlabels` map<string,string> COMMENT 'tagsmap',\n)\n    COMMENT 'userid 用户标签汇聚'\n    PARTITIONED BY ( `data_date` string COMMENT '数据日期')\n")])])]),a("p",[a("img",{attrs:{src:"https://gitee.com/joeyooa/data-images/raw/master/note/2021/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313039353630333134312e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c.png",alt:"68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313039353630333134312e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c"}})]),t._v(" "),a("ul",[a("li",[t._v("开发udf函数“cast_to_json”将用户身上的标签汇聚成json字符串，执行命令将按分区存储的标签进行汇聚：")])]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("insert overwrite table dw.userprofile_userlabel_map_all partition(data_date= \"data_date\")  \n  select userid,  \n         cast_to_json(concat_ws(',',collect_set(concat(labelid,':',labelweight)))) as userlabels\n      from “用户各维度的标签表” \n    where data_date= \" data_date \" \ngroup by userid\n")])])]),a("ul",[a("li",[a("p",[t._v("汇聚后用户标签的存储格式如图所示")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/joeyooa/data-images/raw/master/note/2021/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313039353733313536372e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c.png",alt:"68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313039353733313536372e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c"}})])]),t._v(" "),a("li",[a("p",[t._v("将用户身上的标签进行聚合便于查询和计算。例如，在画像产品中，输入用户id后通过直接查询该表，解析标签id和对应的标签权重后，即可在前端展示该用户的相关信息")])])]),t._v(" "),a("h3",{attrs:{id:"id-mapping"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#id-mapping"}},[t._v("#")]),t._v(" ID-MAPPING")]),t._v(" "),a("p",[t._v("开发用户标签的时候，有项非常重要的内容——"),a("strong",[t._v("ID-MApping")]),t._v("，"),a("strong",[t._v("即把用户不同来源的身份标识通过数据手段识别为同一个主体")]),t._v("。用户的属性、行为相关数据分散在不同的数据来源中，通过ID-MApping能够把用户在不同场景下的行为串联起来，消除数据孤岛。下图展示了用户与设备间的多对多关系。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://camo.githubusercontent.com/722c0b31c5d57bdf4d6d3987e48ea88edef2b19276dc39abca30c895737fb28d/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313130303031323438342e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c33646c61586870626c38304e444d784f44677a4d413d3d2c73697a655f31362c636f6c6f725f4646464646462c745f3730237069635f63656e746572",alt:"img"}})]),t._v(" "),a("p",[t._v("-举例来说，用户在未登录App的状态下，在App站内访问、搜索相关内容时，记录的是设备id（即cookieid）相关的行为数据。而用户在登录App后，访问、收藏、下单等相关的行为记录的是账号id（即userid）相关行为数据。虽然是同一个用户，但其在登录和未登录设备时记录的行为数据之间是未打通的。通过ID-MApping打通userid和cookieid的对应关系，可以在用户登录、未登录设备时都能捕获其行为轨迹。")]),t._v(" "),a("p",[t._v("下面通过一个案例介绍如何通过Hive的ETL工作完成ID-Mapping的数据清洗工作。")]),t._v(" "),a("p",[a("strong",[t._v("缓慢变化维是在维表设计中常见的一种方式，维度并不是不变的，随时间也会发生缓慢变化")]),t._v("。如用户的手机号、邮箱等信息可能会随用户的状态变化而改变，再如商品的价格也会随时间变化而调整上架的价格。因此在设计用户、商品等维表时会考虑用缓慢变化维来开发。同样，在设计ID-Mapping表时，由于一个用户可以在多个设备上登录，一个设备也能被多个用户登录，所以考虑用缓慢变化维表来记录这种不同时间点的状态变化（图3-9）。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://gitee.com/joeyooa/data-images/raw/master/note/2021/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313130303230333138332e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c.png",alt:"68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303232313130303230333138332e706e673f2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c"}})]),t._v(" "),a("p",[t._v("拉链表是针对缓慢变化维表的一种设计方式，记录一个事物从开始到当前状态的全部状态变化信息。")]),t._v(" "),a("p",[t._v("在上图中，通过拉链表记录了userid每一次关联到不同cookieid的情况。如userid为44463729的用户，在20190101这天登录某设备，在6号那天变换了另一个设备登录。其中start_date表示该记录的开始日期，end_date表示该记录的结束日期，当end_date为99991231时，表示该条记录当前仍然有效。")]),t._v(" "),a("p",[t._v("​    首先需要从埋点表和访问日志表里面获取到cookieid和userid同时出现的访问记录。下面案例中，"),a("code",[t._v("ods.page_event_log")]),t._v("是埋点日志表，"),a("code",[t._v("ods.page_view_log")]),t._v("是访问日志表，将获取到的userid和cookieid信息插入cookieid-userid关系表（"),a("code",[t._v("ods.cookie_user_signin")]),t._v("）中。代码执行如下：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" OVERWRITE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" ods"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cookie_user_signin "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("PARTITION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'${data_date}'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n         "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" userid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                cookieid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                from_unixtime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("eventtime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'yyyyMMdd'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" signdate\n           "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" ods"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("page_event_log      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 埋点表")]),t._v("\n           "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'${data_date}'")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v("\n         "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" userid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                cookieid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                from_unixtime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("viewtime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'yyyyMMdd'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" signdate\n           "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" ods"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("page_view_log   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 访问日志表")]),t._v("\n           "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'${data_date}'")]),t._v("\n           "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t\n")])])]),a("p",[t._v("​    创建ID-Map的拉链表，将每天新增到ods.cookie_user_signin表中的数据与拉链表历史数据做比较，如果有变化或新增数据则进行更新。")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("CREATE TABLE `dw.cookie_user_zippertable`(\n`userid` string COMMENT '账号ID', \n`cookieid` string COMMENT '设备ID', \n`start_date` string COMMENT 'start_date', \n`end_date` string COMMENT 'end_date')\nCOMMENT 'id-map拉链表'\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\n")])])]),a("p",[t._v("​    创建完成后，每天ETL调度将数据更新到ID-Mapping拉链表中，任务执行如下。")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" OVERWRITE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" dw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cookie_user_zippertable\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" \n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mobile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reg_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CASE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHEN")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'99991231'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("AND")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userid "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("IS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("NOT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("THEN")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'${data_date}'")]),t._v("\n                  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ELSE")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("end_date\n             "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("END")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" end_date\n       "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" dw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cookie_user_zippertable t1\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("LEFT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n                 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" ods"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cookie_user_signin\n                "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" data_date"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'${data_date}'")]),t._v("\n              "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("t2\n           "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userid "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userid\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v("\n       "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" userid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              cookieid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'${data_date}'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" start_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'99991231'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" end_date\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" ods"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cookie_user_signin\n       "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'${data_date\n       }'")]),t._v("\n          "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t\n")])])]),a("p",[t._v("​    数据写入表中，如上图所示。")]),t._v(" "),a("p",[t._v("​    对于该拉链表，可查看某日（如20190801）的快照数据。")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("select  * \nfrom dw.cookie_user_zippertable \nwhere start_date<='20190801' and end_date>='20190801'\n")])])]),a("p",[t._v("​    例如，目前存在一个记录userid和cookieid关联关系的表，但是为"),a("strong",[t._v("多对多")]),t._v("的记录（即一个userid对应多条cookieid记录，以及一条cookieid对应多条userid记录）。这里可以通过拉链表的日期来查看某个时间点userid对应的cookieid。查看某个用户（如32101029）在某天（如20190801）关联到的设备id")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" cookieid \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" dw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cookie_user_zippertable \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" userid"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'32101029'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" start_date"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'20190801'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" end_date"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'20190801'")]),t._v("\n")])])]),a("p",[t._v("​    可看出用户‘32101029’在历史中曾登录过3个设备，通过限定时间段可找到特定时间下用户的登录设备。")]),t._v(" "),a("p",[t._v("​    在开发中需要注意关于userid与cookieid的多对多关联，如果不加条件限制就做关联，很可能引起数据膨胀问题：")]),t._v(" "),a("p",[t._v("​    在实际应用中，会遇到许多需要将userid和cookieid做关联的情况。例如，需要在userid维度开发出该用户近30日的购买次数、购买金额、登录时长、登录天数等标签。前两个标签可以很容易地从相应的业务数据表中根据算法加工出来，而登录时长、登录天数的数据存储在相关日志数据中，日志数据表记录的userid与cookieid为多对多关系。因此在结合业务需求开发标签时，要确定好标签口径定义。")]),t._v(" "),a("h2",{attrs:{id:"_2-标签数据存储之mysql"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-标签数据存储之mysql"}},[t._v("#")]),t._v(" 2. 标签数据存储之MySQL")]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("        MySQL作为关系型数据库，在用户画像中可用于元数据管理、监控预警数据、结果集存储等应用中。下面详细介绍这3个应用场景。")]),t._v(" "),a("h3",{attrs:{id:"元数据管理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#元数据管理"}},[t._v("#")]),t._v(" 元数据管理")]),t._v(" "),a("p",[a("font",{attrs:{color:"tomato"}},[t._v("Hive适合于大数据量的批处理作业，对于量级较小的数据，MySQL具有更快的读写速度")]),t._v("。Web端产品读写MySQL数据库会有更快的速度，方便标签的定义、管理。")],1),t._v(" "),a("p",[t._v("        在介绍"),a("strong",[t._v("用户画像产品化")]),t._v("的时候，我们会介绍元数据录入和查询功能，将相应的数据存储在MySQL中。用户标签的元数据表结构设计也会在之后进行详细的介绍。这里给出了平台标签视图和元数据管理页面。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210221192719476.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210221192731558.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}})]),t._v(" "),a("p",[t._v("        平台标签视图中的标签元数据可以维护在MySQL关系数据库中，便于标签的编辑、查询和管理。")]),t._v(" "),a("h3",{attrs:{id:"监控预警数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#监控预警数据"}},[t._v("#")]),t._v(" 监控预警数据")]),t._v(" "),a("p",[t._v("        MySQL还可用于"),a("strong",[t._v("存储每天对ETL结果的监控信息")]),t._v("。从整个画像调度流的关键节点来看，需要监控的环节主要包括对每天标签的产出量、服务层数据同步情况的监控等主要场景。下图展示的是用户画像调度流主要模块。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210221193123119.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}})]),t._v(" "),a("h4",{attrs:{id:"_1-标签计算数据监控"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-标签计算数据监控"}},[t._v("#")]),t._v(" 1.标签计算数据监控")]),t._v(" "),a("p",[t._v("        主要用于监控每天标签ETL的数据量是否出现异常，如果有异常情况则发出告警邮件，同时暂停后面的ETL任务。")]),t._v(" "),a("h4",{attrs:{id:"_2-服务层同步数据监控"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-服务层同步数据监控"}},[t._v("#")]),t._v(" 2. 服务层同步数据监控")]),t._v(" "),a("p",[t._v("        服务层一般采用"),a("code",[t._v("HBase")]),t._v("、"),a("code",[t._v("Elasticsearch")]),t._v("等作为数据库"),a("strong",[t._v("存储标签数据")]),t._v("供线上调用，将标签相关数据从Hive数仓向服务层同步的过程中，有出现差错的可能，"),a("strong",[t._v("因此需要记录相关数据在Hive中的数量及同步到对应服务层后的数量")]),t._v("，如果数量不一致则触发告警。")]),t._v(" "),a("p",[t._v("        在对画像的数据监控中，调度流每跑完相应的模块，就将该模块的监控数据插入MySQL中，当校验任务判断达到触发告警阈值时，发送告警邮件，同时中断后续的调度任务。待开发人员解决问题后，可重启后续调度。")]),t._v(" "),a("h4",{attrs:{id:"_3-结果集存储"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-结果集存储"}},[t._v("#")]),t._v(" 3. 结果集存储")]),t._v(" "),a("p",[t._v("        结果集可以用来存储多维透视分析用的标签、圈人服务用的用户标签、当日记录各标签数量，用于校验标签数据是否出现异常。")]),t._v(" "),a("p",[t._v("        有的线上业务系统使用MySQL、Oracle等关系型数据库存储数据，如短信系统、消息推送系统等。在打通画像数据与线上业务系统时，需要考虑将存储在Hive中的用户标签相关数据同步到各业务系统，此时"),a("strong",[t._v("MySQL可用于存储结果集")]),t._v("。")]),t._v(" "),a("p",[a("font",{attrs:{color:"tomato"}},[t._v("Sqoop是一个用来将Hadoop和关系型数据库中的数据相互迁移的工具。它可以将一个关系型数据库（如MySQL、Oracle、PostgreSQL等）中的数据导入Hadoop的HDFS中，也可以将HDFS中的数据导入关系型数据库中")])],1),t._v(" "),a("p",[t._v("        下面通过一个案例来讲解如何使用Sqoop将Hive中的标签数据迁移到MySQL中。")]),t._v(" "),a("p",[a("font",{attrs:{color:"RoyalBlue"}},[t._v("电商、保险、金融等公司的客服部门的日常工作内容之一是对目标用户群（如已流失用户、高价值用户等）进行主动外呼，以此召回用户来平台进行购买或复购")]),t._v("。这里可以借助用户画像系统实现该功能。")],1),t._v(" "),a("p",[t._v("        将Hive中存储的与用户身份相关的数据同步到客服系统中，首先在Hive中建立一张记录用户身份相关信息的表（例如"),a("code",[t._v("dw.userprofile_userservice_all")]),t._v("）。设置日期分区以满足按日期选取当前人群的需要。")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("dw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userprofile_userservice_all "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" string "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'userid'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("user_sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" string "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'user_sex'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("city"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" string "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'city'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("payid_money"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" string "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'payid_money'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("payid_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" string "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'payid_num'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("latest_product"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" string "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'latest_product'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("date")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" string "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'date'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("data_status"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" string "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data_status'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'userid 用户客服数据'")]),t._v("\nPARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" string "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'数据日期'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("        在MySQL中建立一张用于接收同步数据的表（"),a("code",[t._v("userservice_data")]),t._v("）。")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("userservice_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DEFAULT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'用户id'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("user_sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("NOT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'用户性别'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("city"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DEFAULT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'城市'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("payid_money"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DEFAULT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'消费金额'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("payid_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DEFAULT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'消费次数'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("latest_product"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DEFAULT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'最近购买产品'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("date")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("NOT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'传输日期'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("data_status"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("varchar")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DEFAULT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'0'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'0:未传输,1:传输中,2:成功,3:失败'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("PRIMARY")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("KEY")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ENGINE")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("InnoDB")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AUTO_INCREMENT")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2261628")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DEFAULT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CHARSET")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("utf8 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMMENT")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'用户客服数据表'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("        通过Python脚本调用shell命令，将Hive中的数据同步到MySQL中。执行如下脚本：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# -*- coding: utf-8 -*-")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MySQLdb\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\ndef export_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hive_tab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(":\n    sqoop_command "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"sqoop export --connect jdbc:mysql://10.xxx.xxx.xxx:3306/mysql_database --username username --password password  --table mysql_table --export-dir hdfs://nameservice1/user/hive/warehouse\n/dw.db/"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" hive_tab "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/data_date="')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("\" --input-fields-terminated-by '\\001'\"")]),t._v("\n    os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("system"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sqoop_command"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sqoop_command"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'__main__'")]),t._v(":\n    export_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dw.userprofile_userservice_all"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'20181201'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("        其中用到了 sqoop 从 Hive 导出数据到 MySQL 的命令：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("sqoop export\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--connect 指定JDBC连接字符串,包括IP 端口 数据库名称 \\")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--username  JDBC连接的用户名\\")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--passowrd  JDBC连接的密码\\")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--table  表名\\")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--export-dir  导出的Hive表, 对应的是HDFS地址 \\")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--input fileds-terminated-by ‘,’ 分隔符号")]),t._v("\n")])])]),a("p",[t._v("        不熟悉Sqoop使用的小伙伴可以去看我的这篇文章"),a("a",{attrs:{href:"https://alice.blog.csdn.net/article/details/113065391",target:"_blank",rel:"noopener noreferrer"}},[t._v("《硬核 | Sqoop入门指南》"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("        同步后 MySQL中的数据如图所示")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210221195149779.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"_3-标签数据存储之hbase"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-标签数据存储之hbase"}},[t._v("#")]),t._v(" 3. 标签数据存储之HBase")]),t._v(" "),a("h3",{attrs:{id:"_1-hbase简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-hbase简介"}},[t._v("#")]),t._v(" 1. HBase简介")]),t._v(" "),a("p",[a("font",{attrs:{color:"Tomato"}},[a("strong",[t._v("HBase是一个高性能、列存储、可伸缩、实时读写的分布式存储系统")])]),t._v("，同样运行在HDFS之上。与Hive不同的是，HBase能够在数据库上实时运行，而不是跑MapReduce任务，适合进行大数据的"),a("strong",[t._v("实时查询")]),t._v("。")],1),t._v(" "),a("p",[t._v("        画像系统中每天在Hive里跑出的结果集数据可同步到 HBase数据库 ，用于线上实时应用的场景。")]),t._v(" "),a("p",[t._v("        下面介绍几个基本概念：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("row key")]),t._v("：用来表示唯一一行记录的"),a("strong",[t._v("主")]),t._v("键，HBase的数据是按照 row key 的"),a("strong",[t._v("字典顺序")]),t._v("进行全局排列的。访问HBase中的行只有3种方式：")]),t._v(" "),a("li",[a("strong",[t._v("通过单个row key访问")]),t._v("；")]),t._v(" "),a("li",[a("strong",[t._v("通过row key的正则访问")]),t._v("；")]),t._v(" "),a("li",[a("strong",[t._v("全表扫描")])])]),t._v(" "),a("p",[t._v("        由于HBase通过 rowkey 对数据进行检索，而rowkey 由于长度限制的因素不能将很多查询条件拼接在 rowkey 中，因此 HBase 无法像关系数据库那样根据多种条件对数据进行筛选。一般地，HBase需建立"),a("strong",[t._v("二级索引")]),t._v("来满足根据复杂条件查询数据的需求。")]),t._v(" "),a("p",[t._v("        Rowkey设计时需要遵循三大原则：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("唯一性原则")]),t._v("：rowkey需要保证唯一性，不存在重复的情况。在画像中一般使用用户id作为rowkey")]),t._v(" "),a("li",[a("strong",[t._v("长度原则")]),t._v("：rowkey的长度一般为10-100bytes")]),t._v(" "),a("li",[a("strong",[t._v("散列原则")]),t._v("：rowkey的散列分布有利于数据均衡分布在每个RegionServer，可实现负载均衡")])]),t._v(" "),a("p",[t._v("--  columns family：指列簇，HBase中的每个列都归属于某个列簇。列簇是表的schema的一部分，必须在使用表之前定义。划分columns family的原则如下：")]),t._v(" "),a("ul",[a("li",[t._v("是否具有相似的数据格式")]),t._v(" "),a("li",[t._v("是否具有相似的访问类型")])]),t._v(" "),a("p",[t._v("        常用的增删改查命令如下：")]),t._v(" "),a("p",[t._v("        1）创建一个表，指定表名和列簇名：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<table name>'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<column family>'")]),t._v("\n")])])]),a("p",[t._v("        2）扫描表中数据，并显示其中的10条记录：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("scan  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<table name>'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("{"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("LIMIT")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("}\n")])])]),a("p",[t._v("        3）使用get命令读取数据：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("get  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<table name>'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'row1'")]),t._v("\n")])])]),a("p",[t._v("        4）插入数据：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("put  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<table name>'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'row1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<colfamily:colname>'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<value>'")]),t._v("\n")])])]),a("p",[t._v("        5）更新数据：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("put  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<table name>'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'row '")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Column family:column name'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'new value'")]),t._v("\n")])])]),a("p",[t._v("        6）在删除表之前先将其"),a("strong",[t._v("禁用")]),t._v("，然后删除：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("disable")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<table name>'")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("drop")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<table name>'")]),t._v("\n")])])]),a("p",[t._v("        下面通过一个案例来介绍HBase在画像系统中的应用场景和工程化实现方式。")]),t._v(" "),a("h3",{attrs:{id:"_2-应用场景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-应用场景"}},[t._v("#")]),t._v(" 2. 应用场景")]),t._v(" "),a("p",[t._v("        渠道运营人员为促进未注册的新安装用户注册、下单，计划通过App首页弹窗（如下图所示）发放红包或优惠券的方式进行引导。在该场景中可通过画像系统实现对应功能。")]),t._v(" "),a("p",[t._v("        业务逻辑上，渠道运营人员通过组合用户标签（如“未注册用户”和“安装距今天数”小于××天）筛选出对应的用户群，然后选择将对应人群推送到“广告系统”，这样每天画像系统的ETL调度完成后对应人群数据就被推送到HBase数据库进行存储。满足条件的新用户来访App时，由在线接口读取HBase数据库，在查询到该用户时为其推送该弹窗。")]),t._v(" "),a("p",[t._v("        下面通过某工程案例来讲解HBase在该触达用户场景中的应用方式。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210222223852955.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:""}})]),t._v(" "),a("h3",{attrs:{id:"_3-工程化案例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-工程化案例"}},[t._v("#")]),t._v(" 3. 工程化案例")]),t._v(" "),a("p",[a("strong",[t._v("运营人员在画像系统中根据业务规则定义组合用户标签筛选出用户群，并将该人群上线到广告系统中")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210222224208179.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:""}})]),t._v(" "),a("p",[t._v("        在业务人员配置好规则后，下面我们来看在数据调度层面是如何运行的。")]),t._v(" "),a("p",[t._v("        用户标签数据经过ETL将每个用户身上的标签聚合后插入到目标表中，如"),a("code",[t._v("dw.userprofile_userlabel_map_all")]),t._v("。聚合后数据存储为每个用户id，以及他身上对应的标签集合，数据格式如图所示：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210222230712227.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}})]),t._v(" "),a("p",[t._v("        接下来需要将 Hive 中的数据导入HBase，便于线上接口实时调用库中数据。")]),t._v(" "),a("p",[a("strong",[t._v("HBase的服务器体系结构遵循主从服务器架构")]),t._v("（如图所示），同一时刻只有一个"),a("strong",[t._v("HMaster")]),t._v("处于活跃状态，当活跃的Master挂掉后，Backup HMaster自动接管整个HBase集群。在同步数据前，首先需要判断HBase的当前活跃节点是哪台机器。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210222230813445.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}})]),t._v(" "),a("p",[t._v("        执行如下脚本：")]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断活跃节点")]),t._v("\nglobal activenode\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token for-or-select variable"}},[t._v("node")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"10.xxx.xx.xxx"')]),t._v(","),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"10.xxx.xx.xxx"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(":   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 两台机器作为Master，判断哪台HMaster处于活跃状态")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("command")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"curl http://"')]),t._v("+ str"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("node"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" + "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('":9870/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus"')]),t._v("\n    status "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os.popen"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("command"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(".read"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    print"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"HBase Master status: "')]),t._v(".format"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("status"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"active"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" status"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(":\n        activenode "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node\n")])])]),a("p",[t._v("        执行完毕后，可通过返回的“State”字段判断当前节点状态（活跃为“active”，不活跃为“standby”），如图所示。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210222230930222.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}})]),t._v(" "),a("p",[t._v("        为避免数据都写入一个region，造成HBase的数据倾斜问题。在当前HMaster活跃的节点上，创建预分区表：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'userprofile_labels'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" { NAME "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"f"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" BLOCKCACHE "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"true"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" BLOOMFILTER "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ROWCOL"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" COMPRESSION "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'snappy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" IN_MEMORY "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'true'")]),t._v(" }"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" {NUMREGIONS "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("SPLITALGO "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'HexStringSplit'")]),t._v("}\n")])])]),a("p",[t._v("        将待同步的数据写入HFile，HFile中的数据以 key-value 键值对方式存储，然后将 HFile 数据使用 BulkLoad 批量写入 HBase 集群中。Scala脚本执行如下：")]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token function"}},[t._v("import")]),t._v(" org.apache.hadoop.fs."),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("FileSystem, Path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("import")]),t._v(" org.apache.hadoop.HBase.client.ConnectionFactory\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("import")]),t._v(" org.apache.hadoop.HBase."),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("HBaseConfiguration, KeyValue, TableName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("import")]),t._v(" org.apache.hadoop.HBase.io.ImmutableBytesWritable\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("import")]),t._v(" org.apache.hadoop.HBase.mapreduce."),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("HFileOutputFormat2, LoadIncrementalHFiles"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("import")]),t._v(" org.apache.hadoop.HBase.util.Bytes\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("import")]),t._v(" org.apache.hadoop.mapreduce.Job\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("import")]),t._v(" org.apache.spark.sql.SparkSession\n\nobject Hive2HBase "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  def main"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args: Array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("String"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(": Unit "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      // 传入日期参数 和 当前活跃的master节点\n    val data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    val node "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   //当前活跃的节点ip\n\n    val spark "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkSession\n      .builder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      .appName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Hive2HBase"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      .config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark.serializer"')]),t._v(","),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org.apache.spark.serializer.KryoSerializer"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      .config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark.storage.memoryFraction"')]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"0.1"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      .config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark.shuffle.memoryFraction"')]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"0.7"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      .config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark.memory.useLegacyMode"')]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"true"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      .enableHiveSupport"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      .getOrCreate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n       //创建HBase的配置\n    val conf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" HBaseConfiguration.create"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        conf.set"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"HBase.zookeeper.quorum"')]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"10.xxx.xxx.xxx,10.xxx.xxx.xxx"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        conf.set"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"HBase.zookeeper.property.clientPort"')]),t._v(", "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"8020"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    //为了预防hfile文件数过多无法进行导入，设置参数值\n    conf.setInt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"HBase.hregion.max.filesize"')]),t._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10737418240")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    conf.setInt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"HBase.mapreduce.bulkload.max.hfiles.perRegion.perFamily"')]),t._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3200")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      \n    val Data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark.sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("\"select userid,userlabels from dw.userprofile_usergroup_labels_all where data_date='"),a("span",{pre:!0,attrs:{class:"token variable"}},[t._v("${data_date}")]),t._v("'\"")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    val dataRdd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Data.rdd.flatMap"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("    \n      val rowkey "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row.getAs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("String"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userid"')]),t._v(".toLowerCase"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      val tagsmap "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row.getAs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("String, Object"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userlabels"')]),t._v(".toLowerCase"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      val sbkey "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" new StringBuffer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  // 对MAP结构转化 a-"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("b  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),t._v("\n      val sbvalue "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" new StringBuffer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token variable"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("((")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" tagsmap"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("{\n        sbkey.append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(' "'),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v('"'),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        val labelght "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" if "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("value "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(' ""'),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v('{\n          "'),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("999999")]),t._v('"\n        } else {\n          value\n        }\n        sbvalue.append'),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labelght "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(' "'),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v('"'),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      }\n      val item "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sbkey.substring"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("sbkey.length "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      val score "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sbvalue.substring"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("sbvalue.length "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      Array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rowkey"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v('"f"'),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v('"i"'),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("item"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")])]),t._v(",\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rowkey,"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"f"')]),t._v(","),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"s"')]),t._v(",score"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    // 将rdd转换成HFile需要的格式\n    val rdds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dataRdd.filter"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("x._1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" null"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(".sortBy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x._1,x._2._1, x._2._2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")]),t._v(".map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      //KeyValue的实例为value\n      val rowKey "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Bytes.toBytes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x._1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      val family "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Bytes.toBytes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x._2._1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      val colum "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Bytes.toBytes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x._2._2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      val value "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Bytes.toBytes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x._2._3.toString"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("new ImmutableBytesWritable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rowKey"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(", new KeyValue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rowKey, family, colum, value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    //文件保存在hdfs的位置\n    val locatedir "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hdfs://"')]),t._v(" + node.toString + "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('":8020/user/bulkload/hfile/usergroup_HBase_"')]),t._v(" + data_date\n\n    //在locatedir生成的Hfile文件\n    rdds.saveAsNewAPIHadoopFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("locatedir,\n      classOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("ImmutableBytesWritable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(",\n      classOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("KeyValue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(",\n      classOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("HFileOutputFormat2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(",\n      conf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    //HFile导入到HBase\n    val load "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" new LoadIncrementalHFiles"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    //HBase的表名\n    val tableName "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userprofile_labels"')]),t._v("\n    //创建HBase的链接,利用默认的配置文件,读取HBase的master地址\n    val conn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ConnectionFactory.createConnection"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    //根据表名获取表\n    val table "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" conn.getTable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TableName.valueOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tableName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")]),t._v("\n\n    try "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      //获取HBase表的region分布\n      val regionLocation "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" conn.getregionLocation"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TableName.valueOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tableName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")]),t._v("\n      //创建一个hadoop的mapreduce的job\n      val job "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Job.getInstance"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      //设置job名称，任意命名\n      job.setJobName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Hive2HBase"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      //输出文件的内容KeyValue\n      job.setMapOutputValueClass"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("KeyValue"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      //设置文件输出key, outkey要用ImmutableBytesWritable\n      job.setMapOutputKeyClass"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("ImmutableBytesWritable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      //配置HFileOutputFormat2的信息\n      HFileOutputFormat2.configureIncrementalLoad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job, table, regionLocation"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      //开始导入\n      load.doBulkLoad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("new Path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("locatedir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(", conn.getAdmin, table, regionLocation"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" finally "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      table.close"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      conn.close"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    spark.close"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("p",[t._v("        提交Spark任务，将HFile中数据bulkload到HBase中。执行完成后，可以在HBase中看到该数据已经写入“userprofile_labels”中")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210222231310996.png",alt:""}}),t._v(" "),a("font",{attrs:{color:"\ttomato"}},[t._v("在线接口在查询HBase中数据时，由于HBase无法像关系数据库那样根据多种条件对数据进行筛选")]),t._v("（类似SQL语言中的where筛选条件）。"),a("font",{attrs:{color:"\ttomato"}},[t._v("一般地HBase需建立"),a("strong",[t._v("二级索引")]),t._v("来满足根据复杂条件查询数据的需求")]),t._v("，本案中选用 "),a("strong",[t._v("Elasticsearch")]),t._v(" 存储HBase索引数据")],1),t._v(" "),a("p",[t._v("        在组合标签查询对应的用户人群场景中，首先通过组合标签的条件在 Elasticsearch 中查询对应的索引数据，然后通过索引数据去 HBase中批量获取 rowkey 对应的数据（"),a("strong",[t._v("Elasticsearch中的documentid和HBase中的rowkey都设计为用户id")]),t._v("）")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210222231635143.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}}),t._v("\n        为了避免从 Hive 向 HBase 灌入数据时缺失，在向HBase数据同步完成后，还需要"),a("strong",[t._v("校验HBase和Hive中数据量是否一致")]),t._v("，如出现较大的波动则发送告警信息。")]),t._v(" "),a("p",[t._v("        下面通过Python脚本来看该HBase状态表数据校验逻辑：")]),t._v(" "),a("div",{staticClass:"language-powershell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-powershell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查询Hive中数据")]),t._v("\ndef check_Hive_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(":\n      r = os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("popen"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Hive -S -e\\"')]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("select")]),t._v(" count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" dw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userprofile_usergroup_labels_all where data_date="),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\"+data_date+\"'")]),t._v("\\"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('""')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      Hive_userid_count = r"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      r"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      Hive_count = str"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Hive_userid_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n      print "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Hive_result: "')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" str"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Hive_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      print "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Hive select finished!"')]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查询HBase中数据 ")]),t._v("\ndef check_HBase_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(":\n      r = os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("popen"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("\"HBase  org.apache.hadoop.HBase.mapreduce.RowCounter 'userprofile_labels'\\\"")]),t._v(" 2>&1 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("|")]),t._v("grep ROWS"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('")\n      HBase_count = r.read().strip()[5:]\n      r.close()\n      print "')]),t._v("HBase result: "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('" + str(HBase_count)\n      print "')]),t._v("HBase "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("select")]),t._v(" finished"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\n\n# 连接 DB,将查询结果插入表\ndb = MySQLdb.connect(host="')]),t._v("xx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xx"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('",port=3306,user="')]),t._v("username"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('", passwd="')]),t._v("password"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('", db="')]),t._v("xxx"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('", charset="')]),t._v("utf8"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('")\ncursor = db.cursor()\ncursor.execute("')]),t._v("INSERT INTO service_monitor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" service_type"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Hive_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" HBase_count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" VALUES"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\"+datestr_+\"'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'advertisement'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"+str(Hive_userid_count)+"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"+str(HBase_count)+"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v('"'),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("commit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("        本案例中将 userid 作为 rowkey 存入HBase，一方面在组合标签的场景中可以支持条件查询多用户人群，另一方面可以支持单个用户标签的查询，例如查看某 id 用户身上的标签，以便运营人员决定是否对其进行运营操作。")]),t._v(" "),a("p",[a("strong",[t._v("HBase在离线数仓环境的服务架构如图所示")]),t._v("：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210222232403415.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}})]),t._v(" "),a("h1",{attrs:{id:""}},[a("a",{staticClass:"header-anchor",attrs:{href:"#"}},[t._v("#")])]),t._v(" "),a("h2",{attrs:{id:"_4-标签数据存储之elasticsearch"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-标签数据存储之elasticsearch"}},[t._v("#")]),t._v(" 4. 标签数据存储之Elasticsearch")]),t._v(" "),a("h3",{attrs:{id:"elasticsearch简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#elasticsearch简介"}},[t._v("#")]),t._v(" Elasticsearch简介")]),t._v(" "),a("p",[a("font",{attrs:{color:"tomato"}},[t._v("Elasticsearch 是一个开源的分布式全文检索引擎，可以近乎实时地存储、检索数据")]),t._v("。而且"),a("font",{attrs:{color:"RoyalBlue"}},[t._v("可扩展性很好，可以扩展到上百台服务器，处理PB级别的数据")]),t._v("。对于"),a("strong",[t._v("用户标签查询")]),t._v("、"),a("strong",[t._v("用户人群计算")]),t._v("、"),a("strong",[t._v("用户群多维透视分析")]),t._v("这类对响应时间要求较高的场景，也可以考虑选用Elasticsearch进行存储。")],1),t._v(" "),a("p",[t._v("        Elasticsearch是面向文档型数据库，"),a("strong",[t._v("一条数据在这里就是一个文档")]),t._v("，用 json 作为文档格式。为了更清晰地理解 Elasticsearch 查询的一些概念，将其和关系型数据库的类型进行对照。")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",[t._v("Elasticsearch")]),t._v(" "),a("th",[t._v("MySQL")]),t._v(" "),a("th")])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("index")]),t._v(" "),a("td",[t._v("database")]),t._v(" "),a("td",[t._v("数据库")])]),t._v(" "),a("tr",[a("td",[t._v("type")]),t._v(" "),a("td",[t._v("table")]),t._v(" "),a("td",[t._v("表")])]),t._v(" "),a("tr",[a("td",[t._v("document")]),t._v(" "),a("td",[t._v("row")]),t._v(" "),a("td",[t._v("行")])]),t._v(" "),a("tr",[a("td",[t._v("mapping")]),t._v(" "),a("td",[t._v("column")]),t._v(" "),a("td",[t._v("列")])]),t._v(" "),a("tr",[a("td",[t._v("GET http://...")]),t._v(" "),a("td",[t._v("SELECT * FROM ...")]),t._v(" "),a("td",[t._v("查询数据")])]),t._v(" "),a("tr",[a("td",[t._v("PUT http://...")]),t._v(" "),a("td",[t._v("UPDATE table SET...")]),t._v(" "),a("td",[t._v("插入数据")])])])]),t._v(" "),a("p",[t._v("        在关系型数据库中查询数据时可通过选中数据库、表、行、列来定位所查找的内容，在Elasticsearch中通过"),a("strong",[t._v("索引（index）、类型（type）、文档（document）、字段")]),t._v("来定位查找内容。一个Elasticsearch集群可以包括多个索引（数据库），也就是说，其中包含了很多类型（表），这些类型中包含了很多的文档（行），然后每个文档中又包含了很多的字段（列）。Elasticsearch的交互可以使用Java API，也可以使用 "),a("strong",[t._v("HTTP")]),t._v(" 的"),a("strong",[t._v("RESTful API")]),t._v("方式。")]),t._v(" "),a("h3",{attrs:{id:"应用场景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#应用场景"}},[t._v("#")]),t._v(" 应用场景")]),t._v(" "),a("p",[t._v("        在上一节的内容中，我们谈到基于 HBase 的存储方案并没有解决数据的 "),a("strong",[t._v("高效检索")]),t._v(" 问题。在实际应用中，经常有"),a("strong",[t._v("根据特定的几个字段进行组合后检索")]),t._v("的应用场景，而 "),a("font",{attrs:{color:"blue"}},[t._v("HBase 采用 rowkey 作为一级索引，不支持多条件查询")]),t._v("，如果要对库里的非 rowkey 进行数据检索和查询，往往需要通过 MapReduce 等分布式框架进行计算，时间延迟上会比较高，"),a("strong",[t._v("难以同时满足用户对于复杂条件查询和高效率响应这两方面的需求")]),t._v("。")],1),t._v(" "),a("p",[a("font",{attrs:{color:"\tTomato"}},[t._v("为了既能支持对数据的"),a("strong",[t._v("高效查询")]),t._v("，同时也能支持通过条件筛选进行复杂查询，需要在HBase上构建"),a("strong",[t._v("二级索引")]),t._v("，以满足对应的需要")]),t._v("。在本案中我们采用"),a("strong",[t._v("Elasticsearch")]),t._v("存储 HBase 的索引信息，以支持复杂高效的查询功能。")],1),t._v(" "),a("p",[t._v("        主要查询过程包括：")]),t._v(" "),a("p",[t._v("        1）在Elasticsearch中存放用于检索条件的数据，并将rowkey 也存储进去；")]),t._v(" "),a("p",[t._v("        2）使用Elasticsearch的 API 根据组合标签的条件查询出rowkey的集合；")]),t._v(" "),a("p",[t._v("        3）使用上一步得到的 rowkey 去HBase数据库查询对应的结果\n"),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210224004308194.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        HBase存储数据的索引放在Elasticsearch中，实现了"),a("strong",[t._v("数据和索引的分离")]),t._v("。在Elasticsearch中"),a("code",[t._v("documentid")]),t._v("是文档的唯一标识，在HBase中"),a("code",[t._v("rowkey")]),t._v("是记录的唯一标识。在工程实践中，两者可同时选用用户在平台上的唯一标识（如userid或deviceid）作为rowkey或documentid，进而解决 HBase 和 Elasticsearch 索引关联的问题。")]),t._v(" "),a("p",[t._v("        下面通过使用 Elasticsearch 解决用户人群计算和分析应用场景的案例来了解这一过程。")]),t._v(" "),a("p",[t._v("        对汇聚后的用户标签表dw."),a("code",[t._v("userprofile_userlabel_map_all")]),t._v("中的数据进行清洗，过滤掉一些无效字符，达到导入Elasticsearch的条件，如图所示：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210224230659422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        然后将dw."),a("code",[t._v("userprofile_userlabel_map_all")]),t._v("数据写入"),a("strong",[t._v("Elasticsearch")]),t._v(" 中，Scala代码如下：")]),t._v(" "),a("div",{staticClass:"language-scala extra-class"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("object")]),t._v(" HiveDataToEs "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" main"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" Array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("Unit")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" spark "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkSession"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("builder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("AppName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"EsData"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark.serializer"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org.apache.spark.serializer.KryoSerializer"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark.dynamicAllocation.enabled"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"false"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es.index.auto.create"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"true"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es.nodes"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"10.xx.xx.xx"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es.batch.write.retry.count"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"3"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 默认重试3次")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es.batch.write.retry.wait"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"5"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 每次重试等待时间为5秒")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"thread_pool.write.queue_size"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"1000"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"thread_pool.write.size"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"50"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"thread_pool.write.type"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"fixed"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   \n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es.batch.size.bytes"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"20mb"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es.batch.size.entries"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2000"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  \n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es.http.timeout"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"100m"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("enableHiveSupport"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getOrCreate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" data_date  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("toString\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" hiveDF "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n      s"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n         | SELECT userid, tagsmap FROM dw.userprofile_userlabel_map_all where data_date = \'${data_date}\'\n      """')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stripMargin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// dw.userprofile_userlabel_map_all 是聚合用户标签的表")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" rdd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hiveDF"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      row "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" userid "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getAs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userid"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" userlabels "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getAs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("Map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Object"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userlabels"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        Map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userid"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" userid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userlabels"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" userlabels"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    EsSpark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("saveToEs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rdd "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userprofile/tags"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es.mApping.id"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userid"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   \n    spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stop"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("p",[t._v("        工程依赖如下：")]),t._v(" "),a("div",{staticClass:"language-xml extra-class"},[a("pre",{pre:!0,attrs:{class:"language-xml"}},[a("code",[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("dependency")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("groupId")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("org.elasticsearch"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("groupId")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("artifactId")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("elasticsearch-hadoop"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("artifactId")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("version")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("6.4.2"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("version")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token tag"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("dependency")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])]),a("p",[t._v("        将该工程打包之后提交任务，传入日期分区参数 “20190101”执行。提交命令"),a("code",[t._v("“spark-submit--class com.example.HiveDataToEs--master yarn--deploy-mode client--executor-memory 2g--num-executors 50--driver-memory 3g--executor-cores 2 spark-hive-to-es.jar 20190101”")])]),t._v(" "),a("p",[t._v("        任务执行完毕后，当日 userid 维度的用户标签数据全部导入Elasticsearch中。使用RESTfulAPI查询包含某个标签的用户量，可实时得到返回结果。")]),t._v(" "),a("div",{staticClass:"language-json extra-class"},[a("pre",{pre:!0,attrs:{class:"language-json"}},[a("code",[t._v("# 查询命令\nGET userprofile/tags/_search\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token property"}},[t._v('"size"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token property"}},[t._v('"aggs"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token property"}},[t._v('"tagcounts"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token property"}},[t._v('"terms"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token property"}},[t._v('"field"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tags.ACTION_U_01_003"')]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210224231410177.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:"在这里插入图片描述"}}),t._v("\n        从返回结果中可以看到，用户总量（total）为100000000人，包含标签“"),a("code",[t._v("ACTION_U_01_003")]),t._v("”的用户有2500000人（doc_count）。")]),t._v(" "),a("p",[t._v("        查询人群 index 查看标签总量：")]),t._v(" "),a("div",{staticClass:"language-json extra-class"},[a("pre",{pre:!0,attrs:{class:"language-json"}},[a("code",[t._v("# 查询命令\nGET userprofile/_search \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token property"}},[t._v('"query"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token property"}},[t._v('"match_all"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),a("p",[t._v("        查询结果如图所示：\n"),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/2021022423172936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:"在这里插入图片描述"}}),t._v("\n        在人群的计算和分析场景中，经过产品的迭代，前期采用 "),a("strong",[t._v("Impala")]),t._v(" 进行计算，一般耗费几十秒到几分钟的时间，在使用 Elasticsearch 后，实现了对人群计算的秒级响应。")]),t._v(" "),a("h3",{attrs:{id:"工程化案例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#工程化案例"}},[t._v("#")]),t._v(" 工程化案例")]),t._v(" "),a("p",[t._v("        下面通过一个工程案例来讲解实现画像产品中“用户人群”和“人群分析”功能对用户群计算秒级响应的一种解决方案。")]),t._v(" "),a("p",[t._v("        在每天的 ETL 调度中，需要将 Hive 计算的标签数据导入Elasticsearch中。如图所示，在标签调度完成且通过校验后（图中的“标签监控预警”任务执行完成后），将标签数据同步到Elasticsearch中。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210224232257644.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        在与 Elasticsearch 数据同步完成并通过校验后，向在  MySQL 中维护的状态表中插入一条状态记录，表示当前日期的 Elasticsearch 数据可用，线上计算用户人群的接口则读取最近日期对应的数据。如果某天因为调度延迟等方面的原因，没有及时将当日数据导入Elasticsearch中，接口也能读取最近一天对应的数据，是一种可行的灾备方案。")]),t._v(" "),a("p",[t._v("        例如，数据同步完成后向MySQL状态表“elasticsearch_state”中插入记录（如图所示），当日数据产出正常时，state字段为“0”，产出异常时为“1”。图3-29中1月20日导入的数据出现异常，则“state”状态字段置1，线上接口扫描该状态记录位后不读取1月20日数据，而是取用最近的1月19日数据。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210224232536590.png#pic_center",alt:"在这里插入图片描述"}}),t._v("\n        为了避免从 Hive 向 Elasticsearch 中灌入数据时发生数据缺失，在向状态表更新状态位前需要校验 Elasticsearch 和 Hive 中的数据量是否一致。下面通过Python脚本来看"),a("strong",[t._v("数据校验逻辑")]),t._v("：")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查询Hive中的数据")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("monitor_hive_data")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    hive_user "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("\" select count(1) from dw.userprofile_userlabel_map_all where data_date='{}' \"")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    user_count "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("popen"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hive -S -e \\""')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" hive_user "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\""')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" user_count\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查询es中的数据")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("monitor_es_data")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    userid_search "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"curl http://10.xxx.xxx.xxx:9200/_cat/count/"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"_userid/"')]),t._v("\n    userid_num "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("popen"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("userid_search"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" userid_num\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 比较Hive和es中的数据，如通过校验，更新MySQL状态位")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("update_es_data")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n    data_date: 查询数据日期\n    '''")]),t._v("\n    esdata "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" monitor_es_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查询es中的数据")]),t._v("\n    hivedata "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" monitor_hive_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查询Hive中的数据")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"esdata ======>{}"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("esdata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hivedata ======>{}"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hivedata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 更新MySQL状态位 ")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("esdata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" hivedata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        db "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MySQLdb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("connect"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("host"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"10.xx.xx.xx"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" port"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3306")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" user"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"username"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" passwd"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"password"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                             db"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userprofile"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" charset"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"utf8"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        cursor "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" db"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cursor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("try")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            select_command "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"INSERT INTO `elasticsearch_state` VALUES (\'"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_date"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("\"', 'elasticsearch', '0', '2');\"")]),t._v("\n            cursor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("execute"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("select_command"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            db"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("commit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("except")]),t._v(" Exception "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" e"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            db"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rollback"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n           exit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("        上面介绍了在工程化调度流中何时将Hive中的用户标签数据灌入Elasticsearch中，之后业务人员在画像产品端计算人群或透视分析人群时（如图所示），\n"),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210224232746353.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:"在这里插入图片描述"}})]),t._v(" "),a("p",[t._v("通过RESTful API访问 Elasticsearch 进行计算")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210224232806313.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:"在这里插入图片描述"}})]),t._v(" "),a("blockquote",[a("p",[t._v("        结合前面几期文章，分别为大家讲解了使用 Hive、MySQL、HBase 和 Elasticsearch 存储标签数据的解决方案，包括："),a("font",{attrs:{color:"RoyalBlue"}},[t._v("Hive存储数据相关标签表、人群计算表的表结构设计以及ID-Mapping的一种实现方式；MySQL存储标签元数据、监控数据及结果集数据；HBase存储线上接口实时调用的数据；Elasticsearch存储标签用于人群计算和人群多维透视分析")]),t._v("。存储过程中涉及如下相关表。")],1),t._v(" "),a("ul",[a("li",[a("code",[t._v("dw.userprofile_attritube_all")]),t._v("：存储人口属性维度的标签表；")]),t._v(" "),a("li",[a("code",[t._v("dw.userprofile_action_all")]),t._v("：存储行为属性维度的标签表；")]),t._v(" "),a("li",[a("code",[t._v("dw.userprofile_consume_all")]),t._v("：存储用户消费维度的标签表；")]),t._v(" "),a("li",[a("code",[t._v("dw.userprofile_riskmanage_all")]),t._v("：存储风险控制维度的标签表；")]),t._v(" "),a("li",[a("code",[t._v("dw.userprofile_social_all")]),t._v("：存储社交属性维度的标签表；")]),t._v(" "),a("li",[a("code",[t._v("dw.userprofile_userlabel_map_all")]),t._v("：汇聚用户各维度标签的表；")]),t._v(" "),a("li",[a("code",[t._v("dw.userprofile_usergroup_labels_all")]),t._v("：存储计算后人群数据的表。")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);