(window.webpackJsonp=window.webpackJsonp||[]).push([[113],{492:function(t,s,a){"use strict";a.r(s);var n=a(30),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"用户画像-开发性能调优"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#用户画像-开发性能调优"}},[t._v("#")]),t._v(" 用户画像：开发性能调优")]),t._v(" "),a("h2",{attrs:{id:"前言"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#前言"}},[t._v("#")]),t._v(" 前言")]),t._v(" "),a("p",[t._v("        马上就快过年了，祝福小伙伴们牛年大吉，牛气冲天。本期文章分享的是赵老师在《方法论与工程化解决解决方案》一书中提到的关于如何在用户画像项目开发中进行"),a("strong",[t._v("性能调优")]),t._v("的例子，希望大家耐心看完后有所收获！")]),t._v(" "),a("hr"),t._v(" "),a("h2",{attrs:{id:"一、数据倾斜调优"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一、数据倾斜调优"}},[t._v("#")]),t._v(" 一、数据倾斜调优")]),t._v(" "),a("p",[t._v("        数据倾斜是开发画像过程中常遇到的问题，当任务执行一直卡在map 100%、reduce 99%，最后的1%花了几个小时都没执行完时，这时一般是遇到了"),a("strong",[t._v("数据倾斜")]),t._v("。")]),t._v(" "),a("p",[t._v("        问题出现的原因是当进行分布式计算时，由于"),a("strong",[t._v("某些节点需要计算的数据较多，导致其他节点的reduce阶段任务执行完成时，该节点的任务还没有执行完成")]),t._v("，"),a("strong",[t._v("造成其他节点等待该节点执行完成")]),t._v("的情况。比如两张大表在join的时候大部分key对应10条数据，但是个别几个key对应了100万条数据，对应10条数据的task很快执行完成了，但对应了100万数据的key则要执行几个小时。")]),t._v(" "),a("p",[t._v("        下图便是一个典型的例子。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210208210738487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:""}})]),t._v(" "),a("p",[t._v("​         bb这个key在3个节点上有11条数据，aa和cc在3个节点上分别有2条和1条数据，这些数据都会被拉取到一个task上处理。处理bb这个task的运行时间可能是处理aa和cc的task的运行时间数倍，"),a("strong",[t._v("整体运行速度由最慢的task决定")]),t._v("。")]),t._v(" "),a("p",[t._v("        下面介绍两种解决"),a("strong",[t._v("数据倾斜")]),t._v("问题的方案。")]),t._v(" "),a("p",[a("font",{attrs:{color:"RoyalBlue"}},[a("strong",[t._v("方案一：过滤掉倾斜数据")])])],1),t._v(" "),a("p",[t._v("        当少量key重复次数特别多，如果这种key不是业务需要的key，可以直接过滤掉。这里有一张埋点日志表 ods.page_event_log ，需要和订单表 dw.order_info_fact 做join关联。在执行Hive的过程中发现任务卡在map 100%、reduce 99%，最后的1%一直运行不完。考虑应该是在join的过程中出现了数据倾斜，下面进行排查。")]),t._v(" "),a("p",[t._v("        对于ods.page_event_log表查看出现次数最多的key：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" cookieid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n       "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" num\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" ods"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("page_event_log\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"20190101"')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" cookieid\n    distribute "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" cookieid\n    sort "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" num\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("desc")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("limit")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("        将 key 按出现次数从多到少排序")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210208211636888.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:""}})]),t._v(" "),a("p",[t._v("        同样地，对订单表dw.order_info_fact查看出现次数最多的key：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" cookieid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n       "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" num\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" dw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("order_info_fact\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" cookieid\n    distribute "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" cookieid\n    sort "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("desc")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("limit")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n")])])]),a("p",[t._v("        将key按出现次数从多到少排序")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210208211736133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:"在这里插入图片描述"}})]),t._v(" "),a("p",[t._v("从上面的例子可以看出，日志表和订单表通过 cookieid  进行join，当 cookieid 为0的时候，join操作将会产生142286×142286条数据，数量如此庞大的节点系统无法处理过来。同样当 cookieid 为 NULL 值和 空值 时也会出现这种情况，而且 cookieid 为这3个值时并没有实际的业务意义。因此在对两个表做关联时，排除掉这3个值以后，就可以很快计算出结果了。")]),t._v(" "),a("p",[a("font",{attrs:{color:"RoyalBlue"}},[a("strong",[t._v("方案二：引入随机数")])])],1),t._v(" "),a("p",[t._v("        数据按照类型 group by 时，会将相同的key所需的数据拉取到一个节点进行聚合，而当某组数据量过大时，会出现其他组已经计算完成而当前任务未完成的情况。"),a("strong",[t._v("可以考虑加入随机数，将原来的一组key强制拆分为多组进行聚合")]),t._v("。下面通过一个案例进行介绍。")]),t._v(" "),a("p",[t._v("        现需要统计用户的订单量，执行如下代码：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n       t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("order_num\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" user_id\n          "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" dim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_info_fact   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用户维度表")]),t._v("\n       "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"20190101"')]),t._v("\n          "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" user_status_id"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n       "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t1\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" order_num\n       "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" dw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dw_order_fact      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 订单表")]),t._v("\n       "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" site_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("600")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("900")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n          "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" order_status_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("in")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n     "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" user_id\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t2\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("        用户维度表中有2000万条数据，订单表有10亿条数据，任务在未优化前执行了1个小时也没有跑出结果，判断可能是出现了"),a("strong",[t._v("数据倾斜")]),t._v("。")]),t._v(" "),a("p",[t._v("        订单表中某些 key 值数量较多，在group by 的过程中拉取到一个 task 上执行时，会出现其他task执行完毕，等待该task执行的情况。")]),t._v(" "),a("p",[t._v("        这里可以将原本相同的key通过添加随机前缀的方式变成多个 key ，"),a("strong",[t._v("这样将原本被一个 task 处理的 key 分散到多个 task 上先做一次聚合，然后去掉前缀再进行一次聚合得到最终结果")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210208212354588.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:"在这里插入图片描述"}})]),t._v(" "),a("p",[t._v("修改后代码执行如下：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n       t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("order_num\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" user_id\n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" dim_user_info_fact\n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" data_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"20190101"')]),t._v("\n     "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t1\n         "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("order_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" order_num\n               "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("round")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rand"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" rnd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("             "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" order_num\n                     "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" dw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("order_info_fact\n                     "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pay_status "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                     "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("round")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rand"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t\n               "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t2\n              "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id\n")])])]),a("h2",{attrs:{id:"二、合并小文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#二、合并小文件"}},[t._v("#")]),t._v(" 二、合并小文件")]),t._v(" "),a("p",[t._v("        在Spark执行“insert overwrite table表名”的语句时，由于"),a("strong",[t._v("多线程并行向HDFS写入且RDD默认分区为200个")]),t._v("，因此默认情况下会产生200个小文件。")]),t._v(" "),a("p",[t._v("        Spark中可以使用 "),a("code",[t._v("reparation")]),t._v(" 或 "),a("code",[t._v("coalesce")]),t._v(" 对RDD的分区重新进行划分，reparation 是 coalesce 接口中 shuffle 为true的实现。")]),t._v(" "),a("p",[t._v("        在Spark内部会对每一个分区分配一个task 执行，如果task过多，那么每个task处理的数据量很小，这就会造成线程频繁在task之间切换，导致集群工作效率低下。为解决这个问题，常采用"),a("strong",[t._v("RDD重分区函数来减少分区数量，将小分区合并为大分区，从而提高集群工作效率")]),t._v("。")]),t._v(" "),a("div",{staticClass:"language-scala extra-class"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 合并插入用户宽表数据的分区")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" executesqls "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    |     select user_id,\n    |           org_id,\n    |           org_name,\n    |           sum(act_weight) as act_weight,\n    |           sum(cnt) as cnt\n    |       from dw.peasona_user_tag_relation\n    |      where user_id is not null\n    |        and user_id <> \'null\'\n    |   group by user_id,org_id,org_name\n"""')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stripMargin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("coalesce"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" datardd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" executesqls"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getAs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"user_id"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" org_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getAs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org_id"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" org_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getAs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org_name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" act_weight "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getAs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"act_weight"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" cnt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getAs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cnt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    Row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("org_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("org_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("act_weight"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("cnt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createDataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("datardd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StructType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Seq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"user_id"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org_id"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org_name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"act_weight"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cnt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createOrReplaceTempView"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"user_act_info"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  s"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n    | INSERT OVERWRITE TABLE dw.peasona_user_tag_relation partition(data_date="$data_date")\n    |     SELECT user_id,org_id,org_name,act_weight,cnt\n    |      FROM user_act_info\n  """')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stripMargin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h2",{attrs:{id:"三、缓存中间数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#三、缓存中间数据"}},[t._v("#")]),t._v(" 三、缓存中间数据")]),t._v(" "),a("p",[t._v("        Spark的一个重要的能力就是"),a("strong",[t._v("将数据持久化缓存")]),t._v("，这样在多个操作期间都可以访问这些持久化的数据。"),a("strong",[t._v("当持久化一个RDD时，每个节点的其他分区都可以使用RDD在内存中进行计算，在该数据上的其他action操作将直接使用内存中的数据，这样会使其操作计算速度加快")]),t._v("。对RDD的复杂操作如果没有持久化，那么一切的操作都会从源头开始，一步步往后计算，不会复用原始数据。")]),t._v(" "),a("p",[t._v("        在画像标签每天ETL的时候，对于一些中间计算结果可以不落磁盘，只需把数据缓存在内存中。而使用Hive进行ETL时需要将一些中间计算结果落在临时表中，使用完临时表后再将其删除。")]),t._v(" "),a("p",[t._v("        RDD可以使用 "),a("code",[t._v("persist")]),t._v(" 或 "),a("code",[t._v("cache")]),t._v("方法进行持久化，使用 "),a("code",[t._v("StorageLevel")]),t._v(" 对象给 persist 方法设置存储级别时，常用的存储级别如下所示。")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("MEMORY_ONLY")]),t._v("：只存储在内存中")]),t._v(" "),a("li",[a("strong",[t._v("MEMORY_ONLY_2")]),t._v("：只存储在内存中，每个分区在集群中两个节点上建立副本；")]),t._v(" "),a("li",[a("strong",[t._v("DISK_ONLY")]),t._v("：只存储在磁盘中；")]),t._v(" "),a("li",[a("strong",[t._v("MEMORY_AND_DISK")]),t._v("：先存储在内存中，内存不够的话存储在磁盘中")])]),t._v(" "),a("p",[t._v("        其中 "),a("code",[t._v("cache")]),t._v(" 方法等同于调用 "),a("strong",[t._v("persist")]),t._v("（）的 "),a("code",[t._v("MEMORY_ONLY")]),t._v("方法")]),t._v(" "),a("p",[t._v("        在画像标签开发中，一般从Hive中读取数据，然后将需要做中间处理的DataFrame注册成缓存表。")]),t._v(" "),a("p",[t._v("        这里介绍一个开发画像标签时缓存中间数据的案例。")]),t._v(" "),a("p",[t._v("        执行如下代码：")]),t._v(" "),a("div",{staticClass:"language-scala extra-class"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 读取原数据 下单用户")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" peopleRDD "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"C:\\\\Users\\\\king\\\\Desktop\\\\practice\\\\cookiesession"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("_"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('","')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("       "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// RDD[Array[String]]")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" row "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" Row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// RDD[Row]")]),t._v("\npeopleRDD"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("persist"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("StorageLevel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MEMORY_ONLY"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npeopleRDD"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createOrReplaceTempView"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("“user_base_info”"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("        这里将读取的用户数据缓存在内存中并注册为一张视图。后续直接从视图中读取对应用户数据。"),a("strong",[t._v("在该Spark任务执行完成后，释放内存，不需要清除该缓存数据。")])]),t._v(" "),a("h2",{attrs:{id:"四、开发中间表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#四、开发中间表"}},[t._v("#")]),t._v(" 四、开发中间表")]),t._v(" "),a("p",[t._v("        在用户画像迭代开发的过程中，初期开发完标签后，通过对标签加工作业的血缘图整理，可以找到使用相同数据源的标签，对这部分标签，可以通过加工中间表缩减每日画像调度作业时间。")]),t._v(" "),a("p",[t._v("        做中间层设计前需要明确几个重要的点：")]),t._v(" "),a("p",[t._v("        1）这个中间层对应的业务场景、业务目标是什么？")]),t._v(" "),a("p",[t._v("        2）业务方有了这份中间层数据以后可以进行哪些维度的分析，ETL时有了这份中间层数据可以减少对哪些数据的重复开发计算？")]),t._v(" "),a("p",[t._v("        3) 这个业务场景分析中包含哪些分析维度和指标？")]),t._v(" "),a("p",[t._v("        4）同时面向很多业务场景的中间层不一定是好的中间层。")]),t._v(" "),a("p",[t._v("        在开发中间表前，首先需要梳理目前用户标签计算时依赖的上游数据仓库的表（如图5-5所示）")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210208213800193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        和标签的血缘依赖\n"),a("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210208213832522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        例如在开发过程中，可以在 dwd 层的日分区存放当天日期对应的订单，而 dws层作为服务层，其日分区用于存放当天日期对应的全量数据。这样，在日常调度计算的过程中，可避免在dwd层重复计算历史数据，只需计算当天的新增数据，既节省了ETL时间，也不会影响服务层的数据。")]),t._v(" "),a("p",[t._v("        通过对用户标签的血缘图进行梳理，找到共同依赖的上游数据。")]),t._v(" "),a("blockquote",[a("p",[t._v("        之前在笔者的项目开发过程中，ETL调度时间过长是一个较难解决的“瓶颈”，每天的调度在跑完计算标签、标签校验预警、计算人群、人群校验预警、同步到服务层等环节后往往需要几个小时，最后提供到服务层数据时也比较晚了。在这个过程中为了减少调度时间，我们也做了很多尝试，包括对一些Hive表设计多个分区，并行跑任务插入数据；对一些执行时间过长的脚本进行调优；梳理数据血缘开发中间层表，对一些常见的公共数据直接从中间层表获取数据，减少数据的重复开发计算等。在经过多次迭代后也取得了不错的效果，将整体调度时间压缩了1/3，可以满足每天及时将画像数据输出到服务层的需要。")]),t._v(" "),a("p",[t._v("        本期介绍了画像系统在数据开发中可能遇到的需要调优的场景。通过对数据倾斜、合并小文件、缓存中间数据、开发中间表几个常见问题的处理，可以优化ETL作业流程，减少调度的整体时间。")])])])}),[],!1,null,null,null);s.default=e.exports}}]);