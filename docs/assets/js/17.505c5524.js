(window.webpackJsonp=window.webpackJsonp||[]).push([[17],{408:function(s,a,t){"use strict";t.r(a);var e=t(30),n=Object(e.a)({},(function(){var s=this,a=s.$createElement,t=s._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h2",{attrs:{id:"hive小文件过多问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hive小文件过多问题"}},[s._v("#")]),s._v(" HIVE小文件过多问题")]),s._v(" "),t("h3",{attrs:{id:"一、小文件产生原因"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#一、小文件产生原因"}},[s._v("#")]),s._v(" 一、小文件产生原因")]),s._v(" "),t("p",[s._v("hive 中的小文件肯定是向 hive 表中导入数据时产生，所以先看下向 hive 中导入数据的几种方式")]),s._v(" "),t("div",{staticClass:"language-h extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("// 1.直接向表中插入数据\n// 这种方式每次插入时都会产生一个文件，多次插入少量数据就会出现多个小文件，但是这种方式生产环境很少使用，可以说基本没有使用的\ninsert into table A values (1,'zhangsan',88),(2,'lisi',61);\n\n// 2.通过load方式加载数据\n// 使用 load 方式可以导入文件或文件夹，当导入一个文件时，hive表就有一个文件，当导入文件夹时，hive表的文件数量为文件夹下所有文件的数量\nload data local inpath '/export/score.csv' overwrite into table A  -- 导入文件\nload data local inpath '/export/score' overwrite into table A   -- 导入文件夹\n\n// 3.通过查询方式加载数据\n// 这种方式是生产环境中常用的，也是最容易产生小文件的方式.insert 导入数据时会启动 MR 任务，MR中 reduce 有多少个就输出多少个文件.所以，文件数量=ReduceTask数量*分区数.也有很多简单任务没有reduce，只有map阶段，则文件数量=MapTask数量*分区数。每执行一次 insert 时hive中至少产生一个文件，因为 insert 导入时至少会有一个MapTask。像有的业务需要每10分钟就要把数据同步到 hive 中，这样产生的文件就会很多。\ninsert overwrite table A  select s_id,c_name,s_score from B;\n\n\n\n")])])]),t("h3",{attrs:{id:"二、小文件过多产生的影响"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#二、小文件过多产生的影响"}},[s._v("#")]),s._v(" 二、小文件过多产生的影响")]),s._v(" "),t("ol",[t("li",[t("p",[s._v("首先对底层存储HDFS来说，HDFS本身就不适合存储大量小文件，小文件过多会导致namenode元数据特别大, 占用太多内存，严重影响HDFS的性能.（在namenode中，一个文件占用的内存大小是固定150字节）")]),s._v(" "),t("p",[s._v("（给namenode内存中fsImage的合并造成压力，如果namenode内存使用完了，这个集群将不能再存储文件了；)")])]),s._v(" "),t("li",[t("p",[s._v("对 hive 来说，在进行查询时，每一个小文件会创建一个maptask。每个小文件都会当成一个块，启动一个Map任务来完成，而一个Map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的Map数量是受限的。")]),s._v(" "),t("p",[s._v("(虽然map阶段都设置了小文件合并，"),t("code",[s._v("org.apache.hadoop.hive.ql.io.CombineHiveInputFormat")]),s._v("，太多小文件导致合并时间较长，查询缓慢；)")])])]),s._v(" "),t("h3",{attrs:{id:"三、怎么解决小文件过多"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#三、怎么解决小文件过多"}},[s._v("#")]),s._v(" 三、怎么解决小文件过多")]),s._v(" "),t("h4",{attrs:{id:"_1-使用-hive-自带的-concatenate-命令-自动合并小文件"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-使用-hive-自带的-concatenate-命令-自动合并小文件"}},[s._v("#")]),s._v(" 1. 使用 hive 自带的 concatenate 命令，自动合并小文件")]),s._v(" "),t("div",{staticClass:"language-sql extra-class"},[t("pre",{pre:!0,attrs:{class:"language-sql"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#对于非分区表")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("alter")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" A concatenate"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#对于分区表")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("alter")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" B "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("day")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20201224")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" concatenate"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),t("p",[s._v("举例：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#向 A 表中插入数据")]),s._v("\nhive "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("default"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" insert into table A values "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(","),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'aa'")]),s._v(",67"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(","),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(","),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'bb'")]),s._v(",87"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\nhive "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("default"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" insert into table A values "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v(","),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'cc'")]),s._v(",67"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(","),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v(","),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'dd'")]),s._v(",87"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\nhive "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("default"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" insert into table A values "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),s._v(","),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'ee'")]),s._v(",67"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(","),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("6")]),s._v(","),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'ff'")]),s._v(",87"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#执行以上三条语句，则A表下就会有三个小文件,在hive命令行执行如下语句")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#查看A表下文件数量")]),s._v("\nhive "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("default"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" dfs -ls /user/hive/warehouse/A"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\nFound "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v(" items\n-rwxr-xr-x   "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v(" root supergroup        "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("378")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2020")]),s._v("-12-24 "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v(":46 /user/hive/warehouse/A/000000_0\n-rwxr-xr-x   "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v(" root supergroup        "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("378")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2020")]),s._v("-12-24 "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v(":47 /user/hive/warehouse/A/000000_0_copy_1\n-rwxr-xr-x   "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v(" root supergroup        "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("378")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2020")]),s._v("-12-24 "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v(":48 /user/hive/warehouse/A/000000_0_copy_2\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#可以看到有三个小文件，然后使用 concatenate 进行合并")]),s._v("\nhive "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("default"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" alter table A concatenate"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#再次查看A表下文件数量")]),s._v("\nhive "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("default"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" dfs -ls /user/hive/warehouse/A"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\nFound "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" items\n-rwxr-xr-x   "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v(" root supergroup        "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("778")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2020")]),s._v("-12-24 "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v(":59 /user/hive/warehouse/A/000000_0\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#已合并成一个文件")]),s._v("\n\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 注意：  ")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("、concatenate 命令只支持 RCFILE 和 ORC 文件类型。  \n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("、使用concatenate命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令。  \n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v("、当多次使用concatenate后文件数量不在变化，这个跟参数 mapreduce.input.fileinputformat.split.minsize"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("256mb 的设置有关，可设定每个文件的最小size。\n")])])]),t("h4",{attrs:{id:"_2-使用hadoop的archive将小文件归档"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-使用hadoop的archive将小文件归档"}},[s._v("#")]),s._v(" 2. 使用hadoop的archive将小文件归档")]),s._v(" "),t("p",[s._v("Hadoop Archive简称HAR，是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时，仍然允许对文件进行透明的访问")]),s._v(" "),t("p",[s._v("类似Windows下的zip压缩，对于namenode来说就是一个整体，同时采用har协议，可以将里面的文件一个一个取出来。")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#用来控制归档是否可用")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.archive.enabled"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#通知Hive在创建归档时是否可以设置父目录")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.archive.har.parentdir.settable"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#控制需要归档文件的大小")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" har.partfile.size"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1099511627776")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#使用以下命令进行归档")]),s._v("\nALTER TABLE A ARCHIVE PARTITION"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dt"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2020-12-24'")]),s._v(", "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("hr")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'12'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#对已归档的分区恢复为原文件")]),s._v("\nALTER TABLE A UNARCHIVE PARTITION"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dt"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2020-12-24'")]),s._v(", "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("hr")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'12'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#注意:   ")]),s._v("\n归档的分区可以查看不能 insert overwrite，必须先 unarchive\n")])])]),t("h4",{attrs:{id:"_3-调整参数减少map数量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-调整参数减少map数量"}},[s._v("#")]),s._v(" 3. 调整参数减少Map数量")]),s._v(" "),t("ul",[t("li",[t("strong",[s._v("设置map输入合并小文件的相关参数")]),s._v("：")])]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("#执行Map前进行小文件合并\n#CombineHiveInputFormat底层是 Hadoop的 CombineFileInputFormat 方法\n#此方法是在mapper中将多个文件合成一个split作为输入\nset hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; -- 默认\n\n#每个Map最大输入大小(这个值决定了合并后文件的数量)\nset mapred.max.split.size=256000000;   -- 256M\n\n#一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)\nset mapred.min.split.size.per.node=100000000;  -- 100M\n\n#一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)\nset mapred.min.split.size.per.rack=100000000;  -- 100M\n")])])]),t("ul",[t("li",[t("strong",[s._v("设置map输出和reduce输出进行合并的相关参数")]),s._v(":")])]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#设置map端输出进行合并，默认为true")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapfiles "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#设置reduce端输出进行合并，默认为false")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapredfiles "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#设置合并文件的大小")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.size.per.task "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("256")]),s._v("*1000*1000"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   -- 256M\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.smallfiles.avgsize"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("16000000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   -- 16M \n")])])]),t("ul",[t("li",[t("strong",[s._v("启用压缩")])])]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# hive的查询结果输出是否进行压缩")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.exec.compress.output"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# MapReduce Job的结果输出是否使用压缩")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapreduce.output.fileoutputformat.compress"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("true"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),t("h4",{attrs:{id:"_4-减少reduce的数量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-减少reduce的数量"}},[s._v("#")]),s._v(" 4. 减少Reduce的数量")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#reduce 的个数决定了输出的文件的个数，所以可以调整reduce的个数控制hive表的文件数量，")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#hive中的分区函数 distribute by 正好是控制MR中partition分区的，")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#然后通过设置reduce的数量，结合分区函数让数据均衡的进入每个reduce即可。")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#设置reduce的数量有两种方式，第一种是直接设置reduce个数")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapreduce.job.reduces"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#第二种是设置每个reduce的大小，Hive会根据数据总大小猜测确定一个reduce个数")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.exec.reducers.bytes.per.reducer"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("5120000000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" -- 默认是1G，设置为5G\n\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#执行以下语句，将数据均衡的分配到reduce中")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapreduce.job.reduces"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\ninsert overwrite table A partition"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dt"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" * from B\ndistribute by rand"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n\n解释：如设置reduce数量为10，则使用 rand"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("， 随机生成一个数 x % "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),s._v(" ，\n这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小\n")])])]),t("h4",{attrs:{id:"_5-combinerinputformat"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-combinerinputformat"}},[s._v("#")]),s._v(" 5.CombinerInputFormat")]),s._v(" "),t("p",[s._v("将多个小文件捏合在一起进行统一切片，减少maptask的数量")]),s._v(" "),t("h5",{attrs:{id:"未修改前情况"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#未修改前情况"}},[s._v("#")]),s._v(" 未修改前情况")]),s._v(" "),t("p",[s._v("目前"),t("code",[s._v("/files")]),s._v("文件夹有4个文件")]),s._v(" "),t("p",[t("img",{attrs:{src:"C:%5CUsers%5Cfallrain%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1631599077570.png",alt:"1631599077570"}})]),s._v(" "),t("p",[s._v("执行未修改前作业")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("hadoop jar hadoop-learning-1.0.jar com.shaonaiyi.hadoop.CombinerWC /files/* /output/comwc/\n")])])]),t("h5",{attrs:{id:"查看结果"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#查看结果"}},[s._v("#")]),s._v(" 查看结果")]),s._v(" "),t("p",[s._v("可在YARN的Web UI界面上看到有4个Map Task")]),s._v(" "),t("p",[t("img",{attrs:{src:"C:%5CUsers%5Cfallrain%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1631599180881.png",alt:"1631599180881"}})]),s._v(" "),t("div",{staticClass:"language-java extra-class"},[t("pre",{pre:!0,attrs:{class:"language-java"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//添加一行代码")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//合并小文件CombineTextInputFormat")]),s._v("\njob"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("setInputFormatClass")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("CombineTextInputFormat")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),t("p",[t("img",{attrs:{src:"C:%5CUsers%5Cfallrain%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1631598985493.png",alt:"1631598985493"}})]),s._v(" "),t("h5",{attrs:{id:"执行修改后作业"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#执行修改后作业"}},[s._v("#")]),s._v(" 执行修改后作业")]),s._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("hadoop jar hadoop-learning-1.0.jar com.shaonaiyi.hadoop.CombinerWC /files/* /output/comwc/\n")])])]),t("p",[s._v("可在YARN的Web UI界面上看到只有1个Map Task")]),s._v(" "),t("p",[t("img",{attrs:{src:"C:%5CUsers%5Cfallrain%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1631599230335.png",alt:"1631599230335"}})]),s._v(" "),t("h4",{attrs:{id:"_6-jvm重用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-jvm重用"}},[s._v("#")]),s._v(" 6.JVM重用")]),s._v(" "),t("p",[s._v("③同时开启了jvm重用，提高处理的效率，因为对于小文件来说，执行的时间比开关jvm的时间还短，这个时候我们就多个任务开关一次，不过开启jvm重启是一个双刃剑，在没有小文件的场景下，不要开启这个功能。")]),s._v(" "),t("h4",{attrs:{id:"_7-开启merge功能"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-开启merge功能"}},[s._v("#")]),s._v(" 7.开启merge功能")]),s._v(" "),t("p",[s._v("当Hive的输入由很多个小文件组成时，如果不涉及文件合并的话，那么每个小文件都会启动一个map task。\n如果文件过小，以至于map任务启动和初始化的时间大于逻辑处理的时间，会造成资源浪费，甚至发生OutOfMemoryError错误。\n因此，当我们启动一个任务时，如果发现输入数据量小但任务数量多时，需要注意在Map前端进行输入小文件合并操作。\n同理，向一个表写数据时，注意观察reduce数量，注意输出文件大小。")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("、 Map输入小文件合并\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#每个Map处理的最大输入文件大小(256MB)")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.max.split.size"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("256000000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  \n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#一个节点上split文件的最小值")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.min.split.size.per.node"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("100000000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" \n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#一个交换机下split文件的最小值")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.min.split.size.per.rack"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("100000000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" \n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#执行Map前进行小文件合并")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.input.format"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("org.apache.hadoop.hive.ql.io.CombineHiveInputFormat"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  \n")])])]),t("p",[s._v("在开启了org.apache.hadoop.hive.ql.io.CombineHiveInputFormat 后，\n一个data node节点上多个小文件会进行合并，合并文件数由mapred.max.split.size限制的大小决定。\nmapred.min.split.size.per.node决定了多个data node上的文件是否需要合并\nmapred.min.split.size.per.rack决定了多个交换机上的文件是否需要合并")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("、输出文件合并\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#在Map-Only的任务结束时就会合并小文件")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapfiles "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#在MapR-educe的任务结束时合并小文件")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapredfiles "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("默认为false"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#合并文件的大小")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.size.per.task "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("256")]),s._v("*1000*1000"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.smallfiles.avgsize"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("16000000")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),t("p",[s._v("四、小文件的预防")]),s._v(" "),t("p",[s._v("网上有些解决方案，是调节参数，这些参数在我使用的"),t("code",[s._v("Hive2")]),s._v("是默认都开启了的：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("//每个Map最大输入大小"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("这个值决定了合并后文件的数量"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.max.split.size"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("256000000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  \n//一个节点上split的至少的大小"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("这个值决定了多个DataNode上的文件是否需要合并"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.min.split.size.per.node"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("100000000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n//一个交换机下split的至少的大小"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("这个值决定了多个交换机上的文件是否需要合并"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("  \n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" mapred.min.split.size.per.rack"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("100000000")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n//执行Map前进行小文件合并\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.input.format"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("org.apache.hadoop.hive.ql.io.CombineHiveInputFormat"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" \n//设置map端输出进行合并，默认为true\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapfiles "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),s._v("\n//设置reduce端输出进行合并，默认为false\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.mapredfiles "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("true")]),s._v("\n//设置合并文件的大小\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.size.per.task "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("256")]),s._v("*1000*1000\n//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("set")]),s._v(" hive.merge.smallfiles.avgsize"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("16000000")]),s._v("\n")])])]),t("p",[s._v("有些公司用的版本不同，低版本可能有些配置不一样，最好检查一下上面这些配置是否设置，然后根据自己的实际集群情况进行设置。")]),s._v(" "),t("p",[s._v("小文件的预防，主要还是要根据小文件的产生原因，来进行预防。")]),s._v(" "),t("ol",[t("li",[s._v("动态分区插入的时候，保证有静态分区，不要误判导致产生大量分区，大量分区加起来，自然就有大量小文件；")]),s._v(" "),t("li",[s._v("如果源表是有大量小文件的，在导入数据到目标表的时候，如果只是"),t("code",[s._v("insert into dis select * from origin")]),s._v("的话，目标表通常也有很多小文件。如果有分区，比如"),t("code",[s._v("dt, hour")]),s._v("，可以使用"),t("code",[s._v("distribute by dt, hour")]),s._v("，保证每个小时的数据在一个reduce里面；")]),s._v(" "),t("li",[s._v("类似"),t("code",[s._v("sqoop")]),s._v("增量导入，还有"),t("code",[s._v("hive")]),s._v("一些表的查询增量导入，这些肯定是有小文件的，需要进行"),t("strong",[s._v("一周甚至一天")]),s._v("定时任务的小文件合并")])])])}),[],!1,null,null,null);a.default=n.exports}}]);