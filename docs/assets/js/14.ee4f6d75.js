(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{406:function(t,a,e){"use strict";e.r(a);var s=e(30),r=Object(s.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"mapreduce核心知识点"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce核心知识点"}},[t._v("#")]),t._v(" MapReduce核心知识点")]),t._v(" "),e("h2",{attrs:{id:"mapreduce的原理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce的原理"}},[t._v("#")]),t._v(" MapReduce的原理")]),t._v(" "),e("p",[t._v("        Hadoop 中 MapReduce 最核心的思想就是"),e("strong",[t._v("分而治之")]),t._v("，通过 MapReduce 这个名字就可以看出，MapReduce 包含有 Map 和 Reduce 两个部分。它将一个大型的计算问题分解成一个个小的，简单的计算任务，交给 MapReduce 中的 Map 部分执行，随后 Reduce 部分会对 Map 部分输出的中间结果进行聚合计算，输出最终的统计结果。")]),t._v(" "),e("p",[t._v("        为了方便大家理解，可以看下 MapReduce 的简要模型图：\n"),e("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210429000836579.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        每个子任务在框架中都是高度并行计算的，然后 MapReduce 框架将各个计算子任务的计算结果进行合并，得出最终的计算结果。")]),t._v(" "),e("p",[e("strong",[t._v("每个子任务在 MapReduce 内部都是高度并行计算的，子任务的高度并行化极大地提高了 Hadoop 处理海量数据的性能")]),t._v("。MapReduce 的并行计算模型如图所示：")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210429002500174.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        由图可知，MapReduce 框架将一个大型的计算任务拆分为多个简单的计算任务，交由多个 Map 并行计算，每个 Map 的计算结果经过中间结果处理阶段的处理后输入 Reduce 阶段，Reduce 阶段将输入的数据进行合并处理，输出最终的计算结果 。")]),t._v(" "),e("p",[t._v("        同时，"),e("strong",[t._v("用户无须关心 MapReduce 底层各个节点之间的通信机制与通信过程，只需简单地编写 map() 函数和 reduce() 函数即可开发 Hadoop MapReduce 程度")]),t._v("。")]),t._v(" "),e("h2",{attrs:{id:"mapreduce的部署结构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce的部署结构"}},[t._v("#")]),t._v(" MapReduce的部署结构")]),t._v(" "),e("p",[t._v("        MapReduce 框架由一个主节点（ResourceManager）、多个子节点（NodeManager）和每个执行任务的 MR AppMaster 共同组成 。通常会将 MapReduce 的计算节点和存储节点部署在同一台服务器上，如图所示：")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210429130001564.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        这种部署结构"),e("strong",[t._v("可以使 MapReduce 框架在已经存储好数据的节点上快速、高效地调度任务，尽可能地不用通过 RPC 从其他服务器上获取数据来执行任务，使整个集群的网络带宽被高效利用，极大地提升了处理任务的效率")]),t._v("。")]),t._v(" "),e("h2",{attrs:{id:"mapreduce-的运行流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce-的运行流程"}},[t._v("#")]),t._v(" MapReduce 的运行流程")]),t._v(" "),e("p",[t._v("        MapReduce 编程模型"),e("strong",[t._v("简化了分布式系统中并行计算的复杂度，开发人员能够不必关心 MapReduce 程序的底层实现细节，只专注于解决业务需求")]),t._v("。")]),t._v(" "),e("p",[t._v("        在 MapReduce 框架内部，整个运行流程可以分为如下四个阶段，其中每个阶段中的数据传输格式也不一样。")]),t._v(" "),e("p",[t._v("        简单运行流程如下所示：")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210429132000329.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        大致流程：")]),t._v(" "),e("p",[t._v("        （1）原始数据经过 Hadoop 框架的处理，将 “（k，原始数据行）”格式的数据输入 Map 阶段，即 Map 阶段接收到的数据都是 “（k，元素数据行）”的。")]),t._v(" "),e("p",[t._v("        （2）数据经过 Map 阶段处理之后，输出 “{（k1,v1),(k2,v2)}”格式的中间结果")]),t._v(" "),e("p",[t._v("        （3）Map阶段输出的中间结果经由 Hadoop 的中间结果处理阶段（如聚合、排序等）之后，会形成 “ {(k1,[v1,v2]) ...} ”格式的数据")]),t._v(" "),e("p",[t._v("        （4）中间结果处理阶段形成的 “{(k1,[v1,v2]) ...}”格式的数据会输入 Reduce 阶段进行处理。此时，key相同的数据会被输入进同一个 Reduce 函数进行处理（也可以由用户自定义数据分发规则）")]),t._v(" "),e("p",[t._v("        （5）数据经过 Reduce 阶段处理之后，最终会形成“{(k1,v3)}” 格式的数据存入 HDFS 中")]),t._v(" "),e("p",[t._v("        另外，如果觉得不够清晰，也可以参考下下面这个版本的 MapReduce 运行流程。")]),t._v(" "),e("p",[e("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210429232425836.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述"}}),t._v("\n        （1）原始数据被切分为多个小的数据分片输入 map() 函数，这些小的数据分片往往是原始数据的数据行，它们以   “(k，line)”  的格式输入 map() 函数，其中 k 表示数据的偏移量，line 表示整行数据。")]),t._v(" "),e("p",[t._v("        （2）map() 函数并行处理输入的数据分片，根据具体的业务规则对输入的数据进行相应的处理，输出中间处理结果，这些中间处理结果往往以“{(k1,v1),(k2,v2)}” 的格式存在。")]),t._v(" "),e("p",[t._v("        （3）中间处理阶段将 map() 函数输出的中间结果根据 key 进行聚合处理，输出聚合结果，这些聚合结果的格式为：“{(k1,[v1,v2])}”。")]),t._v(" "),e("p",[t._v("        （4）中间处理阶段将输出的聚合结果输入 reduce () 函数进行处理( key相同的数据会被输入同一个 reduce()函数中，用户也可以自定义数据分发规则 )，reduce（）函数对这些数据进行进一步聚合和计算等。")]),t._v(" "),e("p",[t._v("        （5）reduce 函数将最终的结果以 “ (k,v) ”的格式输出到 HDFS 中。")]),t._v(" "),e("h2",{attrs:{id:"mapreduce的容错机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce的容错机制"}},[t._v("#")]),t._v(" MapReduce的容错机制")]),t._v(" "),e("p",[t._v("        MapReduce 容错包括 Task（任务）容错，AppMaster 容错、NodeManager 容错和 ResourceManager 容错。")]),t._v(" "),e("h3",{attrs:{id:"_1、task-容错"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1、task-容错"}},[t._v("#")]),t._v(" 1、Task 容错")]),t._v(" "),e("p",[t._v("        AppMaster 一段时间没有收到任务进度的更新，就会将任务标记为失败，但是不会立刻杀死执行任务的进程，而是等待一定的超时时间。该超时时间可以在"),e("code",[t._v("mapred-site.xml")]),t._v("文件中进行配置，具体的属性为"),e("code",[t._v("mapreduce.task.timeout")]),t._v("：")]),t._v(" "),e("div",{staticClass:"language-xml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-xml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("properties")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("mapreduce.task.timeout"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("600000"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])]),e("p",[t._v("        超时时间默认值为 10 min，即任务被标记为失败的 10 min 之后才会将任务失败的进程杀死。")]),t._v(" "),e("p",[t._v("        MapReduce 提供了重试机制，重试的次数主要由 "),e("code",[t._v("map-site.xml")]),t._v("文件中的 "),e("code",[t._v("mapreduce.map.maxattempts")]),t._v("属性和"),e("code",[t._v("mapreduce.reduce.maxattempts")]),t._v("属性配置，代码如下所示：")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("properties"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("mapreduce.map.maxattempts"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),e("span",{pre:!0,attrs:{class:"token operator"}},[e("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[t._v("4")]),t._v("<")]),t._v("/value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/property"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("properties"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("mapreduce.reduce.maxattempts"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),e("span",{pre:!0,attrs:{class:"token operator"}},[e("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[t._v("4")]),t._v("<")]),t._v("/value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/property"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n")])])]),e("p",[t._v("        默认重试次数为4，即任务失败后，MapReduce 框架会重试4次，如果任务依然失败，MapReduce才会认为任务彻底失败了。")]),t._v(" "),e("p",[t._v("        也可以配置允许任务失败的最大百分比，可以由属性 "),e("code",[t._v("mapreduce.map.failures.maxpercent")]),t._v(" 和 "),e("code",[t._v("mapreduce.reduce.failures.maxprecent")]),t._v(" 进行配置。")]),t._v(" "),e("h3",{attrs:{id:"_2、appmaster-容错"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2、appmaster-容错"}},[t._v("#")]),t._v(" 2、AppMaster 容错")]),t._v(" "),e("p",[t._v("        AppMaster也提供了重试机制，YARN中的应用程序失败之后，最多尝试次数由"),e("code",[t._v("mapred-site.xml")]),t._v("文件中的"),e("code",[t._v("mapreduce.am.max-attempts")]),t._v("属性配置：")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("properties"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("mapreduce.am.max-attempts"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),e("span",{pre:!0,attrs:{class:"token operator"}},[e("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[t._v("4")]),t._v("<")]),t._v("/value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/property"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n")])])]),e("p",[t._v("        尝试次数默认值为2，即当 AppMaster 失败2次之后，运行的任务将会失败。")]),t._v(" "),e("p",[t._v("        在 MapReduce 内部，YARN 框架对 AppMaster 的最大尝试次数做了限制。其中，每个在 YARN 中运行的应用程序不能超过这个数量限制，具体限制由 yarn-site.xml 文件中的 "),e("code",[t._v("yarn.resourcemanager.am.max-attempts")]),t._v("属性控制，配置信息如下所示：")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("properties"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("yarn.resourcemanager.am.max-attempts"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),e("span",{pre:!0,attrs:{class:"token operator"}},[e("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[t._v("2")]),t._v("<")]),t._v("/value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/property"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n")])])]),e("h3",{attrs:{id:"_3、nodemanager-容错"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3、nodemanager-容错"}},[t._v("#")]),t._v(" 3、NodeManager 容错")]),t._v(" "),e("p",[t._v("        当 NodeManager 发生故障，停止向 ResourceManager 节点发送心跳信息时，ResourceManager 节点并不会立即移除 NodeManager，而是要等待一段时间，该时间可以由 "),e("code",[t._v("yarn.resourcemanager.nm.liveness-monitor.expiry-interval-ms")]),t._v(" 属性设置，代码如下：")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("properties"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("yarn.resourcemanager.nm.liveness-monitor.expiry-interval-ms"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("60000")]),e("span",{pre:!0,attrs:{class:"token operator"}},[e("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[t._v("0")]),t._v("<")]),t._v("/value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/property"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n")])])]),e("p",[t._v("        等待时间默认值为 10 min，即 NodeManager 发生故障之后，ResourceManager 节点接收不到 NodeManager 发生过来的心跳信息，过 10 min 之后才会将 NodeManager 移除 。")]),t._v(" "),e("p",[t._v("        当 NodeManager 上运行的失败任务数量达到一定的值时，AppMaster 就会将该节点上的任务调度到其他节点上。任务失败的阈值由 mapred-site.xml 文件中的 "),e("code",[t._v("mapreduce.job.maxtaskfailures.per.tracker")]),t._v(" 属性设置，代码如下所示：")]),t._v(" "),e("div",{staticClass:"language-xml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-xml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("properties")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("mapreduce.job.maxtaskfailures.per.tracker"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("3"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])]),e("p",[t._v("        此默认值为3，即当一个 NodeManager 上有超过3个任务失败，AppMaster 就会将该节点上的任务调度到其他节点上 。")]),t._v(" "),e("h3",{attrs:{id:"resourcemanager-容错"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#resourcemanager-容错"}},[t._v("#")]),t._v(" ResourceManager 容错")]),t._v(" "),e("p",[t._v("        新版本的 Hadoop 中提供了 ResourceManager 节点的 HA 机制，如果主 ResourceManager 失败，备 ResouceManager 会迅速接管工作。")]),t._v(" "),e("p",[t._v("        Hadoop 中对 ResourceManager节点提供了检查点机制，当所有的 ResourceManager 节点失败后，重启 ResouceManager 节点，可以从上一个失败的 ResourceManager 节点保存的检查点进行状态恢复。")]),t._v(" "),e("p",[t._v("        这些检查点的存储是由 "),e("code",[t._v("yarn-site.xml")]),t._v("文件中的 "),e("code",[t._v("yarn-resourcemanager.store.class")]),t._v("属性设置的，代码如下所示:")]),t._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("properties"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("yarn-resourcemanager.store.class"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n     "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/value"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("/property"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n")])])]),e("p",[t._v("        当然，默认是保存到文件中。")]),t._v(" "),e("h2",{attrs:{id:"mapreduce的优化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce的优化"}},[t._v("#")]),t._v(" MapReduce的优化")]),t._v(" "),e("p",[t._v("        技术面试中，关于 MapReduce 优化的考察频率可能不如 Spark，Flink，但是作为 Hadoop 知识的热门考点，我们对于它的优化还是要有一个清晰的认识 。 这里，我们从以下几个小点逐一展开。")]),t._v(" "),e("h3",{attrs:{id:"mapreduce跑的慢的原因"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce跑的慢的原因"}},[t._v("#")]),t._v(" MapReduce跑的慢的原因")]),t._v(" "),e("p",[t._v("        MapReduce程序效率的瓶颈在于两点：")]),t._v(" "),e("ol",[e("li",[t._v("计算机性能")])]),t._v(" "),e("p",[t._v("        CPU、内存、磁盘健康、网络")]),t._v(" "),e("ol",{attrs:{start:"2"}},[e("li",[t._v("I/O 操作优化")])]),t._v(" "),e("ul",[e("li",[t._v("数据倾斜")]),t._v(" "),e("li",[t._v("Map 和 Reduce 数设置不合理")]),t._v(" "),e("li",[t._v("Map运行时间太长，导致 Reduce 等待过久")]),t._v(" "),e("li",[t._v("小文件过多")]),t._v(" "),e("li",[t._v("大量的不可分块的超大文件")]),t._v(" "),e("li",[t._v("Spill 次数过多")]),t._v(" "),e("li",[t._v("Merge 次数过多等。")])]),t._v(" "),e("h3",{attrs:{id:"mapreduce优化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce优化"}},[t._v("#")]),t._v(" MapReduce优化")]),t._v(" "),e("p",[t._v("        关于 MapReduce 优化方法主要从以下6个方面进行考虑，分别是：数据倾斜、Map阶段、Reduce阶段、IO传输、数据倾斜问题和常用的调优参数。")]),t._v(" "),e("h4",{attrs:{id:"_1、数据输入"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1、数据输入"}},[t._v("#")]),t._v(" 1、数据输入")]),t._v(" "),e("p",[t._v("        （1）"),e("strong",[t._v("合并小文件")]),t._v("：在执行 MR 任务之前将小文件进行合并，大量的小文件会产生大量的 MR 任务，增大 Map 任务装载次数，而任务的装载比较耗时，从而导致 MR 运行较慢。")]),t._v(" "),e("p",[t._v("        （2）"),e("strong",[t._v("采用 CombineText InputFormat 来作为输入")]),t._v("，解决输入端大量小文件场景。")]),t._v(" "),e("h4",{attrs:{id:"_2、map阶段"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2、map阶段"}},[t._v("#")]),t._v(" 2、Map阶段")]),t._v(" "),e("p",[t._v("        （1）减少溢写（spill）次数：通过调整 io.sort.mb 及 sort.spill.percent 参数值，增大触发 Spill 的内存上限，减少 Spill 次数，从而减少磁盘 IO 。")]),t._v(" "),e("p",[t._v("        （2）减少合并（Merge）次数：通过调整"),e("code",[t._v("io.sort.factor")]),t._v("参数，增大 Merge 的文件数目，减少 Merge 的次数，从而缩短 MR 处理时间。")]),t._v(" "),e("p",[t._v("        （3）在 Map 之后，不影响业务逻辑前提下，先进行 Combine 处理，减少 \tI/O 。")]),t._v(" "),e("h4",{attrs:{id:"_3、reduce-阶段"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3、reduce-阶段"}},[t._v("#")]),t._v(" 3、Reduce 阶段")]),t._v(" "),e("p",[t._v("        （1）合理设置 Map 和 Reduce 数：两个都不能设置的太少，也不能设置的太多。太少，会导致 Task 等待，延长处理时间；太多，会导致 Map，Reduce 任务间竞争资源，造成处理超时等错误 。")]),t._v(" "),e("p",[t._v("        （2）设置 Map、Reduce 共存：调整 "),e("code",[t._v("slowstart.completedmap")]),t._v("参数，使 Map 运行到一定程度后，Reduce 也开始运行，减少 Reduce 的等待时间 。")]),t._v(" "),e("p",[t._v("        （3）规避使用 Reduce：因为 Reduce 在用于连接数据集的时候将会产生大量的网络消耗。")]),t._v(" "),e("p",[t._v("        （4）合理设置 Reduce 端的 Buffer：默认情况下，数据达到一个阈值的时候，Buffer 中的数据就会写入磁盘，然后 Reduce 会从磁盘中获得所有的数据。也就是说，Buffer 和 Reduce 是没有直接关联的，中间多次写磁盘 -> 读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得 Buffer 中的一部分数据可以直接输送到 Reduce，从而减少 IO 开销 : "),e("code",[t._v("mapreduce.reduce.input.buffer.percent")]),t._v("，默认为 0.0 。当值大于 0 的时候，会保留指定比例的内存读 Buffer 中的数据直接拿给 Reduce 使用 。 这样一来，设置 Buffer 需要内存，读取数据需要内存，Reduce 计算也需要内存，所以要根据作业的用运行情况进行调整 。")]),t._v(" "),e("h4",{attrs:{id:"_4、i-o-传输"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4、i-o-传输"}},[t._v("#")]),t._v(" 4、I/O 传输")]),t._v(" "),e("p",[t._v("        （1）采用 数据 压缩的方式，减少  网络 IO 的时间 。 安装 Snappy 和 LZO 压缩编码器。")]),t._v(" "),e("p",[t._v("        （2）使用 SequenceFile 二进制文件。")]),t._v(" "),e("h4",{attrs:{id:"_5、数据倾斜问题"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5、数据倾斜问题"}},[t._v("#")]),t._v(" 5、数据倾斜问题")]),t._v(" "),e("h5",{attrs:{id:"_1-数据倾斜现象"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-数据倾斜现象"}},[t._v("#")]),t._v(" 1. "),e("strong",[t._v("数据倾斜现象：")])]),t._v(" "),e("ul",[e("li",[t._v("数据频率倾斜——某一个区域的数据量要远远大于其他的区域。")]),t._v(" "),e("li",[t._v("数据大小倾斜——部分记录的大小远远大于平均值")])]),t._v(" "),e("h5",{attrs:{id:"_2-减少数据倾斜的方法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-减少数据倾斜的方法"}},[t._v("#")]),t._v(" 2."),e("strong",[t._v("减少数据倾斜的方法：")])]),t._v(" "),e("h6",{attrs:{id:"方法1-抽样和范围分区"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方法1-抽样和范围分区"}},[t._v("#")]),t._v(" 方法1 ：抽样和范围分区")]),t._v(" "),e("p",[t._v("        可以通过对原始数据进行抽样得到的结果集来预设分区边界值。")]),t._v(" "),e("h6",{attrs:{id:"方法2-自定义分区"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方法2-自定义分区"}},[t._v("#")]),t._v(" 方法2：自定义分区")]),t._v(" "),e("p",[t._v("        基于输出键的背景知识进行自定义分区。例如，如果 Map 输出键的单词来源于一本书。且其中某几个专业词汇较多，那么就可以自定义分区将这些专业词汇发送给固定的一部分 Reduce 实例。而其他的都发送给剩余的 Reduce 实例。")]),t._v(" "),e("h6",{attrs:{id:"方法3-combine"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方法3-combine"}},[t._v("#")]),t._v(" 方法3：Combine")]),t._v(" "),e("p",[t._v("        使用 Combine 可以大量的减少数据倾斜。在可能的情况下，Combine 的目的就是聚合并精简数据。")]),t._v(" "),e("h6",{attrs:{id:"方法4-采用-map-join-尽量避免-reduce-join"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方法4-采用-map-join-尽量避免-reduce-join"}},[t._v("#")]),t._v(" 方法4：采用 Map Join，尽量避免 Reduce Join")]),t._v(" "),e("p",[t._v("        这个我们上面说过了，Reduce 在用于连接数据集的时候将会产生大量的网络消耗，所以我们采用 MapJoin，尽量避免 Reduce Join 。")]),t._v(" "),e("h4",{attrs:{id:"_6、常用的调优参数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_6、常用的调优参数"}},[t._v("#")]),t._v(" 6、常用的调优参数")]),t._v(" "),e("h5",{attrs:{id:"_1、资源相关参数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1、资源相关参数"}},[t._v("#")]),t._v(" 1、资源相关参数")]),t._v(" "),e("p",[t._v("（1）以下参数是在用户自己的MR应用程序中配置就可以生效（"),e("code",[t._v("mapred-default.xml")]),t._v("）")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("配置参数")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("参数说明")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.map.memory.mb")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.reduce.memory.mb")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.map.cpu.vcores")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("每个MapTask可使用的最多cpu core数目，默认值: 1")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.reduce.cpu.vcores")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("每个ReduceTask可使用的最多cpu core数目，默认值: 1")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.reduce.shuffle.parallelcopies")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("每个Reduce去Map中取数据的并行数。默认值是5")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.reduce.shuffle.merge.percent")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Buffer中的数据达到多少比例开始写入磁盘。默认值0.66")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.reduce.shuffle.input.buffer.percent")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Buffer大小占Reduce可用内存的比例。默认值0.7")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.reduce.input.buffer.percent")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("指定多少比例的内存用来存放Buffer中的数据，默认值是0.0")])])])]),t._v(" "),e("p",[t._v("        \n（2）应该在YARN启动之前就配置在服务器的配置文件中才能生效（"),e("code",[t._v("yarn-default.xml")]),t._v("）")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("配置参数")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("参数说明")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("yarn.scheduler.minimum-allocation-mb")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("给应用程序Container分配的最小内存，默认值：1024")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("yarn.scheduler.maximum-allocation-mb")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("给应用程序Container分配的最大内存，默认值：8192")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("yarn.scheduler.minimum-allocation-vcores")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("每个Container申请的最小CPU核数，默认值：1")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("yarn.scheduler.maximum-allocation-vcores")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("每个Container申请的最大CPU核数，默认值：32")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("yarn.nodemanager.resource.memory-mb")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("给Containers分配的最大物理内存，默认值：8192")])])])]),t._v(" "),e("p",[t._v("（3）Shuffle性能优化的关键参数，应在YARN启动之前就配置好（"),e("code",[t._v("mapred-default.xml")]),t._v("）")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("配置参数")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("参数说明")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.task.io.sort.mb")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Shuffle的环形缓冲区大小，默认100m")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.map.sort.spill.percent")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("环形缓冲区溢出的阈值，默认80%")])])])]),t._v(" "),e("h5",{attrs:{id:"_2、容错相关参数-mapreduce性能优化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2、容错相关参数-mapreduce性能优化"}},[t._v("#")]),t._v(" 2、容错相关参数(MapReduce性能优化)")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("配置参数")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("参数说明")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.map.maxattempts")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.reduce.maxattempts")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("mapreduce.task.timeout")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”")])])])]),t._v(" "),e("h2",{attrs:{id:"ref"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ref"}},[t._v("#")]),t._v(" Ref")]),t._v(" "),e("blockquote",[e("p",[t._v("1、《海量数据处理与大数据技术实战》\n2、《Hadoop权威指南》")])])])}),[],!1,null,null,null);a.default=r.exports}}]);