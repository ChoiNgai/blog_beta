(window.webpackJsonp=window.webpackJsonp||[]).push([[66],{452:function(s,a,t){"use strict";t.r(a);var e=t(30),n=Object(e.a)({},(function(){var s=this,a=s.$createElement,t=s._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h1",{attrs:{id:"sqoop入门"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sqoop入门"}},[s._v("#")]),s._v(" Sqoop入门")]),s._v(" "),t("h2",{attrs:{id:"前言"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#前言"}},[s._v("#")]),s._v(" 前言")]),s._v(" "),t("p",[s._v("        我们在日常开发中需要经常接触到关系型数据库，如MySQL，Oracle等等，用它们来将处理后的数据进行存储。为了能够在Hadoop上分析这些数据，我们需要一些“工具”，将关系型数据库中的"),t("strong",[s._v("结构化数据")]),s._v("存储到HDFS上。本篇文章，菌哥将介绍的一个操作最简单，同时也是在工作中使用频率极高的开源组件——"),t("strong",[s._v("Sqoop")]),s._v("，希望您能在耐心看完之后，有所收获！")]),s._v(" "),t("hr"),s._v(" "),t("h2",{attrs:{id:"_1、sqoop简介"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1、sqoop简介"}},[s._v("#")]),s._v(" 1、Sqoop简介")]),s._v(" "),t("p",[s._v("        Sqoop全称是 "),t("strong",[s._v("Apache Sqoop")]),s._v("，是一个"),t("strong",[s._v("开源工具")]),s._v("，能够将数据从数据存储空间（"),t("strong",[s._v("数据仓库，系统文档存储空间，关系型数据库")]),s._v("）导入 Hadoop 的 "),t("strong",[s._v("HDFS")]),s._v("或"),t("strong",[s._v("列式数据库HBase")]),s._v("，供 MapReduce 分析数据使用，也可以被 Hive 等工具使用。当 MapReduce 分析出结果数据后，Sqoop 可以将结果数据导出到数据存储空间，供其他客户端调用查看结果。")]),s._v(" "),t("p",[s._v("        需要注意的是，数据传输的过程大部分是自动的，通过 "),t("strong",[s._v("MapReduce")]),s._v(" 过程来实现，只需要依赖数据库的"),t("strong",[s._v("Schema")]),s._v("信息。Sqoop所执行的操作是"),t("strong",[s._v("并行")]),s._v("的，数据传输"),t("strong",[s._v("性能高")]),s._v("，具备较好的"),t("strong",[s._v("容错性")]),s._v("，并且能够"),t("strong",[s._v("自动转换")]),s._v("数据类型。")]),s._v(" "),t("p",[s._v("        Sqoop存在两个版本，版本号分别是1.4.x和1.9.x，通常被称为"),t("strong",[s._v("Sqoop1")]),s._v("和"),t("strong",[s._v("Sqoop2")]),s._v("。Sqoop2在架构和实现上，对于Sqoop1做了比较大幅度的改进，因此两个版本之间是不兼容的。基于实际应用场景考虑，下面介绍的内容全都是基于"),t("strong",[s._v("Sqoop1")]),s._v("的讲解。")]),s._v(" "),t("h2",{attrs:{id:"_2、sqoop架构"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2、sqoop架构"}},[s._v("#")]),s._v(" 2、Sqoop架构")]),s._v(" "),t("p",[s._v("        Sqoop的出现使 Hadoop 或 HBase 和数据存储空间之间的数据导入/导出变得简单，这得益于Sqoop的优良架构特征和其对数据的强大转化能力。Sqoop 导入/导出数据可抽象为下图：")]),s._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210124010518931.png#pic_center",alt:""}}),s._v("\n        从图中可以看出，Sqoop作为 Hadoop 或 HBase 和数据存储空间之间的桥梁，很容易实现 Hadoop 或 HBase 和数据存储空间之间的数据传输。")]),s._v(" "),t("p",[s._v("        Sqoop 的架构也非常简单，主要由3部分组成：Sqoop 客户端、数据存储与挖掘（HDFS/HBase/Hive）、数据存储空间，如图所示：\n        \n"),t("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210124012248834.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:""}}),s._v("\n        由图中可以看出，Sqoop协调 Hadoop 中的 Map 任务将数据从数据存储空间（"),t("strong",[s._v("数据仓库、系统文档、关系型数据库")]),s._v("）导入 HDFS/HBase供数据分析使用，同时数据分析人员也可以使用 Hive 对这些数据进行挖掘。当分析、挖掘出有价值的结果数据之后，Sqoop 又可以协调 Hadoop 中的 Map 任务将结果数据"),t("strong",[s._v("导出")]),s._v("到数据存储空间。")]),s._v(" "),t("blockquote",[t("p",[t("font",{attrs:{color:"purple"}},[s._v("注意：Sqoop 只负责数据传输，不负责数据分析，所以只会涉及 Hadoop 的 Map 任务，不会涉及 Reduce 任务")])],1)]),s._v(" "),t("h2",{attrs:{id:"_3、sqoop数据导入过程"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3、sqoop数据导入过程"}},[s._v("#")]),s._v(" 3、Sqoop数据导入过程")]),s._v(" "),t("p",[s._v("        Sqoop数据导入过程："),t("strong",[s._v("从表中读取一行行数据记录，经过Sqoop的传输，再通过Hadoop的Map任务将数据写入HDFS")]),s._v("，如图所示：")]),s._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210124111551907.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}}),s._v("\n        从图中可以看出，Sqoop数据导入过程如下：\n        （1）Sqoop通过JDBC获取所需要的数据库元数据信息，如表列名、数据类型等，并将这些元数据信息导入Sqoop。\n        （2）Sqoop生成一个与表名相同的记录容器类，记录容器类完成数据的序列化和反序列化过程，并保存表的每一行数据。\n        （3）Sqoop生成的记录容器类向Hadoop的Map作业提供序列化和反序列化的功能。\n        （4）Sqoop启动Hadoop的Map作业。\n        （5）Sqoop启动的Map作业在数据导入过程中，会通过JDBC读取数据库表中的内容，此时Sqoop生成的记录容器类同样提供反序列化功能。\n        （6）Map作业将读取的数据写入HDFS，此时Sqoop生成的记录容器类提供序列化功能。\n        ")]),s._v(" "),t("h2",{attrs:{id:"_4、sqoop数据导出过程"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4、sqoop数据导出过程"}},[s._v("#")]),s._v(" 4、Sqoop数据导出过程")]),s._v(" "),t("p",[s._v("        Sqoop数据导出过程："),t("strong",[s._v("将通过MapReduce或Hive分析后得出的数据结果导出到关系型数据库，供其他业务查看或生成报表使用")]),s._v("，如图所示：")]),s._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210124114332393.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70#pic_center",alt:""}}),s._v("\n        由图中可以看出，Sqoop数据导出过程如下：")]),s._v(" "),t("p",[s._v("        （1）Sqoop读取数据库的元数据信息（包括数据表列名、数据类型等）\n        （2）Sqoop生成记录容器类，该类与数据库的表对应，提供序列化和反序列化功能。\n        （3）Sqoop生成的记录容器类为Map作业提供序列化和反序列化功能。\n        （4）Sqoop启动Hadoop的Map作业。\n        （5）Map作业读取HDFS中的数据，此时Sqoop生成的记录容器类提供反序列化功能。\n        （6）Map作业将读取的数据通过一批 INSERT 语句写入目标数据库中，每条 INSERT 语句都会批量插入多条记录。\n        ")]),s._v(" "),t("h2",{attrs:{id:"_5、sqoop的安装和配置"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5、sqoop的安装和配置"}},[s._v("#")]),s._v(" 5、Sqoop的安装和配置")]),s._v(" "),t("p",[s._v("        Sqoop的安装非常简单，只需要简单的进行配置即可，下面简单介绍一下Sqoop的安装与配置。")]),s._v(" "),t("blockquote",[t("p",[t("strong",[s._v("注意：安装sqoop的前提是已经具备"),t("font",{attrs:{color:"red"}},[s._v("java")]),s._v("和"),t("font",{attrs:{color:"red"}},[s._v("hadoop")]),s._v("的环境")],1)])]),s._v(" "),t("h3",{attrs:{id:"_5-1-下载sqoop"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-下载sqoop"}},[s._v("#")]),s._v(" 5.1 下载Sqoop")]),s._v(" "),t("p",[s._v("        可以到Apache官网下载Sqoop")]),s._v(" "),t("blockquote",[t("p",[s._v("网址：http://sqoop.apache.org/")])]),s._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210124135936732.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}}),s._v("\n        当然也可以在服务器的命令行输入以下命令进行下载Sqoop：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token function"}},[s._v("wget")]),s._v(" http://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz\n")])])]),t("h3",{attrs:{id:"_5-2-安装并配置sqoop"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-安装并配置sqoop"}},[s._v("#")]),s._v(" 5.2 安装并配置Sqoop")]),s._v(" "),t("p",[s._v("        安装并配置Sqoop主要包括将Sqoop解压到指定的目录下，配置Sqoop系统的环境变量，修改Sqoop的配置文件，将所需要的数据库驱动复制到Sqoop的lib目录下。")]),s._v(" "),t("p",[s._v("        （1）在命令行修改以下命令解压Sqoop，这里我解压的路径是"),t("code",[s._v("/export/server")])]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[t("span",{pre:!0,attrs:{class:"token function"}},[s._v("tar")]),s._v(" -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz /export/server\n")])])]),t("p",[s._v("        （2）修改Sqoop的环境变量")]),s._v(" "),t("p",[s._v("        输入命令"),t("code",[s._v("vim /etc/profile.d/sqoop.sh")]),s._v("添加以下内容：")]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("SQOOP_HOME")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("/export/servers/sqoop-1.4.7.bin__hadoop-2.6.0\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t("span",{pre:!0,attrs:{class:"token environment constant"}},[s._v("PATH")])]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),t("span",{pre:!0,attrs:{class:"token environment constant"}},[s._v("$PATH")]),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v(":")]),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$SQOOP_HOME")]),s._v("/bin\n")])])]),t("p",[s._v("        （3）修改配置文件")]),s._v(" "),t("p",[s._v("        进入到conf目录，修改文件名，然后编辑文件内容")]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("cd")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$SQOOP_HOME")]),s._v("/conf\n"),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("mv")]),s._v(" sqoop-env-template.sh sqoop-env.sh\n"),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("vi")]),s._v(" sqoop-env.sh\n")])])]),t("p",[s._v("        这里我配置上Hadoop和Hive的信息")]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("HADOOP_COMMON_HOME")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" /export/servers/hadoop-2.7.5 \n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("HADOOP_MAPRED_HOME")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" /export/servers/hadoop-2.7.5\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("HIVE_HOME")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" /export/servers/hive\n")])])]),t("p",[s._v("        （4）添加数据库驱动")]),s._v(" "),t("p",[s._v("        我们将所需要的数据库驱动复制到Sqoop的lib目录下，如果你需要从SqlServer上进行抽取数据，那就需要添加SqlServer的相关jar包\n        \n"),t("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210124141750318.png",alt:""}}),s._v("\n        然后将其复制到服务器上Sqoop的lib目录下。")]),s._v(" "),t("p",[s._v("        （5）验证是否安装并配置成功")]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("cdp_etl@ellassay-cdh-utility-2 ~"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("$ sqoop version\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("21")]),s._v("/01/24 "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v(":22:27 INFO sqoop.Sqoop: Running Sqoop version: "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".6-cdh5.16.2\nSqoop "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".6-cdh5.16.2\n"),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("git")]),s._v(" commit "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v("\nCompiled by jenkins on Mon Jun  "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v(" 03:34:57 PDT "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2019")]),s._v("\n")])])]),t("p",[s._v("        当看到输出了Sqoop的版本，说明Sqoop的安装和配置就成功了！\n        ")]),s._v(" "),t("h2",{attrs:{id:"_6、sqoop的使用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6、sqoop的使用"}},[s._v("#")]),s._v(" 6、Sqoop的使用")]),s._v(" "),t("p",[s._v("        Sqoop的使用非常简单，只需要运行简单的命令即可实现将数据从数据库导入到HDFS，同时将数据分析结果从HDFS导出到数据库。")]),s._v(" "),t("h3",{attrs:{id:"_6-1-sqoop的命令"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-1-sqoop的命令"}},[s._v("#")]),s._v(" 6.1 Sqoop的命令")]),s._v(" "),t("p",[s._v("        想知道Sqoop有哪些命令，可以运行"),t("code",[s._v("sqoop help")]),s._v("命令，可以显示 Sqoop 所支持的所有命令信息，如下所示：")]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("alice@node01 ~"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("$ sqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("help")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("21")]),s._v("/01/24 "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v(":28:10 INFO sqoop.Sqoop: Running Sqoop version: "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".6-cdh5.16.2\nusage: sqoop COMMAND "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("ARGS"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\nAvailable commands:\n  codegen            Generate code to interact with database records\n  create-hive-table  Import a table definition into Hive\n  "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("eval")]),s._v("               Evaluate a SQL statement and display the results\n  "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v("             Export an HDFS directory to a database table\n  "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("help")]),s._v("               List available commands\n  "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v("             Import a table from a database to HDFS\n  import-all-tables  Import tables from a database to HDFS\n  import-mainframe   Import datasets from a mainframe server to HDFS\n  job                Work with saved "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("jobs")]),s._v("\n  list-databases     List available databases on a server\n  list-tables        List available tables "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" a database\n  merge              Merge results of incremental imports\n  metastore          Run a standalone Sqoop metastore\n  version            Display version information\n\nSee "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'sqoop help COMMAND'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" information on a specific command.\n")])])]),t("p",[s._v("        根据输出的提示信息，如果需要查看Sqoop具体的命令信息，可以使用"),t("code",[s._v("sqoop help COMMAND")]),s._v("命令。下面我们以"),t("code",[s._v("export")]),s._v("命令为例，如下所示：")]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("alice@node01 ~"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("$ sqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("help")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("21")]),s._v("/01/24 "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v(":30:17 INFO sqoop.Sqoop: Running Sqoop version: "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".6-cdh5.16.2\nusage: sqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("GENERIC-ARGS"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("TOOL-ARGS"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\nCommon arguments:\n   --connect "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("jdbc-uri"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                                       Specify JDBC\n                                                              connect\n                                                              string\n   --connection-manager "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("class-name"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                          Specify\n                                                              connection\n                                                              manager\n                                                              class name\n   --connection-param-file "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("properties-file"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                  Specify\n                                                              connection\n                                                              parameters\n                                                              "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("file")]),s._v("\n   --driver "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("class-name"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                                      Manually\n                                                              specify JDBC\n                                                              driver class\n                                                              to use\n   --hadoop-home "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("hdir"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                                       Override\n                                                              "),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_MAPR")]),s._v("\n                                                              ED_HOME_ARG\n   --hadoop-mapred-home "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("dir"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                                 Override\n                                                              "),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_MAPR")]),s._v("\n                                                              ED_HOME_ARG\n   --help                                                     Print usage\n                                                              instructions\n   --metadata-transaction-isolation-level "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("isolationlevel"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("    Defines the\n                                                              transaction\n                                                              isolation\n                                                              level "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v("\n                                                              metadata\n                                                              queries. For\n                                                              "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("more")]),s._v(" details\n                                                              check\n                                                              java.sql.Con\n                                                              nection\n                                                              javadoc or\n                                                              the JDBC\n                                                              specificaito\n                                                              n\n   --oracle-escaping-disabled "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("boolean"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                       Disable the\n                                                              escaping\n                                                              mechanism of\n                                                              the\n                                                              Oracle/OraOo\n                                                              p connection\n                                                              managers\n-P                                                            Read\n                                                              password\n                                                              from console\n   --password "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("password"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                                      Set\n                                                              authenticati\n                                                              on password\n   --password-alias "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("password-alias"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                          Credential\n                                                              provider\n                                                              password\n                                                              "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("alias")]),s._v("\n   --password-file "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("password-file"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                            Set\n                                                              authenticati\n                                                              on password\n                                                              "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("file")]),s._v(" path\n   --relaxed-isolation                                        Use\n                                                              read-uncommi\n                                                              tted\n                                                              isolation\n                                                              "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" imports\n   --skip-dist-cache                                          Skip copying\n                                                              jars to\n                                                              distributed\n                                                              cache\n   --temporary-rootdir "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("rootdir"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                              Defines the\n                                                              temporary\n                                                              root\n                                                              directory\n                                                              "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" the\n                                                              "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v("\n   --throw-on-error                                           Rethrow a\n                                                              RuntimeExcep\n                                                              tion on\n                                                              error\n                                                              occurred\n                                                              during the\n                                                              job\n   --username "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("username"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("                                      Set\n                                                              authenticati\n                                                              on username\n   --verbose                                                  Print "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("more")]),s._v("\n                                                              information\n                                                              "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("while")]),s._v("\n                                                              working\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("..")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("..")]),s._v("\nGeneric Hadoop command-line arguments:\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("must preceed any tool-specific arguments"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nGeneric options supported are\n-conf "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("configuration file"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("     specify an application configuration "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("file")]),s._v("\n-D "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("property"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("value"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("            use value "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" given property\n-fs "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("local"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("namenode:port"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("      specify a namenode\n-jt "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("local"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("resourcemanager:port"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("    specify a ResourceManager\n-files "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("comma separated list of files"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("    specify comma separated files to be copied to the map reduce cluster\n-libjars "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("comma separated list of jars"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("    specify comma separated jar files to include "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" the classpath.\n-archives "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("comma separated list of archives"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("    specify comma separated archives to be unarchived on the compute machines.\n\nThe general "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("command")]),s._v(" line syntax is\nbin/hadoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("command")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("genericOptions"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("commandOptions"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n\nAt minimum, you must specify --connect, --export-dir, and --table\n")])])]),t("p",[s._v("        可以看到列出了"),t("code",[s._v("export")]),s._v("命令的使用格式和参数信息。")]),s._v(" "),t("p",[s._v("        如果我们想将数据从数据存储空间导入到HDFS，那么我们就需要使用"),t("code",[s._v("import")]),s._v("命令：")]),s._v(" "),t("p",[s._v("        其中"),t("code",[s._v("import")]),s._v("命令常用的参数如下：")]),s._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200219093739974.jpg?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dhaXhpYW95YW5nMTIz,size_16,color_FFFFFF,t_70#pic_center",alt:""}}),s._v("\n        为了方便大家理解，下面我将通过一个例子来使用Sqoop")]),s._v(" "),t("h2",{attrs:{id:"_7、sqoop的使用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7、sqoop的使用"}},[s._v("#")]),s._v(" 7、Sqoop的使用")]),s._v(" "),t("h3",{attrs:{id:"_7-1-全量导入mysql的数据到hdfs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-1-全量导入mysql的数据到hdfs"}},[s._v("#")]),s._v(" 7.1 全量导入MySQL的数据到HDFS")]),s._v(" "),t("p",[s._v("        现在在MySQL的userdb数据库下有一张 emp 表，需要将表数据的内容全量导入到HDFS")]),s._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210124150602849.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}})]),s._v(" "),t("p",[s._v("        我们就只需要执行下面的Sqoop命令即可")]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://192.168.100.100:3306/userdb "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password hadoop "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--delete-target-dir "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--target-dir /sqoopresult "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table emp --m "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n")])])]),t("p",[s._v("        其中 "),t("code",[s._v("--target-dir")]),s._v(" 可以用来指定导出数据存放至HDFS的目录")]),s._v(" "),t("p",[s._v("        为了验证在HDFS导入的数据，请使用以下命令查看导入的数据：\n"),t("code",[s._v("hdfs dfs -cat /sqoopresult/part-m-00000")])]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7369")]),s._v(",SMITH,CLERK,7902,1980-12-17,800,,20\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7499")]),s._v(",ALLEN,SALESMAN,7698,1981-02-20,1600,300,30\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7521")]),s._v(",WARD,SALESMAN,7698,1981-02-22,1250,500,30\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7566")]),s._v(",JONES,MANAGER,7839,1981-04-02,2975,,20\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7654")]),s._v(",MARTIN,SALESMAN,7698,1981-09-28,1250,1400,30\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7698")]),s._v(",BLAKE,MANAGER,7839,1981-05-01,2850,,30\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7782")]),s._v(",CLARK,MANAGER,7839,1981-06-09,2450,,10\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7788")]),s._v(",SCOTT,ANALYST,7566,1987-04-19,3000,,20\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7839")]),s._v(",KING,PRESIDENT,,1981-11-17,5000,,10\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("7844")]),s._v(",TURNER,SALESMAN,7698,1981-09-08,1500,0,30\n")])])]),t("p",[s._v("        可以看出它会在HDFS上默认用逗号"),t("code",[s._v(",")]),s._v("分隔emp表的数据和字段。当然我们也可以通过"),t("code",[s._v("--fields-terminated-by '\\t'")]),s._v("来指定导出的分隔符")]),s._v(" "),t("h3",{attrs:{id:"_7-2-导入子数据集到hdfs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-2-导入子数据集到hdfs"}},[s._v("#")]),s._v(" 7.2 导入子数据集到HDFS")]),s._v(" "),t("p",[s._v("        上面我们导入的是全量的数据，很多时候，我们需要对数据进行过滤处理，导入的是原始数据的一个子数据集，那该怎么办呢？这里提供2种方式：")]),s._v(" "),t("h4",{attrs:{id:"_7-2-1-3-where过滤"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-2-1-3-where过滤"}},[s._v("#")]),s._v(" 7.2.1 3．where过滤")]),s._v(" "),t("p",[t("code",[s._v("--where")]),s._v("可以指定从关系数据库导入数据时的查询条件。它执行在数据库服务器相应的SQL查询，并将结果存储在 HDFS 的目标目录。")]),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://192.168.100.100:3306/sqoopdb "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password hadoop "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--where "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("\"sex ='male'\"")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--target-dir /wherequery "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table employee --m "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n")])])]),t("h4",{attrs:{id:"_7-2-1-4-query查询"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_7-2-1-4-query查询"}},[s._v("#")]),s._v(" 7.2.1.4 query查询")]),s._v(" "),t("p",[s._v("        需要注意："),t("font",{attrs:{color:"\tTomato"}},[s._v("使用query sql语句来进行查找不能加参数--table ;并且必须要添加where条件;where条件后面必须带一个$CONDITIONS 这个字符串;且这个sql语句必须用单引号，不能用双引号。")])],1),s._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://node-1:3306/userdb "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password hadoop "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--target-dir /wherequery12 "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--query "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'select id,name,deg from emp WHERE  id>1203 and $CONDITIONS'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--split-by "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--fields-terminated-by "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'\\t'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--m "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("\n")])])]),t("p",[s._v("        sqoop命令中，"),t("code",[s._v("--split-by")]),s._v(" id通常配合"),t("code",[s._v("-m")]),s._v(" 参数使用。用于指定根据哪个字段进行划分并启动多少个maptask。")]),s._v(" "),t("blockquote",[t("p",[t("font",{attrs:{color:"red"}},[s._v("注意，当 -m 设置的值大于1时，split-by必须设置字段，且只能是int类型")])],1)]),s._v(" "),t("p",[s._v("        另外，关于"),t("code",[s._v("--split-by")]),s._v("参数的深入理解大有学问：")]),s._v(" "),t("p",[s._v("        1、split-by 根据不同的参数类型有不同的切分方法，如 int 型，Sqoop会取最大和最小split-by字段值，然后根据传入的num-mappers来 确定划分几个区域。比如 select max(split_by),min(split-by) from 得到的 max(split-by)和min(split-by) 分别为1000和1，而num-mappers（-m）为2的话，则会分成两个区域 (1,500)和(501-1000),同时也会分成2个sql给2个map去进行导入操作，最后每个map各自获取各自SQL中的数据进行导入工作。")]),s._v(" "),t("p",[s._v("        2、split-by即便是int型，若不是"),t("font",{attrs:{color:"blue"}},[t("strong",[s._v("连续有规律递增")])]),s._v("的话，各个map分配的数据是"),t("font",{attrs:{color:"blue"}},[t("strong",[s._v("不均衡")])]),s._v("的，可能会有些map很忙，有些map几乎没有数据处理的情况，就容易出现"),t("font",{attrs:{color:"blue"}},[t("strong",[s._v("数据倾斜")])]),s._v("，所以一般"),t("code",[s._v("split by")]),s._v("的值为自增主键id。\n        ")],1),s._v(" "),t("h2",{attrs:{id:"ref"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ref"}},[s._v("#")]),s._v(" Ref")]),s._v(" "),t("blockquote",[t("p",[s._v("1、《海量数据处理与大数据实践》\n2、https://www.cnblogs.com/pejsidney/p/8962302.html\n3、《开源组件系列：关系型数据导入（Sqoop与Canal）》")])])])}),[],!1,null,null,null);a.default=n.exports}}]);