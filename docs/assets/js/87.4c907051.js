(window.webpackJsonp=window.webpackJsonp||[]).push([[87],{476:function(n,e,a){"use strict";a.r(e);var r=a(30),t=Object(r.a)({},(function(){var n=this,e=n.$createElement,a=n._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[a("h1",{attrs:{id:"字节抖音大数据开发校招面经总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#字节抖音大数据开发校招面经总结"}},[n._v("#")]),n._v(" "),a("a",{attrs:{href:"https://blog.csdn.net/Jiangzhiqi4551/article/details/109478941",target:"_blank",rel:"noopener noreferrer"}},[n._v("字节抖音大数据开发校招面经总结"),a("OutboundLink")],1)]),n._v(" "),a("h2",{attrs:{id:"问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#问题"}},[n._v("#")]),n._v(" 问题")]),n._v(" "),a("h3",{attrs:{id:"spark"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spark"}},[n._v("#")]),n._v(" Spark")]),n._v(" "),a("p",[n._v("1、Spark中Stage、Executor、Task都是干嘛的？\n2、Spark 数据倾斜\n3、spark为什么比mr快？\n4、Spark的stage的划分是怎么划分的？\n5、介绍一下 Spark RDD、DataFrame、DataSet\n6、Spark的窄依赖和宽依赖\n7、DAG Scheduler作用\n8、Spark中有哪些算子会进行shuffle，有哪些不会进行shuffle\n9、union可以形成宽依赖吗？\n10、Spark 作业调度\n11、Spark的提交模式\n12、yarn-cluster涉及的参数有哪些\n13、有关资源分配参数如何自动分配\n14、Spark shuffle 过程\n15、Driver 怎么管理 excutor的\n16、cache()算子\n17、广播变量介绍一下，什么场景下用广播变量？广播变量和cache区别？\n18、Spark的组件有哪些\n20、如果要增加并发应该使用什么参数，executor和core的比例怎么设置\n21、Spark join的几种实现方式\n22、map算子和flatMap\n23、Hive和Spark区别\nJava部分\n1、HashMap的实现？为什么要转化为红黑树？为什么大于8才转换？\n2、ArrayList和LinkedList的区别？\n3、CMS和G1垃圾收集器的区别？\n4、HashMap是线程安全吗？为什么？\n5、Java的IO模型？BIO和NIO的区别？\n6、常见的gc回收器和区别\n7、Map有哪几种\n8、ConcurrentHashMap怎么保证线程安全\n9、volatile和sychronized关键字原理\n10、hashcode（）相同，equals一定相同吗\n11、HashTable和HashMap有什么区别\n12、四种引用说一说\n13、抽象类与接口的区别\n14、JVM虚拟机，为什么需要虚拟机\n15、多线程和多进程有什么区别？\n16、java有哪些锁\n17、JVM的内存模型讲一下。\n18、hashmap冲突了怎么处理的？\n19、treemap底层是怎么实现的？\n20、线程的状态及状态之间的装换\nHive部分\n1、Hive有哪些引擎？\n2、udf，udtf 怎么写\n3、hive窗口函数row_number、rank、dense_rank的区别？\n4、Hive里面map和reduce的数量怎么确定？哪些参数可以调控map的数量\n5、mapjoin 问题？\n6、cluster by 、sort by、distribute by 、order by 区别\n7、left semi join 作用\n8、Hive分区分桶的区别\n9、Hive底层原理，sql执行过程\n10、HDFS中的数据怎么写入Hive中\n数仓部分\n1、数仓分层每层是做什么的？\n2、事实表维度表构建\n3、OLAP 与 OLTP 的区别\n4、行式数据库和列式数据库区别\nHadoop 部分\n1、MapReduce的shuffle机制？\n2、mr的combiner主要是做什么？\n3、介绍一下MapReduce的过程\n4、表A join 表B，讲一下MapReduce是怎样执行的\n5、NameNode和DataNode\n6、使用Hadoop需要启动哪些进程\n7、HDFS读写机制\n8、Hadoop序列化和反序列化\n9、Hadoop的架构\n10、Hadoop作业提交到Yarn流程\n11、NodeManage挂掉怎么办，两个任务同时请求资源怎么办\n12、Reduce如何获取Map的结果\n13、mapreduce shuffle为什么要环形缓冲区\n14、HDFS小文件过多会怎么样\n数据库部分\n1、MySQL的索引？b+树相对于b树有什么优点？\n2、MySQL 数据量大怎么办（索引、分库分表知识点）\n3、最左匹配原则，ACID\n4、覆盖索引\n5、MySQL中char和varchar有什么区别？\n6、Hash索引和B+Tree索引的区别\n7、联合索引是什么")]),n._v(" "),a("h3",{attrs:{id:"数据结构"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据结构"}},[n._v("#")]),n._v(" 数据结构")]),n._v(" "),a("p",[n._v("1、描述堆排序\n2、基数排序过程\n3、有哪些树结构\n4、B+树的特点\n5、排序算法都有哪些\n6、栈的push和pop的时间复杂度\n7、红黑树特性，红黑树如何实现")]),n._v(" "),a("h3",{attrs:{id:"计网"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#计网"}},[n._v("#")]),n._v(" 计网")]),n._v(" "),a("p",[n._v("1、TCP三次握手四次挥手？\n2、TIME-WAIT什么时候发生？持续时间？\n3、在不同的网络环境中MSL一样吗？\n4、解释最长报文段寿命？\n5、udp应用场景\n6、dns解析过程，本地在哪缓存，服务器上怎么缓存\n7、HTTPS的工作流程\n8、网络结构以及每层应用")]),n._v(" "),a("h3",{attrs:{id:"场景题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#场景题"}},[n._v("#")]),n._v(" 场景题")]),n._v(" "),a("p",[n._v("1、如何从百亿条IP信息中得出访问量前10的IP地址\n2、你自己如何设计一个分布式系统，实现对百亿条数据进行分组并求和\n3、在100亿个无符号整数中取最大100个 topK 有内存限制和无内存限制不考虑分布式\nLinux\n1、linux根目录有哪些文件夹\n2、查看进程的命令有哪些\n3、如何查找指定关键字的进程信息\n4、怎么知道系统CPU负载高")]),n._v(" "),a("h3",{attrs:{id:"coding"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#coding"}},[n._v("#")]),n._v(" Coding")]),n._v(" "),a("p",[n._v("1、实现左旋n位的字符数组？\n2、二叉树的广度优先遍历和深度优先遍历\n3、sql题，求连续活跃5天的用户\n4、实现微信发红包，输入是红包数和总金额，实现随机分配\n5、一个数组里面每个元素表示每天的股票价钱，怎么样买入和卖出能赚最多的钱？只能先买入再卖出\n6、n个个位数，全排列组成一个整数。怎么找到最近的下一个比他大的数。时间复杂度要求o（n）\n7、返回二叉树的镜像，非递归算法。\n8、抖音用户浏览视频日志 TableA（date, user_id, video_id）， 统计2020.03.29观看视频最多的前5个user_id（相同视频要排重）。\n9、leetcode80. 删除排序数组中的重复项 II\n10、数组求top k\n11、sql题 分组求每组top 3\n12、二叉树的后序遍历 非递归实现\n13、用spark求一下dau\n14、子序列的最大和\n15、最小逆序对\n16、二叉树层序遍历，按层换行输出。\n17、一个数组有正数有负数，调整数组中的数使得正负交替\n18、leetcode 字典序的第k小数字\n19、网上购物返现，假设你需要购买某一商品，每天都要购买一次，连续买n天，但每天有不同的价格。你知道每天的价格，如果某一天你花8块买了，未来某一天价格降到了1块，就会返现7块，只能返现一次。求连买n天商品的最小价钱。\n20、快排\n21、求一个成对数组中，只出现一次的数\n22、用两个栈实现栈的排序\n23、用两个栈实现队列\n24、lc丑数3\n25、数据流中的中位数\n26、Leecode513. 找树左下角的值，非递归与递归两种方法\n27、归并排序\n28、两个有序数组间相加和的Topk问题\n29、最少票价问题（动态规划求解）\n30、二叉树的序列化与反序列化\n31、二分查找\n32、无重复字符的最长子串\n33、大数组求中位数\n34、使用spark写一个wordcount\n35、两个字符串的最大公共子串\n36、信封问题\n37、两个二叉树如何判断是不是对称\n38、链表翻转\n39、N个有序数组，取其中m个最小的数，m远小于N个数组中数的个数，时间复杂度是多少\n40、对折链表\n41、给定误差范围，计算正浮点数x的平方根，不可以使用幂运算，可以用四则运算。\n42、给定数组包含正负数(数量至多差1)，要求将其排列成正负彼此相邻形式，要求时间复杂度O(N)，空间复杂度O(1)\n43、 1-N个数字，找出字典序第K大的数字，要求空间O(1)，时间O(K)\n44、给定一个序列和排序中间结果，判断是插入还是归并排序的中间结果，并输出下一次排序结果。\n45、非递归后序遍历\n46、二维数组的顺时针打印\n47、旋转数组的查找\n48、2个有序数组中位数，要求O(logN)\n49、判断一颗二叉树的宽度\n50、求字符串的最长回文子序列\n51、接雨水\n52、在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。\n53、我们可以用2"),a("em",[n._v("1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2")]),n._v("1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？")]),n._v(" "),a("h2",{attrs:{id:"答案"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#答案"}},[n._v("#")]),n._v(" 答案")]),n._v(" "),a("h3",{attrs:{id:"spark-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#spark-2"}},[n._v("#")]),n._v(" Spark")]),n._v(" "),a("p",[n._v("1、Spark中Stage、Executor、Task都是干嘛的？\nstage：每个job被划分为多个stage，一个stage中包含一个taskset\ntask：在executor进程中执行任务的工作单元\nExecutor：运行在worker节点上的一个进程，负责运行某些task，并将数据存在内存或者磁盘上。\nWorker：集群中可以运行Application代码的节点,在Spark on Yarn模式中指的就是NodeManager节点。\nDriver：main()函数，创建SparkContext，由SparkContext进行资源申请，任务的分配和监控等。程序执行完毕后关闭SparkContext。\n2、Spark 数据倾斜\n原理")]),n._v(" "),a("p",[n._v("shuffle时各个task处理的数据量不均匀")]),n._v(" "),a("p",[n._v("如何定位数据倾斜")]),n._v(" "),a("p",[n._v("只会发生在shuffle过程中，可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等\n在web UI上查看，看日志，在代码第几行溢出，附近的shuffle算子\n查看出现问题的RDD或hive表的key分布状况，用countByKey统计一下\n解决方案")]),n._v(" "),a("p",[n._v("预处理，过滤导致倾斜的key\n提高shuffle的并行度\n将reduce join替换为map join：将小的broadcast广播，在大的里面用map遍历，看key是否相同\n两阶段聚合（局部聚合+全局聚合），打上随机前缀，接着对打上随机数后的数据，执行reduceByKey等聚合操作，再对结果进行全局聚合，去掉前缀\n使用随机前缀和扩容RDD进行join\n3、spark为什么比mr快？\nshuffle过程中Hadoop需要落硬盘，spark不一定，可以缓存在内存中，IO少\nHadoop的task是一个个进程，spark是一个个线程；\nHadoop容错性低，spark可以根据RDD之间的血缘关系重算；\nspark有transform和action算子，只有遇到action才会触发job，会做流水线层面的优化；\n多次使用的RDDspark可以进行缓存\n4、Spark的stage的划分是怎么划分的？\n在spark中，会根据RDD之间的依赖关系划分DAG图，对于窄依赖，由于partition依赖关系的确定性，可以在同一个线程里完成，窄依赖就被spark划分到同一个stage中。")]),n._v(" "),a("p",[n._v("stage划分思路：从后往前推，遇到宽依赖就断开，划分为一个stage，遇到窄依赖就将这个RDD加入该stage中。")]),n._v(" "),a("p",[n._v("5、介绍一下 Spark RDD、DataFrame、DataSet\nRDD：弹性分布式数据集，spark中最基本的数据抽象，不可变，可分区，里面元素可并行计算\nDataFrame是一种以RDD为基础的不可变分布式数据集，DataFrame与RDD的主要区别在于，DF带有schema元信息，数据都被组织到有名字的列中，每一列都带有名称和类型\nDataSet：和dataFrame差不多，可以把 DataFrame 当作 Dataset[Row] 的别名，每一行的类型是Row；\n6、Spark的窄依赖和宽依赖\n窄依赖：每个父RDD的一个分片最多被子RDD的一个分片使用。窄依赖以pipeline管道形式顺序执行多条命令，分区内的计算收敛，可以并行地在不同节点进行计算。失败时只需要计算丢失的parent partition")]),n._v(" "),a("p",[n._v("宽依赖：父RDD的分片会被多个子RDD的分片使用。需要所有的父分区都是可用的，失败恢复时，宽依赖牵涉RDD各级的多个parent partition")]),n._v(" "),a("p",[n._v("7、DAG Scheduler作用\n接收提交的job的主入口，根据job构建基于stage的DAG，并将每个Stage打包(task的分区信息和方法，并序列化)成TaskSet交给TaskScheduler调度。stage的划分依据是RDD之间的依赖关系。还会处理 task 执行成功 ， cancel job ， cancel stage 等事件。")]),n._v(" "),a("p",[n._v("8、Spark中有哪些算子会进行shuffle，有哪些不会进行shuffle\n***有 shuffle ***\ndistinct 、 intersection 、 substract 、substractByKey 、join 、lefouterjoin、\nreduceByKey 、 group by 、 groupbykey 、 aggregratebykey 、 combinebykey 、\nsortby 、 sortbykey 、 coalease 、 repartition")]),n._v(" "),a("p",[n._v("无 shuffle")]),n._v(" "),a("p",[n._v("map 、 flatmap 、 mapPartitions 、 filter")]),n._v(" "),a("p",[n._v("9、union可以形成宽依赖吗？\nunion 为窄依赖，不会触发 shuffle ， 不会划分 stage ， 可能会重新分区")]),n._v(" "),a("p",[n._v("10、Spark 作业调度\n调度主要通过 DAG scheduler 和 Task Scheduler。")]),n._v(" "),a("p",[n._v("DAGScheduler：接收提交的job的主入口，根据job构建基于stage的DAG，并将每个Stage打包成TaskSet交给TaskScheduler调度。")]),n._v(" "),a("p",[n._v("TaskScheduler ：将发来的 taskSet 封装为TaskSetManager加入到调度队列中，TaskSetManager负责监控管理同一个Stage中的Tasks，TaskScheduler就是以TaskSetManager为单元来调度任务。")]),n._v(" "),a("p",[n._v("SchedulerBackend 会定期询问 TaskScheduler 有没有任务要运行，TaskScheduler会从调度队列中按照指定的调度策略（FIFO（默认）、FAIR）选择 TaskSetManager 去调度运行。")]),n._v(" "),a("p",[n._v("11、Spark的提交模式\nstandalone：基于Spark自己的Master-Worker集群。没用Yarn。\nyarn-cluster：driver运行在AM中，当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，不适合运行交互类型的作业。\nyarn-client：Client不能离开，Driver在哪提交就在哪\n12、yarn-cluster涉及的参数有哪些\nexecutor-memory")]),n._v(" "),a("p",[n._v("spark.driver.memory")]),n._v(" "),a("p",[n._v("spark.driver.maxResultSize")]),n._v(" "),a("p",[n._v("spark.executor.cores")]),n._v(" "),a("p",[n._v("spark.driver.cores")]),n._v(" "),a("p",[n._v("spark.sql.shuffle.partitions")]),n._v(" "),a("p",[n._v("spark.serializer")]),n._v(" "),a("p",[n._v("spark.network.timeout")]),n._v(" "),a("p",[n._v("spark.yarn.executor.memoryOverhead")]),n._v(" "),a("p",[n._v("spark.dynamicAllocation.maxExecutors")]),n._v(" "),a("p",[n._v("13、有关资源分配参数如何自动分配\nspark.dynamicAllocation.enabled 资源动态分配")]),n._v(" "),a("p",[n._v("spark.dynamicAllocation.minExecutors 动态最小的executor数量")]),n._v(" "),a("p",[n._v("spark.dynamicAllocation.maxExecutors 动态最大的executor数量")]),n._v(" "),a("p",[n._v("14、Spark shuffle 过程\n15、Driver 怎么管理 excutor的\n16、cache()算子\n17、广播变量介绍一下，什么场景下用广播变量？广播变量和cache区别？\n18、Spark的组件有哪些\n20、如果要增加并发应该使用什么参数，executor和core的比例怎么设置\n21、Spark join的几种实现方式\n总体上来说，Join的基本实现为，Spark将参与Join的两张表抽象为流式遍历表(streamIter)和查找表(buildIter)，通常streamIter为大表，buildIter为小表，spark会基于streamIter来遍历，每次取出streamIter中的一条记录rowA，根据Join条件计算keyA，然后根据该 keyA 去 buildIter 中查找所有满足Join条件(keyB==keyA)的记录 rowBs，并将rowBs中每条记录分别与 rowA join得到 join 后的记录。")]),n._v(" "),a("p",[n._v("sort merge join：分别对 streamIter 和 buildIter 进行排序，在遍历streamIter时，对于每条记录，都采用顺序查找的方式从 buildIter 查找对应的记录，每次处理完 streamIter 的一条记录后，对于streamIter的下一条记录，只需从 buildIter 中上一次查找结束的位置开始查找，不必从头找。")]),n._v(" "),a("p",[n._v("broadcast join：如果 buildIter 是一个非常小的表，直接将buildIter广播到每个计算节点，直接在一个map中完成 join，也叫map join。")]),n._v(" "),a("p",[n._v("hash join：将来自buildIter的记录放到 hash表 中，加快查找")]),n._v(" "),a("p",[n._v("22、map算子和flatMap\nmap：将一个RDD中的每个数据项，通过map中的函数映射变为一个新的元素，有多少个输入分区，就有多少个输出分区")]),n._v(" "),a("p",[n._v("flatmap：Transformation算子，第一步和map一样，最后将所有的输出分区合并成一个分区，flatMap会将字符串看成是一个字符数组，并不会把Array[String]也扁平化成字符数组")]),n._v(" "),a("p",[n._v("23、Hive和Spark区别")]),n._v(" "),a("h3",{attrs:{id:"java"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#java"}},[n._v("#")]),n._v(" Java")]),n._v(" "),a("p",[n._v("1、HashMap的实现？为什么要转化为红黑树？为什么大于8才转换？\n实现")]),n._v(" "),a("p",[n._v("由数组和链表组合构成（可以从1.7、1.8展开详细讲）")]),n._v(" "),a("p",[n._v("为什么要转化为红黑树")]),n._v(" "),a("p",[n._v("链表是取一个数需要遍历链表，复杂度为O(N)，而红黑树为O(logN)")]),n._v(" "),a("p",[n._v("为什么大于8才转换")]),n._v(" "),a("p",[n._v("哈希表中节点的频率遵循泊松分布，根据统计，列表长度为8的时候期望就已经很小了，没必要再往后调整。（源码中注释部分有具体解释）")]),n._v(" "),a("p",[n._v("2、ArrayList和LinkedList的区别？\nArrayList 底层是数组，查询效率高，增删效率低，默认初始长度为10，数组扩容新=1.5*旧，然后把原数组的数据，原封不动的复制到新数组中（用的是 Arrays.copyOf() 方法 ），LinkedList 增删快，底层为双向链表，遍历ArrayList要比LinkedList快，遍历的优势在于内存的连续性，CPU的内部缓存结构会缓存连续的内存片段，可以大幅降低读取内存的性能开销。")]),n._v(" "),a("p",[n._v("3、CMS和G1垃圾收集器的区别？\nCMS采用 标记-清理 的算法：")]),n._v(" "),a("p",[n._v("初始标记：stop the world，仅仅只是标记一下GC Roots能直接关联到的对象，速度很快\n并发标记（耗时最长）：进行GC Roots Tracing的过程，收集器线程与用户线程一起工作\n重新标记：stop the world，为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，时间比初始标记阶段稍长一些，但远比并发标记的时间短。\n并发清除（耗时最长）：并发回收垃圾，收集器线程与用户线程一起工作\nG1：")]),n._v(" "),a("p",[n._v("面向服务端应用的垃圾收集器，整体看基于标记整理算法，局部看基于复制（两个region之间），不会产生内存空间碎片，可预测停顿，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，使用G1收集器时，它将整个Java堆划分为多个大小相等的独立区域（Region），新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。G1跟踪各个region中垃圾堆积的价值大小（回收空间及所需时间），在后台维护一个优先列表，在每次被允许的时间内，优先回收高价值的。")]),n._v(" "),a("p",[n._v("初始标记：stop the world，仅仅只是标记一下GC Roots能直接关联到的对象，速度很快\n并发标记：进行GC Roots Tracing的过程（进行可达性分析），可与用户程序并发执行\n最终标记：stop the world，为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，并行执行\n筛选回收：对各个region回收价值和成本排序，制定回收计划。\n4、HashMap是线程安全吗？为什么？\n非线程安全。")]),n._v(" "),a("p",[n._v("JDK7中：多线程并发put，造成循环链表，get时出现死循环。")]),n._v(" "),a("p",[n._v("多线程执行 put：线程A、B同时put，同时执行到 if ((e = p.next) == null)，同时进入下一步，先是 e->A ，然后 e -> B ， 而不是 e -> A -> B ，丢失A。")]),n._v(" "),a("p",[n._v("put、get并发时：put触发resize，get得到的可能是null。")]),n._v(" "),a("p",[n._v("5、Java的IO模型？BIO和NIO的区别？\nBIO：面向流，是阻塞IO，在读取数据时，用户线程发出IO请求后，内核会检查数据是否就绪，没有就绪的话就继续等待数据就绪，用户线程处于阻塞状态并交出CPU，就绪后，内核将数据拷贝到用户线程，接触阻塞状态。")]),n._v(" "),a("p",[n._v("NIO：同步非阻塞，用户线程发起读请求之后，不需要等待，立刻会得到一个结果，结果是error时，说明数据还没准备好，可以再次发送读请求，一旦数据准备好之后，并且再次收到了读请求，就将数据拷贝到用户线程。")]),n._v(" "),a("p",[n._v("AIO：异步非阻塞，用户发起一个IO之后立即返回，也不用自己去轮询，当IO完成之后，内核会给用户线程发信号，告知IO已完成")]),n._v(" "),a("p",[n._v("6、常见的gc回收器和区别\nSerial\nParNew\nParallel\nCMS\nG1\n7、Map有哪几种\nHashTable，实现了map接口\nHashMap\nTreeMap（基于红黑树，有序）\nLinkedHashMap（HashMap的基础上加上双向链表，非同步）\nConcurrentHashMap\n8、ConcurrentHashMap怎么保证线程安全\n底层是基于 数组 + 链表")]),n._v(" "),a("p",[n._v("1.7：由 Segment 数组、HashEntry 组成，采用了分段锁技术，支持 Segment 数组数量的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。")]),n._v(" "),a("p",[n._v("1.8：采用了 CAS （乐观锁）+ synchronized 来保证并发安全性，链表长度>8转为红黑树")]),n._v(" "),a("p",[n._v("9、volatile和sychronized关键字原理\n10、hashcode（）相同，equals一定相同吗\n11、HashTable和HashMap有什么区别\nHashtable 是不允许键或值为 null 的（在put 空值的时候直接抛空指针异常），HashMap 的键值则都可以为 null\nHashtable 继承了 Dictionary类，而 HashMap 继承的是 AbstractMap 类\nHashMap 的初始容量为：16，Hashtable 初始容量为：11\nHashMap 扩容规则为2倍，Hashtable 扩容规则为2倍 + 1\n12、四种引用说一说\n强：存在就不会被回收\n软：第一次回收后，空间还是不够，进行第二次回收，会回收软引用\n弱：第一次就被回收\n虚：对象被回收时收到一个系统通知\n13、抽象类与接口的区别\n14、JVM虚拟机，为什么需要虚拟机\n15、多线程和多进程有什么区别？\n16、java有哪些锁\n17、JVM的内存模型讲一下。\n18、hashmap冲突了怎么处理的？\n19、treemap底层是怎么实现的？\n20、线程的状态及状态之间的装换\nHive部分\n1、Hive有哪些引擎？\nMR、Tez")]),n._v(" "),a("p",[n._v("2、udf，udtf 怎么写\n继承UDF 类，重写 evaluate 方法")]),n._v(" "),a("p",[n._v("继承 UDTF类，重写process方法，调用forward方法输出数据。（之前用阿里的MaxCompute是继承 UDTF，Hive的实现上可能有区别）")]),n._v(" "),a("p",[n._v("3、hive窗口函数row_number、rank、dense_rank的区别？\nrow_number：分组排序 ，123345\nrank：数据在分组中的排名，排名相等会留下空位，12335\ndense_rank：数据在分组中的排名，排名相等不会留下空位，12334\n4、Hive里面map和reduce的数量怎么确定？哪些参数可以调控map的数量\nmap\n没有办法直接设置，主要决定因素为：输入文件的数量、大小，集群设置的文件块的大小")]),n._v(" "),a("p",[n._v("-- 决定每个map处理的最大的文件大小\nset mapred.max.split.size=100000000;")]),n._v(" "),a("p",[n._v("1\n2\n3\nreduce")]),n._v(" "),a("p",[n._v("-- 设置reduce的数量\nset mapred.reduce.tasks = 15;")]),n._v(" "),a("p",[n._v("1\n2\n3\n不指定的话，默认的计算方式为：min（每个任务最大的reduce数，总输入数据量/每个reduce任务处理的数据量）")]),n._v(" "),a("p",[n._v("5、mapjoin 问题？\n小表join大表，小表在左边，大表右边，将小表读入内存，在map端join大表，省略reduce")]),n._v(" "),a("p",[n._v("6、cluster by 、sort by、distribute by 、order by 区别\norder by：全局排序，所有数据都通过一个reducer进行处理，数据集过大可能极慢\nsort by：在每个reducer中进行排序，保证每个reducer的输出有序，但并非全局有序\ndistribute by：控制map的输出在reducer中如何划分，distribute by year 年份相同的数据都分到一起\ncluster by：distribute by year+sort by year asc（升序）的简写方式 = cluster by year\n7、left semi join 作用\nleft semi join（左半开连接）：返回左表的记录，其记录对于右表满足on的筛选条件（可替代in）")]),n._v(" "),a("p",[n._v("8、Hive分区分桶的区别\n分区：将数据从物理上转移到和使用最频繁的用户更近的地方，分区表改变了hive对数据存储的组织方式，warehouse/数据库/表名 —> warehouse/数据库/表名/dt=2020，分区后hive将创建可以反映分区结构的子目录。")]),n._v(" "),a("p",[n._v("分桶：分桶则是指定分桶表的某一列，让该列数据按照哈希取模的方式随机、均匀地分发到各个桶文件中。因为分桶改变了数据的存储方式，它会把哈希取模相同或者在某一区间的数据行放在同一个桶文件中。提高查询效率（对两张在同一列上进行了分桶操作的表进行JOIN操作的时候，只需要对保存相同列值的桶进行JOIN操作即可）")]),n._v(" "),a("p",[n._v("create table ... cluster by user_id into 96 buckets\n1\n插入时需要 set hive.enforce.bucketing=true 强制hive为目标分桶表设置正确的reducer数，如果不指定，需要自己设置正确的reducer数：set mapred.reduce.tasks=96")]),n._v(" "),a("p",[n._v("9、Hive底层原理，sql执行过程\nHiveSQL通过SQL Parser，完成SQL词法，语法解析，将SQL转化为抽象语法树(AST)\n遍历AST，进行Analyzer语义分析，抽象出查询的基本组成单元QueryBlock(QB)\n生成逻辑执行计划：遍历QueryBlock，翻译为执行操作树Operator Tree\n优化逻辑执行计划：逻辑层优化器进行OperatorTree变换，合并不必要的操作符，减少shuffle数据量，得到优化后的操作树\n生成物理执行计划：遍历OperatorTree，翻译为MapReduce任务\n优化物理执行计划：物理层优化器进行MapReduce任务的变换，生成最终的执行计划\n10、HDFS中的数据怎么写入Hive中\nload data inpath ‘文件路径’ into table 对应库表名")]),n._v(" "),a("p",[n._v("数仓部分\n1、数仓分层每层是做什么的？\n操作数据层\n数据仓库层（明细数据层、轻度聚合层）\n数据应用层\n维度表\n临时表\n2、事实表维度表构建\n3、OLAP 与 OLTP 的区别\n4、行式数据库和列式数据库区别\nHadoop 部分\n1、MapReduce的shuffle机制？\n2、mr的combiner主要是做什么？\n3、介绍一下MapReduce的过程\n4、表A join 表B，讲一下MapReduce是怎样执行的\n5、NameNode和DataNode\n6、使用Hadoop需要启动哪些进程\n7、HDFS读写机制\n8、Hadoop序列化和反序列化\n9、Hadoop的架构\n10、Hadoop作业提交到Yarn流程\n11、NodeManage挂掉怎么办，两个任务同时请求资源怎么办\n12、Reduce如何获取Map的结果\n13、mapreduce shuffle为什么要环形缓冲区\n14、HDFS小文件过多会怎么样\n小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能\nHDFS中，小文件过多会占用大量内存，NameNode内存容量严重制约了集群的扩展\nHDFS读写小文件更加耗时，每次都需要从 NN 获取元信息，并与对应的DataNode建立连接\n小文件是如何产生的")]),n._v(" "),a("p",[n._v("动态分区插入数据，产生大量的小文件\n参数设置的 reduce 数量太多\n数据源本身就包含大量的小文件\n小文件的解决方案")]),n._v(" "),a("p",[n._v("设置合并参数；\n少使用textfile；\n少用动态分区；\n设置reduce数量\n对于已有的小文件的解决方案")]),n._v(" "),a("p",[n._v("Hadoop archive命令进行归档；\n重跑数据，指定map reduce参数，指定存储格式（ORC）")]),n._v(" "),a("h3",{attrs:{id:"数据库部分"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据库部分"}},[n._v("#")]),n._v(" 数据库部分")]),n._v(" "),a("p",[n._v("1、MySQL的索引？b+树相对于b树有什么优点？\n2、MySQL 数据量大怎么办（索引、分库分表知识点）\n3、最左匹配原则，ACID\n4、覆盖索引\n5、MySQL中char和varchar有什么区别？\n6、Hash索引和B+Tree索引的区别\n7、联合索引是什么\n数据结构\n1、描述堆排序\n2、基数排序过程\n3、有哪些树结构\n4、B+树的特点\n5、排序算法都有哪些\n6、栈的push和pop的时间复杂度\n7、红黑树特性，红黑树如何实现\n计网\n1、TCP三次握手四次挥手？\n2、TIME-WAIT什么时候发生？持续时间？\n3、在不同的网络环境中MSL一样吗？\n4、解释最长报文段寿命？\n5、udp应用场景\n6、dns解析过程，本地在哪缓存，服务器上怎么缓存\n7、HTTPS的工作流程\n8、网络结构以及每层应用\n场景题\n1、如何从百亿条IP信息中得出访问量前10的IP地址\n2、你自己如何设计一个分布式系统，实现对百亿条数据进行分组并求和\n3、在100亿个无符号整数中取最大100个 topK 有内存限制和无内存限制不考虑分布式\nLinux\n1、linux根目录有哪些文件夹\n/sys 存储设备驱动信息")]),n._v(" "),a("p",[n._v("/home,/root,用户主目录")]),n._v(" "),a("p",[n._v("/usr")]),n._v(" "),a("p",[n._v("/etc,配置文件的存放位置")]),n._v(" "),a("p",[n._v("/lib,存放库文件")]),n._v(" "),a("p",[n._v("/bin ， 可执行命令")]),n._v(" "),a("p",[n._v("/sbin ， 管理员命令")]),n._v(" "),a("p",[n._v("2、查看进程的命令有哪些\nps aux")]),n._v(" "),a("p",[n._v("top")]),n._v(" "),a("p",[n._v("3、如何查找指定关键字的进程信息\n通过管道")]),n._v(" "),a("p",[n._v("ps aux | grep xxx")]),n._v(" "),a("p",[n._v("4、怎么知道系统CPU负载高\ntop命令")]),n._v(" "),a("h3",{attrs:{id:"coding-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#coding-2"}},[n._v("#")]),n._v(" Coding")]),n._v(" "),a("p",[n._v("1、实现左旋n位的字符数组？\n2、二叉树的广度优先遍历和深度优先遍历\n3、sql题，求连续活跃5天的用户\n4、实现微信发红包，输入是红包数和总金额，实现随机分配\n5、一个数组里面每个元素表示每天的股票价钱，怎么样买入和卖出能赚最多的钱？只能先买入再卖出\n6、n个个位数，全排列组成一个整数。怎么找到最近的下一个比他大的数。时间复杂度要求o（n）\n7、返回二叉树的镜像，非递归算法。\n8、抖音用户浏览视频日志 TableA（date, user_id, video_id）， 统计2020.03.29观看视频最多的前5个user_id（相同视频要排重）。\n9、leetcode80. 删除排序数组中的重复项 II\n10、数组求top k\n11、sql题 分组求每组top 3\n12、二叉树的后序遍历 非递归实现\n13、用spark求一下dau\n14、子序列的最大和\n15、最小逆序对\n16、二叉树层序遍历，按层换行输出。\n17、一个数组有正数有负数，调整数组中的数使得正负交替\n例：[-3, 6, 7, -4] ->[6, -3, 7, -4]")]),n._v(" "),a("p",[n._v("18、leetcode 字典序的第k小数字\n19、网上购物返现，假设你需要购买某一商品，每天都要购买一次，连续买n天，但每天有不同的价格。你知道每天的价格，如果某一天你花8块买了，未来某一天价格降到了1块，就会返现7块，只能返现一次。求连买n天商品的最小价钱。\n例如，输入[3,2,1,5]，那么总共花了11块，但返现了3元，所以最少花了8块买了4件商品。")]),n._v(" "),a("p",[n._v("20、快排\n21、求一个成对数组中，只出现一次的数\n22、用两个栈实现栈的排序\n23、用两个栈实现队列\n24、lc丑数3\n25、数据流中的中位数\n26、Leecode513. 找树左下角的值，非递归与递归两种方法\n27、归并排序\n28、两个有序数组间相加和的Topk问题\n29、最少票价问题（动态规划求解）\n30、二叉树的序列化与反序列化\n31、二分查找\n32、无重复字符的最长子串\n33、大数组求中位数\n34、使用spark写一个wordcount\n35、两个字符串的最大公共子串\n36、信封问题\n37、两个二叉树如何判断是不是对称\n38、链表翻转\n39、N个有序数组，取其中m个最小的数，m远小于N个数组中数的个数，时间复杂度是多少\n40、对折链表\n41、给定误差范围，计算正浮点数x的平方根，不可以使用幂运算，可以用四则运算。\n42、给定数组包含正负数(数量至多差1)，要求将其排列成正负彼此相邻形式，要求时间复杂度O(N)，空间复杂度O(1)\n43、 1-N个数字，找出字典序第K大的数字，要求空间O(1)，时间O(K)\n举例1-19字典序为1,10,11,12,13,14,15,16,17,18,19,2,3,4,5,6,7,8,9")]),n._v(" "),a("p",[n._v("44、给定一个序列和排序中间结果，判断是插入还是归并排序的中间结果，并输出下一次排序结果。\n45、非递归后序遍历\n46、二维数组的顺时针打印\n47、旋转数组的查找\n48、2个有序数组中位数，要求O(logN)\n49、判断一颗二叉树的宽度\n50、求字符串的最长回文子序列\n51、接雨水\n52、在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。\n53、我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？")])])}),[],!1,null,null,null);e.default=t.exports}}]);