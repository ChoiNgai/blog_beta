(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{401:function(a,t,e){"use strict";e.r(t);var s=e(30),o=Object(s.a)({},(function(){var a=this,t=a.$createElement,e=a._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"hdfs架构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hdfs架构"}},[a._v("#")]),a._v(" HDFS架构")]),a._v(" "),e("p",[a._v("        HDFS 是 Hadoop 中存储数据的基石，存储着所有的数据，具有高可靠性，高容错性，高可扩展性，高吞吐量等特征，能够部署在大规模廉价的集群上，极大地降低了部署成本。有意思的是，"),e("strong",[a._v("其良好的架构特征使其能够存储海量的数据")]),a._v("。")]),a._v(" "),e("p",[a._v("        HDFS采用 "),e("code",[a._v("Master/Slave")]),a._v(" 架构存储数据，且支持 NameNode 的 HA。HDFS架构主要包含客户端，"),e("code",[a._v("NameNode")]),a._v("，"),e("code",[a._v("SecondaryNameNode")]),a._v(" 和 "),e("code",[a._v("DataNode")]),a._v(" 四个重要组成部分，如图所示：")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210228145111743.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}})]),a._v(" "),e("p",[a._v("        （1）客户端向NameNode发起请求，获取元数据信息，这些元数据信息包括命名空间、块映射信息及 DataNode 的位置信息等。")]),a._v(" "),e("p",[a._v("        （2）NameNode 将元数据信息返回给客户端。")]),a._v(" "),e("p",[a._v("        （3）客户端获取到元数据信息后，到相应的 DataNode 上读/写数据")]),a._v(" "),e("p",[a._v("        （4）相关联的 DataNode 之间会相互复制数据，以达到 DataNode 副本数的要求")]),a._v(" "),e("p",[a._v("        （5）DataNode 会定期向 NameNode 发送心跳信息，将自身节点的状态信息报告给 NameNode。")]),a._v(" "),e("p",[a._v("        （6）SecondaryNameNode 并不是 NameNode 的备份。SecondaryNameNode 会定期获取 NameNode 上的 "),e("code",[a._v("fsimage")]),a._v("和 "),e("code",[a._v("edits log")]),a._v(" 日志，并将二者进行合并，产生 "),e("code",[a._v("fsimage.ckpt")]),a._v(" 推送给 NameNode。")]),a._v(" "),e("h3",{attrs:{id:"_1、namenode"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1、namenode"}},[a._v("#")]),a._v(" 1、NameNode")]),a._v(" "),e("p",[e("font",{attrs:{color:"DarkCyan"}},[a._v("NameNode 是整个 Hadooop 集群中至关重要的组件，它维护着整个 HDFS 树，以及文件系统树中所有的文件和文件路径的元数据信息")]),a._v("。这些元数据信息包括"),e("strong",[a._v("文件名")]),a._v("，"),e("strong",[a._v("命令空间")]),a._v("，"),e("strong",[a._v("文件属性")]),a._v("（文件生成的时间、文件的副本数、文件的权限）、"),e("strong",[a._v("文件数据块")]),a._v("、"),e("strong",[a._v("文件数据块与所在 DataNode 之间的映射关系")]),a._v("等。")],1),a._v(" "),e("p",[e("font",{attrs:{color:"tomato"}},[a._v("一旦 NameNode 宕机或 NameNode 上的元数据信息损坏或丢失，基本上就会丢失 Hadoop 集群中存储的所有数据，整个 Hadoop 集群也会随之瘫痪")]),a._v("。")],1),a._v(" "),e("p",[a._v("        在 Hadoop 运行的过程中， NameNode 的主要功能如下图所示：\n"),e("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210228151714899.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}})]),a._v(" "),e("h3",{attrs:{id:"_2、secondarynamenode"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2、secondarynamenode"}},[a._v("#")]),a._v(" 2、SecondaryNameNode")]),a._v(" "),e("p",[e("font",{attrs:{color:"DarkOrchid"}},[a._v("SecondaryNameNode 并不是 NameNode 的备份，在NameNode 发生故障时也不能立刻接管 NameNode 的工作")]),a._v("。SecondaryNameNode 在 Hadoop 运行的过程中具有两个作用：一个是"),e("strong",[a._v("备份数据镜像")]),a._v("，另一个是"),e("strong",[a._v("定期合并日志与镜像")]),a._v("，因此可以称其为 Hadoop 的"),e("strong",[a._v("检查点")]),a._v("（checkpoint）。"),e("font",{attrs:{color:"RoyalBlue"}},[a._v("SecondaryNameNode 定期合并 NameNode 中的 fsimage 和 edits log，能够防止 NameNode 重启时把整个 fsimage 镜像文件加载到内存，耗费过长的启动时间")]),a._v("。")],1),a._v(" "),e("p",[a._v("        SecondaryNameNode 的工作流程如图所示：")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210228175857945.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}})]),a._v(" "),e("p",[a._v("​        "),e("strong",[a._v("SecondaryNameNode的工作流程如下：")])]),a._v(" "),e("p",[a._v("        （1）"),e("code",[a._v("SecondaryNameNode")]),a._v(" 会通知 "),e("code",[a._v("NameNode")]),a._v(" 生成新的 "),e("code",[a._v("edits log")]),a._v(" 日志文件。")]),a._v(" "),e("p",[a._v("        （2）"),e("code",[a._v("NameNode")]),a._v(" 生成新的 "),e("code",[a._v("edits log")]),a._v(" 日志文件，然后将新的日志信息写到新生成的 "),e("code",[a._v("edits log")]),a._v(" 日志文件中。")]),a._v(" "),e("p",[a._v("        （3）"),e("code",[a._v("SecondaryNameNode")]),a._v(" 复制 "),e("code",[a._v("NameNode")]),a._v(" 上的 "),e("code",[a._v("fsimage")]),a._v(" 镜像和 "),e("code",[a._v("edits log")]),a._v(" 日志文件，此时使用的是 "),e("strong",[a._v("http get")]),a._v(" 方式。")]),a._v(" "),e("p",[a._v("        （4）"),e("code",[a._v("SecondaryNameNode")]),a._v(" 将"),e("code",[a._v("fsimage")]),a._v("将镜像文件加载到内存中，然后执行 "),e("code",[a._v("edits log")]),a._v(" 日志文件中的操作，生成新的镜像文件 "),e("code",[a._v("fsimage.ckpt")]),a._v("。")]),a._v(" "),e("p",[a._v("        （5）"),e("code",[a._v("SecondaryNameNode")]),a._v(" 将 "),e("code",[a._v("fsimage.ckpt")]),a._v(" 文件发送给 "),e("code",[a._v("NameNode")]),a._v("，此时使用的是 "),e("strong",[a._v("http post")]),a._v(" 方式。")]),a._v(" "),e("p",[a._v("        （6）"),e("code",[a._v("NameNode")]),a._v(" 将 "),e("code",[a._v("edits log")]),a._v(" 日志文件替换成新生成的 "),e("code",[a._v("edits.log")]),a._v(" 日志文件，同样将 fsimage文件替换成 "),e("code",[a._v("SecondaryNameNode")]),a._v(" 发送过来的新的 "),e("code",[a._v("fsimage")]),a._v(" 文件。")]),a._v(" "),e("p",[a._v("        （7）"),e("code",[a._v("NameNode")]),a._v(" 更新 "),e("code",[a._v("fsimage")]),a._v(" 文件，将此次执行 "),e("code",[a._v("checkpoint")]),a._v(" 的时间写入 fstime 文件中。")]),a._v(" "),e("p",[a._v("        经过 "),e("code",[a._v("SecondaryNameNode")]),a._v(" 对 "),e("code",[a._v("fsimage")]),a._v(" 镜像文件和 "),e("code",[a._v("edits log")]),a._v(" 日志文件的复制和合并操作之后，"),e("code",[a._v("NameNode")]),a._v(" 中的 "),e("code",[a._v("fsimage")]),a._v(" 镜像文件就保存了最新的 "),e("code",[a._v("checkpoint")]),a._v(" 的元数据信息， "),e("code",[a._v("edits log")]),a._v(" 日志文件也会重新写入数据，两个文件中的数据不会变得很大。因此，当重启 "),e("code",[a._v("NameNode")]),a._v(" 时，不会耗费太长的启动时间。")]),a._v(" "),e("p",[e("strong",[a._v("SecondaryNameNode 周期性地进行 checkpoint 操作需要满足一定的前提条件，这些条件如下")]),a._v("：")]),a._v(" "),e("p",[a._v("        （1）"),e("code",[a._v("edits log")]),a._v(" 日志文件的大小达到了一定的阈值，此时会对其进行合并操作。")]),a._v(" "),e("p",[a._v("        （2）每隔一段时间进行 "),e("strong",[a._v("checkpoint")]),a._v(" 操作。")]),a._v(" "),e("p",[a._v("        这些条件可以在"),e("code",[a._v("core-site.xml")]),a._v("文件中进行配置和调整，代码如下所示：")]),a._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("property"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n         "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("name"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("fs.checkpoint.period"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("/name"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n         "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("value"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("360")]),e("span",{pre:!0,attrs:{class:"token operator"}},[e("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[a._v("0")]),a._v("<")]),a._v("/value"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("/property"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("property"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n         "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("name"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("fs.checkpoint.size"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("/name"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n         "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("value"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("6710886")]),e("span",{pre:!0,attrs:{class:"token operator"}},[e("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[a._v("4")]),a._v("<")]),a._v("/value"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v("/property"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n")])])]),e("p",[a._v("        上述代码配置了 "),e("code",[a._v("checkpoint")]),a._v(" 发生的时间周期和 "),e("code",[a._v("edits log")]),a._v("日志文件的大小阈值，说明如下。")]),a._v(" "),e("p",[a._v("        （1）"),e("strong",[a._v("fs.checkpoint.period")]),a._v("：表示触发 "),e("code",[a._v("checkpoint")]),a._v("发生的时间周期，这里配置的时间周期为 1 h。")]),a._v(" "),e("p",[a._v("        （2）"),e("strong",[a._v("fs.checkpoint.size")]),a._v("：表示 "),e("code",[a._v("edits log")]),a._v(" 日志文件大小达到了多大的阈值时会发生 "),e("code",[a._v("checkpoint")]),a._v("操作，这里配置的 "),e("code",[a._v("edits log")]),a._v("大小阈值为 64 MB。")]),a._v(" "),e("p",[a._v("        上述代码中配置的 "),e("code",[a._v("checkpoint")]),a._v("操作发生的情况如下：")]),a._v(" "),e("p",[a._v("        （1）如果 "),e("code",[a._v("edits log")]),a._v(" 日志文件经过 1 h 未能达到 64 MB，但是满足了 "),e("code",[a._v("checkpoint")]),a._v("发生的周期为 1 h 的条件，也会发生 "),e("code",[a._v("checkpoint")]),a._v(" 操作。")]),a._v(" "),e("p",[a._v("        （2）如果 "),e("code",[a._v("edits log")]),a._v("日志文件大小在 1 h 之内达到了 64MB，满足了 "),e("code",[a._v("checkpoint")]),a._v(" 发生的 "),e("code",[a._v("edits log")]),a._v("日志文件大小阈值的条件，则会发生 "),e("code",[a._v("checkpoint")]),a._v("操作。")]),a._v(" "),e("blockquote",[e("p",[e("strong",[a._v("注意")]),a._v("：如果 NameNode 发生故障或 NameNode 上的元数据信息丢失或损坏导致 NameNode 无法启动，此时就需要人工干预，将 NameNode 中的元数据状态恢复到 SecondaryNameNode 中的元数据状态。此时，如果 SecondaryNameNode 上的元数据信息与 NameNode 宕机时的元数据信息不同步，则或多或少地会导致 Hadoop 集群中丢失一部分数据。出于此原因，"),e("font",{attrs:{color:"tomato"}},[e("strong",[a._v("应尽量避免将 NameNode 和 SecondaryNameNode 部署在同一台服务器上")])]),a._v("。")],1)]),a._v(" "),e("h3",{attrs:{id:"_3、datanode"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3、datanode"}},[a._v("#")]),a._v(" 3、DataNode")]),a._v(" "),e("p",[e("font",{attrs:{color:"SlateBlue"}},[a._v("DataNode 是真正存储数据的节点")]),a._v("，这些数据以"),e("strong",[a._v("数据块")]),a._v("的形式存储在 DataNode 上。一个数据块包含两个文件：一个是"),e("strong",[a._v("存储数据本身的文件")]),a._v("，另一个是"),e("strong",[a._v("存储元数据的文件")]),a._v("（这些元数据主要包括数据块的长度、数据块的检验和、时间戳）。")],1),a._v(" "),e("p",[a._v("        DataNode 运行时的工作机制如图所示：")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://img-blog.csdnimg.cn/20210301233255910.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDMxODgzMA==,size_16,color_FFFFFF,t_70",alt:""}}),a._v("\n        如图所示，DataNode 运行时的工作机制如下：")]),a._v(" "),e("p",[a._v("        （1）DataNode启动之后，向 NameNode "),e("strong",[a._v("注册")]),a._v("。")]),a._v(" "),e("p",[a._v("        （2）NameNode "),e("strong",[a._v("返回")]),a._v("注册成功的消息给 DataNode。")]),a._v(" "),e("p",[a._v("        （3）DataNode 收到 NameNode 返回的注册成功的信息之后，会"),e("strong",[a._v("周期性")]),a._v("地向 NameNode "),e("strong",[a._v("上报")]),a._v("当前 DataNode 的所有块信息，默认发送所有数据块的时间周期是 "),e("strong",[a._v("1h")]),a._v("。")]),a._v(" "),e("p",[a._v("        （4）DataNode "),e("strong",[a._v("周期性")]),a._v("地向NameNode 发送心跳信息；NameNode "),e("strong",[a._v("收到")]),a._v(" DataNode 发来的心跳信息后，会将DataNode 需要执行的命令放入到 心跳信息的 返回数据中，"),e("strong",[a._v("返回")]),a._v("给 DataNode。DataNode 向 NameNode 发送心跳信息的默认时间周期是 "),e("strong",[a._v("3s")]),a._v("。")]),a._v(" "),e("p",[a._v("        （5）NameNode "),e("strong",[a._v("超过一定的时间")]),a._v("没有收到 DataNode 发来的心跳信息，则 NameNode 会认为对应的 DataNode "),e("strong",[a._v("不可用")]),a._v("。默认的超时时间是 10 min。")]),a._v(" "),e("p",[a._v("        （6）"),e("strong",[a._v("在存储上相互关联的 DataNode 会同步数据块，以达到数据副本数的要求")]),a._v("。")]),a._v(" "),e("p",[a._v("        当 DataNode 发生故障导致 DataNode 无法与 NameNode 通信时，NameNode 不会立即认为 DataNode 已经 “死亡”。要经过一段"),e("strong",[a._v("短暂的超时时长")]),a._v("后才会认为 DataNode 已经 “死亡”。HDFS 中默认的超时时长为 10 min + 30 s，可以用如下公式来表示这个超时时长：")]),a._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token function"}},[a._v("timeout")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v(" * dfs.namenode.heartbeat.recheck-interval +10 * dfs.heartbeat.interval\n")])])]),e("p",[a._v("        其中，各参数的含义如下：")]),a._v(" "),e("p",[a._v("        （1）"),e("code",[a._v("timeout")]),a._v("：超时时长。")]),a._v(" "),e("p",[a._v("        （2）"),e("code",[a._v("dfs.namenode.heartbeat.recheck-interval")]),a._v("：检查过期 DataNode 的时间间隔，与 "),e("code",[a._v("dfs.heartbeat.interval")]),a._v(" 结合使用，默认的单位是 ms，默认时间是 5 min。")]),a._v(" "),e("p",[a._v("        （3）"),e("code",[a._v("dfs.heartbeat.interval")]),a._v("：检测数据节点的时间间隔，默认的单位为 s，默认的时间是 3 s。")]),a._v(" "),e("p",[a._v("        所以，可以得出 DataNode 的默认超时时长为 630s，如下所示：")]),a._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token function"}},[a._v("timeout")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v(" * "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("5")]),a._v(" * "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("60")]),a._v(" + "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("10")]),a._v(" * "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("3")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" 630s\n")])])]),e("p",[a._v("        DataNode 的超时时长也可以在 "),e("code",[a._v("hdfs-site.xml")]),a._v("文件中进行配置，代码如下所示：")]),a._v(" "),e("div",{staticClass:"language-xml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-xml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("dfs.namenode.heartbeat.recheck-interval"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("3000"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("dfs.heartbeat.interval"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("2"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n")])])]),e("p",[a._v("        根据上面的公式可以得出，在配置文件中配置的超时时长为：")]),a._v(" "),e("div",{staticClass:"language-bash extra-class"},[e("pre",{pre:!0,attrs:{class:"language-bash"}},[e("code",[e("span",{pre:!0,attrs:{class:"token function"}},[a._v("timeout")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v(" * "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("3000")]),a._v(" / "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("1000")]),a._v(" + "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("10")]),a._v(" * "),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" 26s\n")])])]),e("p",[a._v("        当 DataNode 被 NameNode 判定为 “"),e("strong",[a._v("死亡")]),a._v("”时，HDFS 就会马上自动进行"),e("strong",[a._v("数据块的容错复制")]),a._v("。此时，当被 NameNode 判定为 “死亡” 的 DataNode 重新加入集群中时，如果其存储的数据块并没有损坏，就会造成 "),e("strong",[a._v("HDFS 上某些数据块的备份数超过系统配置的备份数目")]),a._v("。")]),a._v(" "),e("p",[a._v("        HDFS上"),e("strong",[a._v("删除多余的数据块")]),a._v("需要的时间长短和数据块报告的时间间隔有关。该参数可以在 "),e("code",[a._v("hdfs-site.xml")]),a._v("文件中进行配置，代码如下所示：")]),a._v(" "),e("div",{staticClass:"language-xml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-xml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("dfs.blockreport.intervalMsec"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("21600000"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n     "),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("Determines block reporting interval in milliseconds."),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("description")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token tag"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("</")]),a._v("property")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),a._v("\n")])])]),e("p",[a._v("        数据块报告的时间间隔默认为 "),e("code",[a._v("21600000")]),a._v("ms，即 6h，可以通过调整此参数的大小来调整数据块报告的时间间隔。")]),a._v(" "),e("hr"),a._v(" "),e("h2",{attrs:{id:"小结"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#小结"}},[a._v("#")]),a._v(" 小结")]),a._v(" "),e("p",[a._v("        本篇文章算是对 "),e("strong",[a._v("HDFS的架构")]),a._v("解释的比较透彻，相信不论是刚入门的小白，还是已经有了一定基础的大数据学者，看完都会有一定的收获，希望大家平时学习也能够多学会总结，用输出倒逼自己输入！\n        ")]),a._v(" "),e("h2",{attrs:{id:"ref"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ref"}},[a._v("#")]),a._v(" Ref")]),a._v(" "),e("blockquote",[e("p",[a._v("1、《海量数据处理与大数据技术实战》\n2、《大数据平台架构与原型实现》\n3、https://baike.baidu.com/item/hdfs/4836121?fr=aladdin\n4、http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_user_guide.html")])])])}),[],!1,null,null,null);t.default=o.exports}}]);