(window.webpackJsonp=window.webpackJsonp||[]).push([[35],{428:function(s,t,a){"use strict";a.r(t);var e=a(30),r=Object(e.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"hive调优全方位指南"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive调优全方位指南"}},[s._v("#")]),s._v(" Hive调优全方位指南")]),s._v(" "),a("h1",{attrs:{id:"_1、表层面"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、表层面"}},[s._v("#")]),s._v(" 1、表层面")]),s._v(" "),a("h2",{attrs:{id:"_1-1-利用分区表优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-利用分区表优化"}},[s._v("#")]),s._v(" 1.1 利用分区表优化")]),s._v(" "),a("p",[s._v("分区表 是在某一个或者几个维度上对数据进行分类存储，一个分区对应一个目录。如果筛选条件里有分区字段，那么 Hive 只需要遍历对应分区目录下的文件即可，不需要遍历全局数据，使得处理的数据量大大减少，从而提高查询效率。")]),s._v(" "),a("p",[s._v("也就是说：当一个 Hive 表的查询大多数情况下，会根据某一个字段进行筛选时，那么非常适合创建为分区表，该字段即为分区字段。")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" page_view\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("viewTime "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n userid "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BIGINT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n page_url STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n referrer_url STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n ip STRING "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("COMMENT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'IP Address of the User'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nPARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("date")]),s._v(" STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" country STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ROW")]),s._v(" FORMAT DELIMITED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FIELDS")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TERMINATED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'1'")]),s._v("\nSTORED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("AS")]),s._v(" TEXTFILE"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[a("strong",[s._v("1、当你意识到一个字段经常用来做where，建分区表，使用这个字段当做分区字段\n2、在查询的时候，使用分区字段来过滤，就可以避免全表扫描。只需要扫描这张表的一个分区的数据即可")])]),s._v(" "),a("h2",{attrs:{id:"_1-2-利用分桶表优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-利用分桶表优化"}},[s._v("#")]),s._v(" 1.2 利用分桶表优化")]),s._v(" "),a("p",[a("strong",[s._v("跟分区的概念很相似，都是把数据分成多个不同的类别")])]),s._v(" "),a("p",[s._v("1、分区：按照字段值来进行，一个分区，就只是包含这个值的所有记录")]),s._v(" "),a("p",[s._v("不是当前分区的数据一定不在当前分区")]),s._v(" "),a("p",[s._v("当前分区也只会包含当前这个分区值的数据")]),s._v(" "),a("p",[s._v("2、分桶：默认规则，Hash的方式")]),s._v(" "),a("p",[s._v("一个桶中会有多个不同的值")]),s._v(" "),a("p",[s._v("如果一个分桶中，包含了某个值，这个值的所有记录，必然都在这个分桶里面")]),s._v(" "),a("p",[s._v("Hive Bucket，分桶，是指将数据以指定列的值为key进行hash，hash到指定数目的桶里面，这样做的目的和分区表类似，是的筛选时不用全局遍历所有的数据，只需要遍历所在的桶就好了，这样也只可以支持高效采样。")]),s._v(" "),a("p",[s._v("其实最主要的作用就是 "),a("strong",[s._v("采样、join")])]),s._v(" "),a("p",[a("strong",[s._v("如下例就是以 userid 这一列为 bucket 的依据，共设置 32 个 buckets")])]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" page_view"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("viewTime "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" userid "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BIGINT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n         page_url STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" referrer_url STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n         ip STRING "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("COMMENT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'IP Address of the User'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("COMMENT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'This is the page view table'")]),s._v("\nPARTITIONED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("dt STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" country STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CLUSTERED")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("userid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" SORTED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("viewTime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INTO")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("32")]),s._v(" BUCKETS\n")])])]),a("p",[a("strong",[s._v("CLUSTERED BY(userid)：")]),s._v(" 按照userid进行分桶")]),s._v(" "),a("p",[a("strong",[s._v("SORTED BY(viewTime)：")]),s._v(" 按照viewTime进行桶内排序")]),s._v(" "),a("p",[a("strong",[s._v("INTO 32 BUCKETS：")]),s._v(" 分成多少个桶")]),s._v(" "),a("p",[a("strong",[s._v("两个表以相同方式（相同字段）划分桶，两个表的桶个数一定是倍数关系，这样在join的时候速度会大大增加")])]),s._v(" "),a("p",[s._v("采样用的不多，也就不过多阐述了")]),s._v(" "),a("h2",{attrs:{id:"_1-3-选择合适的文件存储格式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-选择合适的文件存储格式"}},[s._v("#")]),s._v(" 1.3 选择合适的文件存储格式")]),s._v(" "),a("p",[s._v("在 HiveSQL 的 create table 语句中，可以使用 stored as … 指定表的存储格式。Apache Hive支持 Apache Hadoop 中使用的几种熟悉的文件格式，比如 TextFile、SequenceFile、RCFile、Avro、ORC、ParquetFile等。存储格式一般需要根据业务进行选择，在我们的实操中，绝大多数表都采用TextFile与Parquet两种存储格式之一。TextFile是最简单的存储格式，它是纯文本记录，也是Hive的默认格式。虽然它的磁盘开销比较大，查询效率也低，但它更多地是作为跳板来使用。RCFile、ORC、Parquet等格式的表都不能由文件直接导入数据，必须由TextFile来做中转。Parquet和ORC都是Apache旗下的开源列式存储格式。列式存储比起传统的行式存储更适合批量OLAP查询，并且也支持更好的压缩和编码。创建表时，特别是宽表，尽量使用 ORC、ParquetFile 这些列式存储格式，因为列式存储的表，每一列的数据在物理上是存储在一起的，Hive查询时会只遍历需要列数据，大大减少处理的数据量。")]),s._v(" "),a("p",[a("strong",[s._v("TextFile")])]),s._v(" "),a("p",[s._v('1、存储方式：行存储。默认格式，如果建表时不指定默认为此格式。，\n2、每一行都是一条记录，每行都以换行符"\\n"结尾。数据不做压缩时，磁盘会开销比较大，数据解析开销也\n比较大。\n3、可结合Gzip、Bzip2等压缩方式一起使用（系统会自动检查，查询时会自动解压）,推荐选用可切分的压\n缩算法。')]),s._v(" "),a("p",[a("strong",[s._v("Sequence File")])]),s._v(" "),a("p",[s._v("1、一种Hadoop API提供的二进制文件，使用方便、可分割、个压缩的特点。\n2、支持三种压缩选择：NONE、RECORD、BLOCK。RECORD压缩率低，一般建议使用BLOCK压缩")]),s._v(" "),a("p",[a("strong",[s._v("RC File")])]),s._v(" "),a("p",[s._v("1、存储方式：数据按行分块，每块按照列存储 。\nA、首先，将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。\nB、其次，块数据列式存储，有利于数据压缩和快速的列存取。\n2、相对来说，RCFile对于提升任务执行性能提升不大，但是能节省一些存储空间。可以使用升级版的ORC格\n式。")]),s._v(" "),a("p",[a("strong",[s._v("ORC File")])]),s._v(" "),a("p",[s._v("1、存储方式：数据按行分块，每块按照列存储\n2、Hive提供的新格式，属于RCFile的升级版，性能有大幅度提升，而且数据可以压缩存储，压缩快，快速列存取。\n3、ORC File会基于列创建索引，当查询的时候会很快。")]),s._v(" "),a("p",[a("strong",[s._v("Parquet File")])]),s._v(" "),a("p",[s._v("1、存储方式：列式存储。\n2、Parquet对于大型查询的类型是高效的。对于扫描特定表格中的特定列查询，Parquet特别有用。Parquet一般使用Snappy、Gzip压缩。默认Snappy。\n3、Parquet支持Impala 查询引擎。\n4、表的文件存储格式尽量采用Parquet或ORC，不仅降低存储量，还优化了查询，压缩，表关联等性能")]),s._v(" "),a("h2",{attrs:{id:"_1-4-选择合适的压缩格式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-选择合适的压缩格式"}},[s._v("#")]),s._v(" 1.4 选择合适的压缩格式")]),s._v(" "),a("p",[s._v("Hive 语句最终是转化为 MapReduce 程序来执行的，而 MapReduce 的性能瓶颈在与 网络IO 和 磁盘IO，要解决性能瓶颈，最主要的是 减少数据量，对数据进行压缩是个好方式。压缩虽然是减少了数据量，但是压缩过程要消耗 CPU，但是在 Hadoop 中，往往性能瓶颈不在于 CPU，CPU 压力并不大，所以压缩充分利用了比较空闲的 CPU。")]),s._v(" "),a("p",[a("strong",[s._v("常用压缩方法对比")])]),s._v(" "),a("table",[a("thead",[a("tr",[a("th",[s._v("压缩格式")]),s._v(" "),a("th",[s._v("是否可拆分")]),s._v(" "),a("th",[s._v("是否自带")]),s._v(" "),a("th",[s._v("压缩率")]),s._v(" "),a("th",[s._v("速度")]),s._v(" "),a("th",[s._v("是否hadoop自带")])])]),s._v(" "),a("tbody",[a("tr",[a("td",[s._v("gzip")]),s._v(" "),a("td",[s._v("否")]),s._v(" "),a("td",[s._v("是")]),s._v(" "),a("td",[s._v("很高")]),s._v(" "),a("td",[s._v("比较快")]),s._v(" "),a("td",[s._v("是")])]),s._v(" "),a("tr",[a("td",[s._v("lzo")]),s._v(" "),a("td",[s._v("是")]),s._v(" "),a("td",[s._v("是")]),s._v(" "),a("td",[s._v("比较高")]),s._v(" "),a("td",[s._v("很快")]),s._v(" "),a("td",[s._v("否")])]),s._v(" "),a("tr",[a("td",[s._v("snappy")]),s._v(" "),a("td",[s._v("否")]),s._v(" "),a("td",[s._v("是")]),s._v(" "),a("td",[s._v("比较高")]),s._v(" "),a("td",[s._v("很快")]),s._v(" "),a("td",[s._v("否")])]),s._v(" "),a("tr",[a("td",[s._v("bzip2")]),s._v(" "),a("td",[s._v("是")]),s._v(" "),a("td",[s._v("否")]),s._v(" "),a("td",[s._v("最高")]),s._v(" "),a("td",[s._v("慢")]),s._v(" "),a("td",[s._v("是")])])])]),s._v(" "),a("p",[a("strong",[s._v("压缩率对比")])]),s._v(" "),a("p",[a("img",{attrs:{src:"https://mmbiz.qpic.cn/mmbiz_png/1OYP1AZw0W1LaS00FoLjZkYUhPfxQ8u9DTUByCtseC1OnkvoD3mYwPZwVFNherQIOksq9IQI0aQTOE7cEZEC6g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1",alt:"图片"}})]),s._v(" "),a("blockquote",[a("p",[s._v("如何选择压缩方式呢？")])]),s._v(" "),a("p",[s._v("1、压缩比例")]),s._v(" "),a("p",[s._v("2、解压缩速度")]),s._v(" "),a("p",[s._v("3、是否支持split")]),s._v(" "),a("p",[a("strong",[s._v("支持切割的文件可以并行的有多个mapper程序处理大数据文件，一般我们选择的都是支持切分的！")])]),s._v(" "),a("blockquote",[a("p",[s._v("压缩带来的缺点和优点")])]),s._v(" "),a("p",[s._v("1、计算密集型，不压缩，否则会进一步增加cpu的负担，真实的场景中hive对cpu的压力很小")]),s._v(" "),a("p",[s._v("2、网络密集型，推荐压缩，减小网络数据传输")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("# Job 输出文件按照 Block\n## 默认值是false\nset mapreduce.output.fileoutputformat.compress=true;   \n## 默认值是Record\nset mapreduce.output.fileoutputformat.compress.type=BLOCK;\nset mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.lzo.LzoCodec;\n\n# Map 输出结结果进行压缩\nset mapred.map.output.compress=true;\nset mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.lzo.LzoCodec;\n\n# 对 Hive 输出结果和中间都进行压缩\nset hive.exec.compress.output=true  ## 默认值是false，不压缩\nset hive.exec.compress.intermediate=true   ## 默认值是false，为true时MR设置的压缩才启用\n")])])]),a("h1",{attrs:{id:"_2、hql层面优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、hql层面优化"}},[s._v("#")]),s._v(" 2、HQL层面优化")]),s._v(" "),a("h2",{attrs:{id:"_2-1-执行计划"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-执行计划"}},[s._v("#")]),s._v(" 2.1 执行计划")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("explain")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" movies"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[a("img",{attrs:{src:"https://mmbiz.qpic.cn/mmbiz_png/1OYP1AZw0W1LaS00FoLjZkYUhPfxQ8u9HKib4Kibh5Sf3KU0SwFBHibaqEYJibw4ugfEmoCnbpeEVJXXibAWZVKGYcA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1",alt:"图片"}})]),s._v(" "),a("h2",{attrs:{id:"_2-1-列、行、分区裁剪"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-列、行、分区裁剪"}},[s._v("#")]),s._v(" 2.1 列、行、分区裁剪")]),s._v(" "),a("p",[s._v("列裁剪就是在查询时只读取需要的列")]),s._v(" "),a("p",[s._v("行裁剪就是在查询时只读取需要的行，也就是提前过滤")]),s._v(" "),a("p",[s._v("分区剪裁就是在查询的时候只读取需要的分区。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("set hive.optimize.cp = true; 列裁剪，取数只取查询中需要用到的列，默认是true\nset hive.optimize.pruner=true;   ## 分区剪裁\n")])])]),a("h2",{attrs:{id:"_2-2-谓词下推"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-谓词下推"}},[s._v("#")]),s._v(" 2.2 谓词下推")]),s._v(" "),a("p",[s._v("将 SQL 语句中的 where 谓词逻辑都尽可能提前执行，减少下游处理的数据量。对应逻辑优化器是PredicatePushDown。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("set hive.optimize.ppd=true;   ## 默认是true\n")])])]),a("p",[a("strong",[s._v("eg:")])]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("age "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("转换为下面的这样的")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" age "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" c "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("h2",{attrs:{id:"_2-3-合并小文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-合并小文件"}},[s._v("#")]),s._v(" 2.3 合并小文件")]),s._v(" "),a("p",[s._v("如果一个mapreduce job碰到一对小文件作为输入，一个小文件启动一个Task，这样的话会出现maptask爆炸的问题。")]),s._v(" "),a("p",[a("strong",[s._v("Map端输入合并")])]),s._v(" "),a("p",[s._v("在执行 MapReduce 程序的时候，一般情况是一个文件的一个数据分块需要一个 mapTask 来处理。但是如果数据源是大量的小文件，这样就会启动大量的 mapTask 任务，这样会浪费大量资源。可以将输入的小文件进行合并，从而减少 mapTask 任务数量。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("## Map端输入、合并文件之后按照block的大小分割（默认）\nset hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n## Map端输入，不合并\nset hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n")])])]),a("p",[a("strong",[s._v("Map/Reduce输出合并")])]),s._v(" "),a("p",[s._v("大量的小文件会给 HDFS 带来压力，影响处理效率。可以通过合并 Map 和 Reduce 的结果文件来消除影响")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("## 是否合并Map输出文件, 默认值为true\nset hive.merge.mapfiles=true;\n## 是否合并Reduce端输出文件,默认值为false\nset hive.merge.mapredfiles=true;\n## 合并文件的大小,默认值为256000000 256M\nset hive.merge.size.per.task=256000000;\n## 每个Map 最大分割大小\nset mapred.max.split.size=256000000; \n## 一个节点上split的最少值\nset mapred.min.split.size.per.node=1;  // 服务器节点\n## 一个机架上split的最少值\nset mapred.min.split.size.per.rack=1;   // 服务器机架\n")])])]),a("p",[a("code",[s._v("hive.merge.size.per.task")]),s._v(" 和 "),a("code",[s._v("mapred.min.split.size.per.node")]),s._v(" 联合起来：")]),s._v(" "),a("p",[a("strong",[s._v("1、默认情况先把这个节点上的所有数据进行合并，如果合并的那个文件的大小超过了256M就开启另外一个文件继续合并")]),s._v(" "),a("strong",[s._v("2、如果当前这个节点上的数据不足256M，那么就都合并成一个逻辑切片。")])]),s._v(" "),a("h2",{attrs:{id:"_2-4-合理设置maptask并行度"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-合理设置maptask并行度"}},[s._v("#")]),s._v(" 2.4 合理设置MapTask并行度")]),s._v(" "),a("p",[a("strong",[s._v("Map数过大")]),s._v(" ：当输入文件特别大，MapTask 特别多，每个计算节点分配执行的 MapTask 都很多，这时候可以考虑减少 MapTask 的数量。增大每个 MapTask 处理的数据量。而且 MapTask 过多，最终生成的结果文件数也太多。")]),s._v(" "),a("p",[a("strong",[s._v("1、Map阶段输出文件太小，产生大量小文件")]),s._v(" "),a("strong",[s._v("2、初始化和创建Map的开销很大")])]),s._v(" "),a("p",[a("strong",[s._v("Map数太小")]),s._v(" ：当输入文件都很大，任务逻辑复杂，MapTask 执行非常慢的时候，可以考虑增加MapTask 数，来使得每个 MapTask 处理的数据量减少，从而提高任务的执行效率。")]),s._v(" "),a("p",[a("strong",[s._v("1、文件处理或查询并发度小，Job执行时间过长")]),s._v(" "),a("strong",[s._v("2、大量作业时，容易堵塞集群")])]),s._v(" "),a("p",[a("strong",[s._v("一个MapReduce Job 的 MapTask 数量是由输入分片InputSplit 决定的。而输入分片是由 FileInputFormat.getSplit() 决定的。一个输入分片对应一个MapTask，而输入分片是由三个参数决定的：")])]),s._v(" "),a("table",[a("thead",[a("tr",[a("th",[s._v("参数")]),s._v(" "),a("th",[s._v("默认值")]),s._v(" "),a("th",[s._v("意义")])])]),s._v(" "),a("tbody",[a("tr",[a("td",[s._v("dfs.blocksize")]),s._v(" "),a("td",[s._v("128M")]),s._v(" "),a("td",[s._v("HDFS默认数据块大小")])]),s._v(" "),a("tr",[a("td",[s._v("mapreduce.input.fileinputformat.split.minsize")]),s._v(" "),a("td",[s._v("1")]),s._v(" "),a("td",[s._v("最小分片大小(MR)")])]),s._v(" "),a("tr",[a("td",[s._v("mapreduce.input.fileinputformat.split.maxsize")]),s._v(" "),a("td",[s._v("256M")]),s._v(" "),a("td",[s._v("最大分片大小(MR)")])])])]),s._v(" "),a("p",[s._v("输入分片大小的计算是这么计算出来的：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("long splitSize = Math.max(minSize, Math.min(maxSize, blockSize))\n")])])]),a("p",[s._v("默认情况下，输入分片大小和 HDFS 集群默认数据块大小一致，也就是默认一个数据块，启用一个MapTask 进行处理，这样做的好处是避免了服务器节点之间的数据传输，提高 job 处理效率")]),s._v(" "),a("p",[a("strong",[s._v("两种经典的控制MapTask的个数方案：减少MapTask数 或者 增加MapTask数")])]),s._v(" "),a("p",[s._v("1、减少 MapTask 数是通过合并小文件来实现，这一点主要是针对数据源\n2、增加 MapTask 数可以通过控制上一个 job 的 reduceTask 个数\n"),a("strong",[s._v("重点注意：不推荐把这个值进行随意设置！")]),s._v(" "),a("strong",[s._v("推荐的方式：使用默认的切块大小即可。如果非要调整，最好是切块的N倍数")])]),s._v(" "),a("blockquote",[a("p",[s._v("最好的方式就是 NodeManager节点个数：N ===》 Task = ( N * 0.95) * MapTask")])]),s._v(" "),a("p",[a("strong",[s._v("合理控制 MapTask 数量")])]),s._v(" "),a("p",[s._v("1、减少 MapTask 数可以通过合并小文件来实现\n2、增加 MapTask 数可以通过控制上一个 ReduceTask 默认的 MapTask 个数")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("输入文件总大小：total_size  \nHDFS 设置的数据块大小：dfs_block_size   \ndefault_mapper_num = total_size / dfs_block_size\n")])])]),a("p",[s._v("MapReduce 中提供了如下参数来控制 map 任务个数，从字面上看，貌似是可以直接设置 MapTask 个数的样子，但是很遗憾不行，这个参数设置只有在大于 default_mapper_num 的时候，才会生效")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("set mapred.map.tasks=10; ## 默认值是2\n")])])]),a("p",[s._v("那如果我们需要减少 MapTask 数量，但是文件大小是固定的，那该怎么办呢?可以通过 mapred.min.split.size 设置每个任务处理的文件的大小，这个大小只有在大于dfs_block_size 的时候才会生效")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("split_size = max(mapred.min.split.size, dfs_block_size)\nsplit_num = total_size / split_size\ncompute_map_num = Math.min(split_num, Math.max(default_mapper_num,\nmapred.map.tasks))\n")])])]),a("p",[s._v("这样就可以减少 MapTask 数量了")]),s._v(" "),a("p",[s._v("让我们来总结一下控制mapper个数的方法：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("1、如果想增加 MapTask 个数，可以设置 mapred.map.tasks 为一个较大的值\n2、如果想减少 MapTask 个数，可以设置 maperd.min.split.size 为一个较大的值\n3、如果输入是大量小文件，想减少 mapper 个数，可以通过设置 hive.input.format 合并小文\n")])])]),a("p",[s._v("如果想要调整 mapper 个数，在调整之前，需要确定处理的文件大概大小以及文件的存在形式（是大量小文件，还是单个大文件），然后再设置合适的参数。不能盲目进行暴力设置，不然适得其反。")]),s._v(" "),a("p",[s._v("MapTask 数量与输入文件的 split 数息息相关，在 Hadoop 源码"),a("code",[s._v("org.apache.hadoop.mapreduce.lib.input.FileInputFormat")]),s._v(" 类中可以看到 split 划分的具体逻辑。可以直接通过参数 "),a("code",[s._v("mapred.map.tasks")]),s._v("（默认值2）来设定 MapTask 数的期望值，但它不一定会生效。")]),s._v(" "),a("h2",{attrs:{id:"_2-5-合理设置reducetask并行度"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-合理设置reducetask并行度"}},[s._v("#")]),s._v(" 2.5 合理设置ReduceTask并行度")]),s._v(" "),a("p",[s._v("如果 ReduceTask 数量过多，一个 ReduceTask 会产生一个结果文件，这样就会生成很多小文件，那么如果这些结果文件会作为下一个 Job 的输入，则会出现小文件需要进行合并的问题，而且启动和初始化ReduceTask 需要耗费资源。")]),s._v(" "),a("p",[s._v("如果 ReduceTask 数量过少，这样一个 ReduceTask 就需要处理大量的数据，并且还有可能会出现数据倾斜的问题，使得整个查询耗时长。默认情况下，Hive 分配的 reducer 个数由下列参数决定：")]),s._v(" "),a("p",[s._v("Hadoop MapReduce 程序中，ReducerTask 个数的设定极大影响执行效率，ReducerTask 数量与输出文件的数量相关。如果 ReducerTask 数太多，会产生大量小文件，对HDFS造成压力。如果ReducerTask 数太少，每个ReducerTask 要处理很多数据，容易拖慢运行时间或者造成 OOM。这使得Hive 怎样决定 ReducerTask 个数成为一个关键问题。遗憾的是 Hive 的估计机制很弱，不指定ReducerTask 个数的情况下，Hive 会猜测确定一个ReducerTask 个数，基于以下两个设定：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("参数1：hive.exec.reducers.bytes.per.reducer (默认256M)\n参数2：hive.exec.reducers.max (默认为1009)\n参数3：mapreduce.job.reduces (默认值为-1，表示没有设置，那么就按照以上两个参数进行设置)\n")])])]),a("p",[s._v("ReduceTask 的计算公式为:")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("N = Math.min(参数2，总输入数据大小 / 参数1)\n")])])]),a("p",[s._v("可以通过改变上述两个参数的值来控制 ReduceTask 的数量。也可以通过")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("set mapred.map.tasks=10;\nset mapreduce.job.reduces=10;\n")])])]),a("p",[s._v("通常情况下，有必要手动指定 ReduceTask 个数。考虑到 Mapper 阶段的输出数据量通常会比输入有大幅减少，因此即使不设定 ReduceTask 个数，重设 参数2 还是必要的。依据经验，可以将 参数2 设定为 M * （0.95 * N） (N为集群中 NodeManager 个数)。一般来说，NodeManage 和 DataNode 的个数是一样的")]),s._v(" "),a("h2",{attrs:{id:"_2-6-join优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-6-join优化"}},[s._v("#")]),s._v(" 2.6 Join优化")]),s._v(" "),a("p",[a("strong",[s._v("1. Join的整体优化原则：")])]),s._v(" "),a("p",[s._v("1、优先过滤后再进行Join操作，最大限度的减少参与join的数据量\n2、小表join大表，最好启动mapjoin，hive自动启用mapjoin, 小表不能超过25M，可以更改\n3、Join on的条件相同的话，最好放入同一个job，并且join表的排列顺序从小到大：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("select a.*,b.*, c.* from a join b on a.id = b.id join c on a.id = c.i\n")])])]),a("p",[s._v("4、如果多张表做join, 如果多个链接条件都相同，会转换成一个JOb")]),s._v(" "),a("p",[a("strong",[s._v("2. 优先过滤数据：")])]),s._v(" "),a("p",[s._v("尽量减少每个阶段的数据量，对于分区表能用上分区字段的尽量使用，同时只选择后面需要使用到的列，最大限度的减少参与 Join 的数据量")]),s._v(" "),a("p",[a("strong",[s._v("3. 小表join大表的原则：")])]),s._v(" "),a("p",[s._v("小表 join 大表的时应遵守小表 join 大表原则，原因是 join 操作的 reduce 阶段，位于 join 左边的表内容会被加载进内存，将条目少的表放在左边，可以有效减少发生内存溢出的几率。join 中执行顺序是从左到右生成 Job，应该保证连续查询中的表的大小从左到右是依次增加的。")]),s._v(" "),a("p",[a("strong",[s._v("4. 使用相同的连接键：")])]),s._v(" "),a("p",[s._v("在 hive 中，当对 3 个或更多张表进行 join 时，如果 on 条件使用相同字段，那么它们会合并为一个MapReduce Job，利用这种特性，可以将相同的 join on 放入一个 job 来节省执行时间。")]),s._v(" "),a("p",[a("strong",[s._v("5. 尽量原子操作：")])]),s._v(" "),a("p",[s._v("尽量避免一个SQL包含复杂的逻辑，可以使用中间表来完成复杂的逻辑。")]),s._v(" "),a("p",[a("strong",[s._v("6. 大表join大表：")])]),s._v(" "),a("p",[s._v("1、空key过滤：有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。")]),s._v(" "),a("p",[s._v("2、空key转换：有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上")]),s._v(" "),a("p",[a("strong",[s._v("7. 启用MapJoin：")])]),s._v(" "),a("p",[s._v("这个优化措施，只要能用的时候一定要用，根据数据量大小来调整小表的大小，一般公司里面可以设置到512 到1G")]),s._v(" "),a("p",[s._v("MapJoin 是将 join 双方比较小的表直接分发到各个 map 进程的内存中，在 map 进程中进行 join 操作，这样就不用进行 reduce 步骤，从而提高了速度。只有 join 操作才能启用 MapJoin。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("## 是否根据输入小表的大小，自动将reduce端的common join 转化为map join，将小表刷入内存中。\n## 对应逻辑优化器是MapJoinProcessor\nset hive.auto.convert.join = true;\n## 刷入内存表的大小(字节)\nset hive.mapjoin.smalltable.filesize = 25000000;\n## hive会基于表的size自动的将普通join转换成mapjoin\nset hive.auto.convert.join.noconditionaltask=true;\n## 多大的表可以自动触发放到内层LocalTask中，默认大小10M\nset hive.auto.convert.join.noconditionaltask.size=10000000;\n")])])]),a("p",[s._v("Hive 可以进行多表 Join。Join 操作尤其是 Join 大表的时候代价是非常大的。MapJoin 特别适合大小表join的情况。在Hive join场景中，一般总有一张相对小的表和一张相对大的表，小表叫 build table，大表叫 probe table。Hive 在解析带 join 的 SQL 语句时，会默认将最后一个表作为 probe table，将前面的表作为 build table 并试图将它们读进内存。如果表顺序写反，probe table 在前面，引发 OOM 的风险就高了。在维度建模数据仓库中，事实表就是 probe table，维度表就是 build table。这种 Join 方式在 map 端直接完成 join 过程，消灭了 reduce，效率很高。而且 MapJoin 还支持非等值连接。当 Hive 执行 Join 时，需要选择哪个表被流式传输（stream），哪个表被缓存（cache）。Hive 将JOIN 语句中的最后一个表用于流式传输，因此我们需要确保这个流表在两者之间是最大的。如果要在\n不同的 key 上 join 更多的表，那么对于每个 join 集，只需在 ON 条件右侧指定较大的表")]),s._v(" "),a("p",[s._v("也可以手动开启mapjoin：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("--SQL方式，在SQL语句中添加MapJoin标记（mapjoin hint）\n--将小表放到内存中，省去shffle操作\n// 在没有开启mapjoin的情况下，执行的是reduceJoin\nSELECT  /*+ MAPJOIN(smallTable) */ smallTable.key, bigTable.value FROM\nsmallTable  JOIN bigTable  ON smallTable.key = bigTable.key;\n")])])]),a("p",[a("strong",[s._v("在高版本中，已经进行了优化，会自动进行优化")])]),s._v(" "),a("p",[a("strong",[s._v("8. Sort-Merge-Bucket(SMB) Map Join：")])]),s._v(" "),a("p",[s._v("它是另一种Hive Join优化技术，使用这个技术的前提是所有的表都必须是分桶表（bucket）和分桶排序的（sort）。分桶表的优化！")]),s._v(" "),a("p",[s._v("具体实现：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("1、针对参与join的这两张做相同的hash散列，每个桶里面的数据还要排序\n2、这两张表的分桶个数要成倍数。\n3、开启 SMB join 的开关！\n")])])]),a("p",[s._v("一些常见的参数设置：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("## 当用户执行bucket map join的时候，发现不能执行时，禁止查询\nset hive.enforce.sortmergebucketmapjoin=false; \n## 如果join的表通过sort merge join的条件，join是否会自动转换为sort merge join\nset hive.auto.convert.sortmerge.join=true;\n## 当两个分桶表 join 时，如果 join on的是分桶字段，小表的分桶数是大表的倍数时，可以启用\nmapjoin 来提高效率。\n# bucket map join优化，默认值是 false\nset hive.optimize.bucketmapjoin=false; \n## bucket map join 优化，默认值是 false\nset hive.optimize.bucketmapjoin.sortedmerge=false;\n")])])]),a("p",[a("strong",[s._v("9. Join数据倾斜优化:")])]),s._v(" "),a("p",[a("strong",[s._v("在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：")])]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("# join的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置\nset hive.skewjoin.key=100000;  \n# 如果是join过程出现倾斜应该设置为true\nset hive.optimize.skewjoin=false;\n")])])]),a("p",[s._v("如果开启了，在 Join 过程中 Hive 会将计数超过阈值 hive.skewjoin.key（默认100000）的倾斜 key 对应的行临时写进文件中，然后再启动另一个 job 做 map join 生成结果。通过 "),a("code",[s._v("hive.skewjoin.mapjoin.map.tasks")]),s._v(" 参数还可以控制第二个 job 的 mapper 数量，默认10000。")]),s._v(" "),a("p",[s._v("例如"),a("code",[s._v("set hive.skewjoin.mapjoin.map.tasks=10000;")])]),s._v(" "),a("h2",{attrs:{id:"_2-7-cbo优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-7-cbo优化"}},[s._v("#")]),s._v(" 2.7 CBO优化")]),s._v(" "),a("p",[s._v("join的时候表的顺序的关系：前面的表都会被加载到内存中。后面的表进行磁盘扫描")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" c "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("Hive 自 0.14.0 开始，加入了一项 “Cost based Optimizer” 来对 HQL 执行计划进行优化，这个功能通过 “hive.cbo.enable” 来开启。在 Hive 1.1.0 之后，这个 feature 是默认开启的，它可以 "),a("strong",[s._v("自动优化 HQL中多个 Join 的顺序，并选择合适的 Join 算法。")])]),s._v(" "),a("p",[s._v("CBO，成本优化器，代价最小的执行计划就是最好的执行计划。传统的数据库，成本优化器做出最优化的执行计划是依据统计信息来计算的。Hive 的成本优化器也一样。Hive 在提供最终执行前，优化每个查询的执行逻辑和物理执行计划。这些优化工作是交给底层来完成的。根据查询成本执行进一步的优化，从而产生潜在的不同决策：如何排序连接，执行哪种类型的连接，并行度等等。要使用基于成本的优化（也称为CBO），请在查询开始设置以下参数：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("set hive.cbo.enable=true;\nset hive.compute.query.using.stats=true;\nset hive.stats.fetch.column.stats=true;\nset hive.stats.fetch.partition.stats=true;\n")])])]),a("h2",{attrs:{id:"_2-8-group-by优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-8-group-by优化"}},[s._v("#")]),s._v(" 2.8 Group By优化")]),s._v(" "),a("p",[s._v("默认情况下，Map 阶段同一个 Key 的数据会分发到一个 Reduce 上，当一个 Key 的数据过大时会产生数据倾斜。进行 group by 操作时可以从以下两个方面进行优化：")]),s._v(" "),a("p",[s._v("1、Map端部分预聚合：")]),s._v(" "),a("p",[s._v("事实上并不是所有的聚合操作都需要在 Reduce 部分进行，很多聚合操作都可以先在 Map 端进行部分聚合，然后在 Reduce 端的得出最终结果。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("## 开启Map端聚合参数设置\nset hive.map.aggr=true;\n# 设置map端预聚合的行数阈值，超过该值就会分拆job，默认值100000\nset hive.groupby.mapaggr.checkinterval=100000  \n")])])]),a("p",[s._v("2、有数据倾斜时进行负载均衡")]),s._v(" "),a("p",[s._v("当 HQL 语句使用 group by 时数据出现倾斜时，如果该变量设置为 true，那么 Hive 会自动进行负载均衡。策略就是把 MapReduce 任务拆分成两个：第一个先做预汇总，第二个再做最终汇总。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("# 自动优化，有数据倾斜的时候进行负载均衡（默认是false） 如果开启设置为true\nset hive.groupby.skewindata=false;\n")])])]),a("p",[s._v("当选项设定为 true 时，生成的查询计划有两个 MapReduce 任务。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("1、在第一个 MapReduce 任务中，map 的输出结果会随机分布到 reduce 中，每个 reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的`group by key`有可能分发到不同的 reduce 中，从而达到负载均衡的目的；\n2、第二个 MapReduce 任务再根据预处理的数据结果按照 group by key 分布到各个 reduce 中，最后完成最终的聚合操作。\n")])])]),a("p",[s._v("Map 端部分聚合：并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果，对应的优化器为 GroupByOptimizer。")]),s._v(" "),a("p",[a("strong",[s._v("那么如何用 group by 方式同时统计多个列？")])]),s._v(" "),a("p",[s._v("简单版：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" some_table t "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("优化版：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("null")]),s._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("null")]),s._v(" d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" some_table\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("union")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("all")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("null")]),s._v(" d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" some_table "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("c\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("union")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("all")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("null")]),s._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" some_table "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("d\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("h2",{attrs:{id:"_2-9-order-by优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-9-order-by优化"}},[s._v("#")]),s._v(" 2.9 Order By优化")]),s._v(" "),a("p",[s._v("order by 只能是在一个 reduce 进程中进行，所以如果对一个大数据集进行 order by ，会导致一个reduce 进程中处理的数据相当大，造成查询执行缓慢。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("1、在最终结果上进行order by，不要在中间的大数据集上进行排序。如果最终结果较少，可以在一个reduce上进行排序时，那么就在最后的结果集上进行order by。\n2、如果是取排序后的前N条数据，可以使用distribute by和sort by在各个reduce上进行排序后前N条，然后再对各个reduce的结果集合合并后在一个reduce中全局排序，再取前N条，因为参与全局排序的order by的数据量最多是reduce个数 * N，所以执行效率会有很大提升。\n")])])]),a("p",[a("strong",[s._v("在Hive中，关于数据排序，提供了四种语法，一定要区分这四种排序的使用方式和适用场景")])]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("、"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("order")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v("：全局排序，缺陷是只能使用一个reduce\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("、sort "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v("：单机排序，单个reduce结果有序\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v("、cluster "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v("：对同一字段分桶并排序，不能和sort "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v("连用\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("、distribute "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" sort "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v("：分桶，保证同一字段值只存在一个结果文件当中，结合sort "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v("保证每个reduceTask结果有序\n")])])]),a("p",[s._v("Hive HQL 中的 order by 与其他 SQL 方言中的功能一样，就是将结果按某字段全局排序，这会导致所有 map 端数据都进入一个 reducer 中，在数据量大时可能会长时间计算不完。\n如果使用 sort by，那么还是会视情况启动多个 reducer 进行排序，并且保证每个 reducer 内局部有序。为了控制map 端数据分配到 reducer 的 key，往往还要配合 distribute by 一同使用。如果不加distribute by 的话，map 端数据就会随机分配到 reducer.")]),s._v(" "),a("p",[s._v("1、方式一：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 直接使用order by来做。如果结果数据量很大，这个任务的执行效率会非常低")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("age "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("order")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" age "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("desc")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("limit")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("2、方式二：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" mapreduce"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("job"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduces"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student distribute "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("case")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("when")]),s._v(" age "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("then")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("when")]),s._v(" age "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("18")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("then")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("else")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("end")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" sort "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("age "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("desc")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[a("strong",[s._v("关于分界值的确定，使用采样的方式，来估计数据分布规律。")])]),s._v(" "),a("h2",{attrs:{id:"_2-10-count-distinct-优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-10-count-distinct-优化"}},[s._v("#")]),s._v(" 2.10 Count Distinct 优化")]),s._v(" "),a("p",[s._v("当要统计某一列去重数时，如果数据量很大，count(distinct) 就会非常慢，原因与 order by 类似，count(distinct) 逻辑只会有很少的 reducer 来处理。这时可以用 group by 来改写：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- 先 group by 在 count")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" age "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" department "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"MA"')]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" age\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("h2",{attrs:{id:"_2-11-怎样写in-exists语句"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-11-怎样写in-exists语句"}},[s._v("#")]),s._v(" 2.11 怎样写in/exists语句")]),s._v(" "),a("p",[s._v("在Hive的早期版本中，in/exists语法是不被支持的，但是从 hive-0.8x 以后就开始支持这个语法。但是不推荐使用这个语法。虽然经过测验，Hive-2.3.6 也支持 in/exists 操作，但还是推荐使用 Hive 的一个高效替代方案：left semi join")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- in / exists 实现")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("name "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("name "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("exists")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("应该转换成：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("-- left semi join 实现")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("name "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("left")]),s._v(" semi "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[a("strong",[s._v("需要注意的是，一定要展示的数据只有左表中的数据！")])]),s._v(" "),a("h2",{attrs:{id:"_2-12-使用-vectorization-矢量查询技术"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-12-使用-vectorization-矢量查询技术"}},[s._v("#")]),s._v(" 2.12 使用 vectorization 矢量查询技术")]),s._v(" "),a("p",[s._v("在计算类似 scan, filter, aggregation 的时候， vectorization 技术以设置批处理的增量大小为 1024 行单次来达到比单条记录单次获得更高的效率。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("set hive.vectorized.execution.enabled=true ;\nset hive.vectorized.execution.reduce.enabled=true;\n")])])]),a("h2",{attrs:{id:"_2-13-多重插入模式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-13-多重插入模式"}},[s._v("#")]),s._v(" 2.13 多重插入模式")]),s._v(" "),a("p",[s._v("如果你碰到一堆SQL，并且这一堆SQL的模式还一样。都是从同一个表进行扫描，做不同的逻辑。可优化的地方：如果有n条SQL，每个SQL执行都会扫描一次这张表")]),s._v(" "),a("p",[s._v("如果一个 HQL 底层要执行 10 个 Job，那么能优化成 8 个一般来说，肯定能有所提高，多重插入就是一个非常实用的技能。一次读取，多次插入，有些场景是从一张表读取数据后，要多次利用，这时可以使用 multi insert 语法：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sale_detail\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("insert")]),s._v(" overwrite "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" sale_detail_multi "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sale_date"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2019'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nregion"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'china'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" shop_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" customer_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" total_price "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("insert")]),s._v(" overwrite "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" sale_detail_multi "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("partition")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("sale_date"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2020'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\nregion"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'china'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" shop_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" customer_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" total_price "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[a("strong",[s._v("说明：multi insert语法有一些限制。")])]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("、一般情况下，单个"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SQL")]),s._v("中最多可以写"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("128")]),s._v("路输出，超过"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("128")]),s._v("路，则报语法错误。\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("、在一个multi "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("insert")]),s._v("中：\n对于分区表，同一个目标分区不允许出现多次。\n对于未分区表，该表不能出现多次。\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v("、对于同一张分区表的不同分区，不能同时有"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("insert")]),s._v(" overwrite和"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("insert")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("into")]),s._v("操作，否则报错返回\n")])])]),a("p",[s._v("Multi-Group by 是 Hive 的一个非常好的特性，它使得 Hive 中利用中间结果变得非常方便。例如：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("status")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("school"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("gender "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" status_updates a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("JOIN")]),s._v(" profiles b\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("userid "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("userid "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ds"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2019-03-20'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" subq1\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INSERT")]),s._v(" OVERWRITE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" gender_summary "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PARTITION")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("ds"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2019-03-20'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" subq1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("gender"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GROUP")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" subq1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("gender\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INSERT")]),s._v(" OVERWRITE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" school_summary "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PARTITION")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("ds"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2019-03-20'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" subq1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("school"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GROUP")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" subq1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("school"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("上述查询语句使用了 Multi-Group by 特性连续 group by 了 2 次数据，使用不同的 Multi-Group by。这一特性可以减少一次 MapReduce 操作。")]),s._v(" "),a("h2",{attrs:{id:"_2-14-启动中间结果压缩"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-14-启动中间结果压缩"}},[s._v("#")]),s._v(" 2.14 启动中间结果压缩")]),s._v(" "),a("p",[a("strong",[s._v("map 输出压缩")])]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("set mapreduce.map.output.compress=true;\nset mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;\n")])])]),a("p",[a("strong",[s._v("中间数据压缩")])]),s._v(" "),a("p",[s._v("中间数据压缩就是对 hive 查询的多个 Job 之间的数据进行压缩。最好是选择一个节省CPU耗时的压缩方式。可以采用 snappy 压缩算法，该算法的压缩和解压效率都非常高。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("set hive.exec.compress.intermediate=true;\nset hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;\nset hive.intermediate.compression.type=BLOCK;\n")])])]),a("p",[a("strong",[s._v("结果数据压缩")])]),s._v(" "),a("p",[s._v("最终的结果数据（Reducer输出数据）也是可以进行压缩的，可以选择一个压缩效果比较好的，可以减少数据的大小和数据的磁盘读写时间；注：常用的 gzip，snappy 压缩算法是不支持并行处理的，如果数据源是 gzip/snappy压缩文件大文件，这样只会有有个 mapper 来处理这个文件，会严重影响查询效率。所以如果结果数据需要作为其他查询任务的数据源，可以选择支持 splitable 的 LZO 算法，这样既能对结果文件进行压缩，还可以并行的处理，这样就可以大大的提高 job 执行的速度了。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("set hive.exec.compress.output=true;\nset mapreduce.output.fileoutputformat.compress=true;\nset mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec;\nset mapreduce.output.fileoutputformat.compress.type=BLOCK;\n")])])]),a("p",[a("strong",[s._v("Hadoop集群支持的压缩算法：")])]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("org.apache.hadoop.io.compress.DefaultCodec\norg.apache.hadoop.io.compress.GzipCodec\norg.apache.hadoop.io.compress.BZip2Codec\norg.apache.hadoop.io.compress.DeflateCodec\norg.apache.hadoop.io.compress.SnappyCodec\norg.apache.hadoop.io.compress.Lz4Codec\ncom.hadoop.compression.lzo.LzoCodec\ncom.hadoop.compression.lzo.LzopCodec\n")])])]),a("h1",{attrs:{id:"_3、hive架构层面"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、hive架构层面"}},[s._v("#")]),s._v(" 3、Hive架构层面")]),s._v(" "),a("h2",{attrs:{id:"_3-1-启用本地抓取-默认开启"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-启用本地抓取-默认开启"}},[s._v("#")]),s._v(" 3.1 启用本地抓取（默认开启）")]),s._v(" "),a("p",[s._v("Hive 的某些 SQL 语句需要转换成 MapReduce 的操作，某些 SQL 语句就不需要转换成 MapReduce 操作，但是同学们需要注意，理论上来说，所有的 SQL 语句都需要转换成 MapReduce 操作，只不过Hive 在转换 SQL 语句的过程中会做部分优化，使某些简单的操作不再需要转换成 MapReduce，例如：")]),s._v(" "),a("p",[s._v("1、只是 select * 的时候\n2、where 条件针对分区字段进行筛选过滤时\n3、带有 limit 分支语句时")]),s._v(" "),a("h2",{attrs:{id:"_3-2-本地执行优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-本地执行优化"}},[s._v("#")]),s._v(" 3.2 本地执行优化")]),s._v(" "),a("p",[s._v("Hive 在集群上查询时，默认是在集群上多台机器上运行，需要多个机器进行协调运行，这种方式很好的解决了大数据量的查询问题。但是在 Hive 查询处理的数据量比较小的时候，其实没有必要启动分布式模式去执行，因为以分布式方式执行设计到跨网络传输、多节点协调等，并且消耗资源。对于小数据集，可以通过本地模式，在单台机器上处理所有任务，执行时间明显被缩短。")]),s._v(" "),a("p",[s._v("三个参数：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("## 打开hive自动判断是否启动本地模式的开关\nset hive.exec.mode.local.auto=true;\n## map任务数最大值，不启用本地模式的task最大个数\nset hive.exec.mode.local.auto.input.files.max=4;\n## map输入文件最大大小，不启动本地模式的最大输入文件大小\nset hive.exec.mode.local.auto.inputbytes.max=134217728;\n")])])]),a("h2",{attrs:{id:"_3-3-jvm重用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-jvm重用"}},[s._v("#")]),s._v(" 3.3 JVM重用")]),s._v(" "),a("p",[s._v("Hive 语句最终会转换为一系列的 MapReduce 任务，每一个MapReduce 任务是由一系列的 MapTask和 ReduceTask 组成的，默认情况下，MapReduce 中一个 MapTask 或者 ReduceTask 就会启动一个JVM 进程，一个 Task 执行完毕后，JVM 进程就会退出。这样如果任务花费时间很短，又要多次启动JVM 的情况下，JVM 的启动时间会变成一个比较大的消耗，这时，可以通过重用 JVM 来解决")]),s._v(" "),a("p",[s._v("JVM也是有缺点的，开启JVM重用会一直占用使用到的 task 的插槽，以便进行重用，直到任务完成后才会释放。如果某个 不平衡的job 中有几个 reduce task 执行的时间要比其他的 reduce task 消耗的时间要多得多的话，那么保留的插槽就会一直空闲却无法被其他的 job 使用，直到所有的 task 都结束了才会释放。")]),s._v(" "),a("p",[s._v("根据经验，一般来说可以使用一个 cpu core 启动一个 JVM，假如服务器有 16 个 cpu core ，但是这个节点，可能会启动 32 个mapTask，完全可以考虑：启动一个JVM，执行两个Task")]),s._v(" "),a("h2",{attrs:{id:"_3-4-并行执行"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-并行执行"}},[s._v("#")]),s._v(" 3.4 并行执行")]),s._v(" "),a("p",[s._v("有的查询语句，Hive 会将其转化为一个或多个阶段，包括：MapReduce 阶段、抽样阶段、合并阶段、limit 阶段等。默认情况下，一次只执行一个阶段。但是，如果某些阶段不是互相依赖，是可以并行执行的。多阶段并行是比较耗系统资源的。")]),s._v(" "),a("p",[s._v("一个 Hive SQL 语句可能会转为多个 MapReduce Job，每一个 job 就是一个 stage，这些 Job 顺序执行，这个在 cli 的运行日志中也可以看到。但是有时候这些任务之间并不是是相互依赖的，如果集群资源允许的话，可以让多个并不相互依赖 stage 并发执行，这样就节约了时间，提高了执行速度，但是如果集群资源匮乏时，启用并行化反倒是会导致各个 Job 相互抢占资源而导致整体执行性能的下降。启用并行化")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("## 可以开启并行执行。\nset hive.exec.parallel=true;\n## 同一个sql允许最大并行度，默认为8。\nset hive.exec.parallel.thread.number=16;\n")])])]),a("h2",{attrs:{id:"_3-5-推测执行"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-推测执行"}},[s._v("#")]),s._v(" 3.5 推测执行")]),s._v(" "),a("p",[s._v("在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("# 启动mapper阶段的推测执行机制\nset mapreduce.map.speculative=true;\n# 启动reducer阶段的推测执行机制\nset mapreduce.reduce.speculative=true;\n")])])]),a("p",[s._v("如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的MapTask或者ReduceTask的话，那么启动推测执行造成的浪费是非常巨大大。其实我一般不使用")]),s._v(" "),a("h2",{attrs:{id:"_3-6-hive严格模式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-hive严格模式"}},[s._v("#")]),s._v(" 3.6 Hive严格模式")]),s._v(" "),a("p",[s._v("所谓严格模式，就是强制不允许用户执行有风险的 HiveQL 语句，一旦执行会直接失败。但是Hive中为了提高SQL语句的执行效率，可以设置严格模式，充分利用Hive的某些特点。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("## 设置Hive的严格模式\nset hive.mapred.mode=strict;\nset hive.exec.dynamic.partition.mode=nostrict;\n")])])]),a("p",[s._v("注意：当设置严格模式之后，会有如下限制：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("、对于分区表，必须添加"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("对于分区字段的条件过滤\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student_ptn "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" age "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("25")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("、"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("order")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v("语句必须包含"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("limit")]),s._v("输出限制\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("order")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" age "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("limit")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("3")]),s._v("、限制执行笛卡尔积的查询\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("、在hive的动态分区模式下，如果为严格模式，则必须需要一个分区列式静态分区\n")])])]),a("h1",{attrs:{id:"_4、数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、数据倾斜"}},[s._v("#")]),s._v(" 4、数据倾斜")]),s._v(" "),a("h2",{attrs:{id:"_4-1-不同数据类型关联产生数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-不同数据类型关联产生数据倾斜"}},[s._v("#")]),s._v(" 4.1 不同数据类型关联产生数据倾斜")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" users a\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("left")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("outer")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" logs b\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("usr_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" cast"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("h2",{attrs:{id:"_4-2-空值过滤"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-空值过滤"}},[s._v("#")]),s._v(" 4.2 空值过滤")]),s._v(" "),a("p",[s._v("在生产环境经常会用大量空值数据进入到一个reduce中去，导致数据倾斜。")]),s._v(" "),a("p",[s._v("解决办法：")]),s._v(" "),a("p",[s._v("自定义分区，将为空的key转变为字符串加随机数或纯随机数，将因空值而造成倾斜的数据分不到多个Reducer。")]),s._v(" "),a("p",[s._v("注意：对于异常值如果不需要的话，最好是提前在where条件里过滤掉，这样可以使计算量大大减少")]),s._v(" "),a("h2",{attrs:{id:"_4-3-group-by"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-group-by"}},[s._v("#")]),s._v(" 4.3 group by")]),s._v(" "),a("p",[s._v("采用sum() group by的方式来替换count(distinct)完成计算。")]),s._v(" "),a("h2",{attrs:{id:"_4-4-map-join"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-4-map-join"}},[s._v("#")]),s._v(" 4.4 map join")]),s._v(" "),a("p",[s._v("以上讲过了")]),s._v(" "),a("h2",{attrs:{id:"_4-5-开启数据倾斜是负载均衡"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-5-开启数据倾斜是负载均衡"}},[s._v("#")]),s._v(" 4.5 开启数据倾斜是负载均衡")]),s._v(" "),a("p",[s._v("以上也讲过了")]),s._v(" "),a("h1",{attrs:{id:"_5、调优方案"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5、调优方案"}},[s._v("#")]),s._v(" 5、调优方案")]),s._v(" "),a("h2",{attrs:{id:"_5-1-日志表和用户表做链接"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-日志表和用户表做链接"}},[s._v("#")]),s._v(" 5.1 日志表和用户表做链接")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("select * from log a left outer join users b on a.user_id = b.user_id;\n")])])]),a("p",[s._v("users 表有 600w+ （假设有5G）的记录，把 users 分发到所有的 map 上也是个不小的开销，而且MapJoin 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。")]),s._v(" "),a("p",[a("strong",[s._v("改进方案：")])]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/*+mapjoin(x)*/")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" log a\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("left")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("outer")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/*+mapjoin(c)*/")]),s._v(" d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("distinct")]),s._v(" user_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" log "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" c "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" users d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" x\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("假如，log 里 user_id 有上百万个，这就又回到原来 MapJoin 问题。所幸，每日的会员 uv 不会太多，有交易的会员不会太多，有点击的会员不会太多，有佣金的会员不会太多等等。所以这个方法能解决很多场景下的数据倾斜问题。")]),s._v(" "),a("h2",{attrs:{id:"_5-2-位图法求连续七天发朋友圈的用户"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-位图法求连续七天发朋友圈的用户"}},[s._v("#")]),s._v(" 5.2 位图法求连续七天发朋友圈的用户")]),s._v(" "),a("p",[s._v("每天都要求 微信朋友圈 过去连续7天都发了朋友圈的小伙伴有哪些？假设每个用户每发一次朋友圈都记录了一条日志。每一条朋友圈包含的内容")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("日期，用户ID，朋友圈内容.....\ndt, userid, content, .....\n")])])]),a("p",[s._v("如果 微信朋友圈的 日志数据，按照日期做了分区。")]),s._v(" "),a("p",[s._v("2020-07-06 file1.log(可能会非常大)\n2020-07-05 file2.log\n…")]),s._v(" "),a("p",[s._v("解决方案：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("假设微信有10E用户，我们每天生成一个长度为10E的二进制数组，每个位置要么是0，要么是1，如果为1，代表该用户当天发了朋友圈。如果为0，代表没有发朋友圈。\n然后每天：10E / 8 / 1024 / 1024 = 119M左右\n求Join实现：两个数组做 求且、求或、异或、求反、求新增\n")])])]),a("h2",{attrs:{id:""}},[a("a",{staticClass:"header-anchor",attrs:{href:"#"}},[s._v("#")])]),s._v(" "),a("h2",{attrs:{id:"_6-reference"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-reference"}},[s._v("#")]),s._v(" "),a("strong",[s._v("6.Reference")])]),s._v(" "),a("p",[s._v("https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution")]),s._v(" "),a("p",[s._v("https://cwiki.apache.org/confluence/display/Hive/GettingStarted")])])}),[],!1,null,null,null);t.default=r.exports}}]);