(window.webpackJsonp=window.webpackJsonp||[]).push([[41],{426:function(e,a,t){"use strict";t.r(a);var r=t(30),v=Object(r.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"hive性能调优-数据倾斜"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#hive性能调优-数据倾斜"}},[e._v("#")]),e._v(" Hive性能调优 | 数据倾斜")]),e._v(" "),t("h3",{attrs:{id:"map数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#map数"}},[e._v("#")]),e._v(" Map数")]),e._v(" "),t("ol",[t("li",[e._v("通常情况下，作业会通过input的目录产生一个或者多个map任务。主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小(目前为128M，可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改)；")]),e._v(" "),t("li",[e._v("举例：a)一个大文件：假设input目录下有1个文件a，大小为780M，那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数。b) 多个小文件：假设input目录下有3个文件a，b，c大小分别为10m，20m，150m，那么hadoop会分隔成4个块（10m，20m，128m，22m），从而产生4个map数。即，如果文件大于块大小(128m)，那么会拆分，如果小于块大小，则把该文件当成一个块。")]),e._v(" "),t("li",[e._v("是不是map数越多越好? 答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。")]),e._v(" "),t("li",[e._v("是不是保证每个map处理接近128m的文件块，就高枕无忧了？答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。")])]),e._v(" "),t("p",[e._v("针对上面的问题3和4，我们需要采取两种方式来解决：即减少map数和增加map数。")]),e._v(" "),t("h3",{attrs:{id:"如何适当的增加map数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何适当的增加map数"}},[e._v("#")]),e._v(" 如何适当的增加map数")]),e._v(" "),t("p",[e._v("当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。针对上面的第4条 假设有这样一个任务：")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("Select data_desc,\ncount(1),\ncount(distinct id),\nsum(case when …),\nsum(case when …),\nsum(…)\nfrom a group by data_desc\n")])])]),t("p",[e._v("如果表a只有一个文件，大小为120M，但包含几千万的记录，如果用1个map去完成这个任务，肯定是比较耗时的，这种情况下，我们要考虑将这一个文件合理的拆分成多个，这样就可以用多个map任务去完成。")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("set mapreduce.job.reduces =10;\ncreate table a_1 as\nselect * from a\ndistribute by rand();\n")])])]),t("p",[e._v("这样会将a表的记录，随机的分散到包含10个文件的a_1表中，再用a_1代替上面sql中的a表，则会用10个map任务去完成。")]),e._v(" "),t("p",[e._v("每个map任务处理大于12M（几百万记录）的数据，效率肯定会好很多。")]),e._v(" "),t("p",[e._v("看上去，貌似这两种有些矛盾，一个是要合并小文件，一个是要把大文件拆成小文件，这点正是重点需要关注的地方，根据实际情况，控制map数量需要遵循两个原则：使大数据量利用合适的map数；使单个map任务处理合适的数据量；")]),e._v(" "),t("h3",{attrs:{id:"调整reduce数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#调整reduce数"}},[e._v("#")]),e._v(" 调整reduce数")]),e._v(" "),t("ol",[t("li"),e._v(" "),t("li",[t("ol",[t("li",[e._v("调整reduce个数方法一")])])])]),e._v(" "),t("p",[e._v("a） 每个Reduce 处理的数据量默认是256MB")]),e._v(" "),t("blockquote",[t("p",[e._v("hive.exec.reducers.bytes.per.reducer=256123456")])]),e._v(" "),t("p",[e._v("b） 每个任务最大的reduce数，默认为1009")]),e._v(" "),t("blockquote",[t("p",[e._v("hive.exec.reducers.max=1009")])]),e._v(" "),t("p",[e._v("c）计算reducer数的公式")]),e._v(" "),t("blockquote",[t("p",[e._v("N=min(参数2，总输入数据量/参数1)")])]),e._v(" "),t("p",[e._v("参数1：每个Reduce处理的最大数据量 参数2：每个任务最大Reduce数量")]),e._v(" "),t("ol",[t("li",[e._v("调整reduce个数方法二")])]),e._v(" "),t("p",[e._v("在hadoop的mapred-default.xml文件中修改 设置每个job的Reduce个数")]),e._v(" "),t("blockquote",[t("p",[e._v("set mapreduce.job.reduces = 15;")])]),e._v(" "),t("ol",[t("li",[e._v("reduce个数并不是越多越好")])]),e._v(" "),t("p",[e._v("a）过多的启动和初始化reduce也会消耗时间和资源；b） 有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；")]),e._v(" "),t("p",[e._v("总结: 在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适.")])])}),[],!1,null,null,null);a.default=v.exports}}]);