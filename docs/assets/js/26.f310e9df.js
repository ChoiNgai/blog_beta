(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{418:function(t,s,a){"use strict";a.r(s);var e=a(30),n=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"hive-调优"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-调优"}},[t._v("#")]),t._v(" Hive 调优")]),t._v(" "),a("h3",{attrs:{id:"分区表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分区表"}},[t._v("#")]),t._v(" 分区表")]),t._v(" "),a("h5",{attrs:{id:"hive-查询基本原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-查询基本原理"}},[t._v("#")]),t._v(" Hive 查询基本原理：")]),t._v(" "),a("blockquote",[a("p",[t._v("Hive的设计思想是通过元数据将HDFS上的文件映射成表，基本的查询原理是当用户通过HQL语句对Hive中的表进行复杂数据处理和计算时，默认将其转换为分布式计算MapReduce程序对HDFS中的数据进行读取处理的过程。")])]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("==普通表结构：==")]),t._v(" "),a("p",[t._v("​\t默认的普通表结构中，表的最后一级目录就是表的目录，而底层的计算会使用表的最后一级目录作为Input进行计算，这种场景下，我们就会遇到一个问题，如果表的数据很多，而我们需要被处理的数据很少，只是其中一小部分，这样就会导致大量不必要的数据被程序加载，在程序中被过滤，导致大量不必要的计算资源的浪费。")]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer//img/image-20211120225439252.png",alt:"image-20211120225439252"}}),t._v(" "),a("p",[t._v("==分区表设计思想：==")]),t._v(" "),a("p",[t._v("​\t针对上面的问题，Hive提供了一种特殊的表结构来解决——分区表结构。分区表结构的设计思想是：根据查询的需求，将数据按照查询的条件"),a("strong",[t._v("一般都以时间")]),t._v("进行划分分区存储，将不同分区的数据单独使用一个HDFS目录来进行存储，当底层实现计算时，根据查询的条件，只读取对应分区的数据作为输入，减少不必要的数据加载，提高程序的性能。")]),t._v(" "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer//img/image-20211120225608249.png",alt:"image-20211120225608249"}}),t._v(" "),a("p",[t._v("==代码测试==：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--创建表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_login_part"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  userid string\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \npartitioned "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logindate string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("row")]),t._v(" format delimited "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("fields")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("terminated")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--开启动态分区")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("exec")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dynamic"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("partition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("mode")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("nonstrict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--按登录日期分区")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("insert")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("into")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_login_part "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("partition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logindate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_login"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n")])])]),a("p",[t._v("我们可以通过"),a("code",[t._v("Explain Extend")]),t._v("来查看具体的执行计划：")]),t._v(" "),a("ul",[a("li",[t._v("不做分区的表格:")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("explain")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extended")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v("\n  logindate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" cnt\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_login\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" logindate "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2021-03-23'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("or")]),t._v(" logindate "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2021-03-24'")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" logindate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("具体如下所示：")]),t._v(" "),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer//img/image-20211120230120281.png",alt:"zo"}}),t._v(" "),a("ul",[a("li",[t._v("做了分区的表格：")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("explain")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extended")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v("\n  logindate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" cnt\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_login_part\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" logindate "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2021-03-23'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("or")]),t._v(" logindate "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2021-03-24'")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" logindate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("具体如下所示：")]),t._v(" "),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120230234579.png",alt:"image-20211120230234579"}}),t._v(" "),a("p",[t._v("从图中可以看到根据"),a("code",[t._v("logindate")]),t._v("做了分区后，查询时文件也做了分离。")]),t._v(" "),a("h3",{attrs:{id:"分桶表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分桶表"}},[t._v("#")]),t._v(" 分桶表")]),t._v(" "),a("h5",{attrs:{id:"主要解决-hive-中的-join-问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#主要解决-hive-中的-join-问题"}},[t._v("#")]),t._v(" 主要解决 Hive 中的 Join 问题")]),t._v(" "),a("blockquote",[a("p",[t._v("表的Join是数据分析处理过程中必不可少的操作，Hive同样支持Join的语法，Hive Join的底层还是通过MapReduce来实现的，但是Hive实现Join时面临一个问题：如果有两张非常大的表要进行Join，两张表的数据量都很大，Hive底层通过MapReduce实现时，无法使用MapJoin提高Join的性能，只能走默认的ReduceJoin，而ReduceJoin必须经过Shuffle过程，相对性能比较差，而且容易产生数据倾斜，如何解决这个问题？")])]),t._v(" "),a("p",[t._v("==分桶表设计思想：==")]),t._v(" "),a("p",[t._v("​\t针对以上的问题，Hive中提供了另外一种表的结构——分桶表结构。分桶表的设计有别于分区表的设计，分区表是将数据划分不同的目录进行存储，而"),a("em",[t._v("分桶表是将数据划分不同的文件进行存储")]),t._v("。分桶表的设计是按照一定的规则"),a("strong",[t._v("通过MapReduce中的多个Reduce来实现")]),t._v("将数据划分到不同的文件中进行存储，构建分桶表。")]),t._v(" "),a("p",[t._v("​\t如果有两张表按照相同的划分规则"),a("strong",[t._v("按照Join的关联字段")]),t._v("将各自的数据进行划分，在Join时，就可以实现Bucket与Bucket的Join，避免不必要的比较。")]),t._v(" "),a("p",[t._v("==举个例子：==")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t当前有两张表，订单表有1000万条，用户表有10万条，两张表的关联字段是userid，现在要实现两张表的Join。")]),t._v(" "),a("p",[t._v("​\t我们将订单表按照userid划分为3个桶，1000万条数据按照userid的hash取余存储在对应的Bucket中。同理，我们再将用户表按照相同的规则，存储在3个桶中。")]),t._v(" "),a("p",[t._v("​\t在Join时，只需要将两张表的Bucket0与Bucket0进行Join，Bucket1与Bucket1进行Join，Bucket2与Bucket2进行Join即可，不用让所有的数据挨个比较，降低了比较次数，提高了Join的性能。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120231013932.png",alt:""}})])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("ul",[a("li",[t._v("创建两张表格")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#  构建分桶dept表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("use")]),t._v(" db_emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 创建分桶表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_dept02"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    deptno string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    dname string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    loc string\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("clustered")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deptno"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" sorted "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deptno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("asc")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("into")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v(" buckets\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("row")]),t._v(" format delimited "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("fields")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("terminated")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("--写入分桶表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("insert")]),t._v(" overwrite "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_dept02\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_dept01"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# \t构建分桶emp表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("use")]),t._v(" db_emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 创建分桶表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_emp02"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n   empno string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   ename string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   job string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   managerid string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   hiredate string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   salary "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("double")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   jiangjin "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("double")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   deptno string\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("clustered")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deptno"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" sorted "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("deptno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("asc")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("into")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v(" buckets\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("row")]),t._v(" format delimited "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("fields")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("terminated")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 写入分桶表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("insert")]),t._v(" overwrite "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_emp02\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_emp01"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("ul",[a("li",[t._v("基于emp 和 dept 表格来join")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 开启分桶SMB join")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bucketmapjoin "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auto"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("convert")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sortmerge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bucketmapjoin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sortedmerge "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 查看执行计划")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("explain")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v("\n  a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("empno"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("salary"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deptno"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dname\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_emp02 a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" tb_dept02 b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deptno "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deptno"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("执行计划如图所示：")]),t._v(" "),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120231522028.png",alt:"image-20211120231522028"}}),t._v(" "),a("p",[t._v("对比普通表下使用join的执行计划：")]),t._v(" "),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120231604745.png",alt:"image-20211120231604745"}}),t._v(" "),a("h3",{attrs:{id:"索引设计"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#索引设计"}},[t._v("#")]),t._v(" 索引设计")]),t._v(" "),a("h5",{attrs:{id:"hive-中的索引"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-中的索引"}},[t._v("#")]),t._v(" Hive 中的索引")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t在传统的关系型数据库例如MySQL、Oracle等数据库中，为了提高数据的查询效率，可以为表中的字段单独构建索引，查询时，可以基于字段的索引快速的实现查询、过滤等操作。")]),t._v(" "),a("p",[t._v("​\tHive中也同样提供了索引的设计，允许用户为字段构建索引，提高数据的查询效率。但是Hive的索引与关系型数据库中的索引并不相同，比如，Hive不支持主键或者外键。Hive索引可以建立在表中的某些列上，以提升一些操作的效率，例如减少MapReduce任务中需要读取的数据块的数量。")]),t._v(" "),a("p",[t._v("​\t在可以预见到分区数据非常庞大的情况下，分桶和索引常常是优于分区的。而分桶由于SMB Join对关联键要求严格，所以并不是总能生效。")])]),t._v(" "),a("p",[t._v("==应用问题==")]),t._v(" "),a("p",[t._v("​\t由于Hive的索引设计过于繁琐，所以从Hive3.0版本开始，取消了对Hive Index的支持及使用，不过如果使用的是Hive1.x或者Hive2.x在特定的场景下依旧可以使用Hive Index来提高性能。")]),t._v(" "),a("p",[t._v("​\t实际工作场景中，一般不推荐使用Hive Index，推荐使用ORC文件格式中的索引来代替Hive Index提高查询性能。")]),t._v(" "),a("h3",{attrs:{id:"hive表数据优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive表数据优化"}},[t._v("#")]),t._v(" Hive表数据优化")]),t._v(" "),a("h5",{attrs:{id:"文件格式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#文件格式"}},[t._v("#")]),t._v(" 文件格式：")]),t._v(" "),a("blockquote",[a("p",[t._v("​\tHive数据存储的本质还是HDFS，所有的数据读写都基于HDFS的文件来实现，为了提高对HDFS文件读写的性能，Hive中提供了多种文件存储格式：TextFile、SequenceFile、RCFile、ORC、Parquet等。不同的文件存储格式具有不同的存储特点，有的可以降低存储空间，有的可以提高查询性能等，可以用来实现不同场景下的数据存储，以提高对于数据文件的读写效率。")])]),t._v(" "),a("p",[t._v("各个格式的具体介绍（来自官方文档）：")]),t._v(" "),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120232203172.png",alt:"image-20211120232203172"}}),t._v(" "),a("h5",{attrs:{id:"textfile"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#textfile"}},[t._v("#")]),t._v(" TextFile:")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\tTextFIle是Hive中默认的文件格式，存储形式为按行存储。工作中最常见的数据文件格式就是TextFile文件，几乎所有的原始数据生成都是TextFile格式，所以Hive设计时考虑到为了避免各种编码及数据错乱的问题，选用了TextFile作为默认的格式。建表时不指定存储格式即为textfile，导入数据时把数据文件拷贝至hdfs不进行处理。")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("TextFile的优点")])])]),t._v(" "),a("p",[t._v("最简单的数据格式，不需要经过处理，可以直接cat查看")]),t._v(" "),a("p",[t._v("可以使用任意的分隔符进行分割")]),t._v(" "),a("p",[t._v("便于和其他工具（Pig, grep, sed, awk）共享数据")]),t._v(" "),a("p",[t._v("可以搭配Gzip、Bzip2、Snappy等压缩一起使用")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("TextFile的缺点")])])]),t._v(" "),a("p",[t._v("耗费存储空间，I/O性能较低")]),t._v(" "),a("p",[t._v("结合压缩时Hive不进行数据切分合并，不能进行并行操作，查询效率低")]),t._v(" "),a("p",[t._v("按行存储，读取列的性能差")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("TextFile的应用场景")])])]),t._v(" "),a("p",[t._v("适合于小量数据的存储查询")]),t._v(" "),a("p",[t._v("一般用于做第一层数据加载和测试使用")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 创建textfile数据表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_sogou_text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  stime string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  userid string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  keyword string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  clickorder string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  url string\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("row")]),t._v(" format delimited "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("fields")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("terminated")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t'")]),t._v("\nstored "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" textfile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h5",{attrs:{id:"sequencefile"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sequencefile"}},[t._v("#")]),t._v(" SequenceFile:")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\tSequenceFile是Hadoop里用来存储序列化的键值对即二进制的一种文件格式。SequenceFile文件也可以作为MapReduce作业的输入和输出，hive也支持这种格式。")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("SequenceFIle的优点")])])]),t._v(" "),a("p",[t._v("以二进制的KV形式存储数据，与底层交互更加友好，性能更快")]),t._v(" "),a("p",[t._v("可压缩、可分割，优化磁盘利用率和I/O")]),t._v(" "),a("p",[t._v("可并行操作数据，查询效率高")]),t._v(" "),a("p",[t._v("SequenceFile也可以用于存储多个小文件")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("SequenceFIle的缺点")])])]),t._v(" "),a("p",[t._v("存储空间消耗最大")]),t._v(" "),a("p",[t._v("与非Hadoop生态系统之外的工具不兼容")]),t._v(" "),a("p",[t._v("构建SequenceFile需要通过TextFile文件转化加载。")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("SequenceFIle的应用")])])]),t._v(" "),a("p",[t._v("适合于小量数据，但是查询列比较多的场景")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 创建SequenceFile数据表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_sogou_seq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  stime string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  userid string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  keyword string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  clickorder string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  url string\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("row")]),t._v(" format delimited "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("fields")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("terminated")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t'")]),t._v("\nstored "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sequencefile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h5",{attrs:{id:"parquet"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#parquet"}},[t._v("#")]),t._v(" Parquet:")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\tParquet是一种支持嵌套结构的"),a("strong",[t._v("列式存储")]),t._v("文件格式，最早是由Twitter和Cloudera合作开发，2015年5月从Apache孵化器里毕业成为Apache顶级项目。是一种支持嵌套数据模型对的列式存储系统，作为大数据系统中OLAP查询的优化方案，它已经被多种查询引擎原生支持，并且部分高性能引擎将其作为默认的文件存储格式。通过数据编码和压缩，以及映射下推和谓词下推功能，Parquet的性能也较之其它文件格式有所提升。")]),t._v(" "),a("p",[t._v("​\t\tParquet 是与语言无关的，而且不与任何一种数据处理框架绑定在一起，适配多种语言和组件，能够与 Parquet 适配的查询引擎包括 Hive, Impala, Pig, Presto, Drill, Tajo, HAWQ, IBM Big SQL等，计算框架包括 MapReduce, Spark, Cascading, Crunch, Scalding, Kite 等")]),t._v(" "),a("p",[t._v("​\t\tParquet是Hadoop生态圈中主流的列式存储格式，并且行业内流行这样一句话流传：如果说HDFS是大数据时代文件系统的事实标准，Parquet 就是大数据时代存储格式的事实标准。Hive中也同样支持使用Parquet格式来实现数据的存储，并且是工作中主要使用的存储格式之一。")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("Parquet的优点")])])]),t._v(" "),a("p",[t._v("更高效的压缩和编码")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120232912397.png",alt:""}})]),t._v(" "),a("p",[t._v("可用于多种数据处理框架")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211120232946607.png",alt:""}})]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("Parquet的缺点")])])]),t._v(" "),a("p",[t._v("不支持update, insert, delete, ACID")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("Parquet的应用")])])]),t._v(" "),a("p",[t._v("适用于字段数非常多，无更新，只取部分列的查询。")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 创建Parquet数据表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_sogou_parquet"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  stime string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  userid string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  keyword string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  clickorder string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  url string\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("row")]),t._v(" format delimited "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("fields")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("terminated")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t'")]),t._v("\nstored "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" parquet"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h5",{attrs:{id:"orc"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#orc"}},[t._v("#")]),t._v(" ORC：")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\t"),a("strong",[t._v("ORC（OptimizedRC File）"),a("strong",[t._v("文件格式也是一种Hadoop生态圈中的")]),t._v("列式存储")]),t._v("格式，源自于RC（RecordColumnar File），用于降低Hadoop数据存储空间和加速Hive查询速度。它并不是一个单纯的列式存储格式，仍然是首先根据行组分割整个表，在每一个行组内进行"),a("strong",[t._v("按列存储")]),t._v("。ORC文件是自描述的，它的元数据使用Protocol Buffers序列化，并且文件中的数据尽可能的压缩以降低存储空间的消耗，目前也被Hive、Spark SQL、Presto等查询引擎支持。")]),t._v(" "),a("p",[t._v("​\tORC文件也是以二进制方式存储的，所以是不可以直接读取，ORC文件也是自解析的，它包含许多的元数据，这些元数据都是同构ProtoBuffer进行序列化的。其中涉及到如下的概念：")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("ORC文件：保存在文件系统上的普通二进制文件，一个ORC文件中可以包含多个stripe，每一stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。")])]),t._v(" "),a("li",[a("p",[t._v("文件级元数据：包括文件的描述信息PostScript、文件meta信息（包括整个文件的统计信息）、所有stripe的信息和文件schema信息。")])]),t._v(" "),a("li",[a("p",[t._v("stripe：一组行形成一个stripe，每次读取文件是以行组为单位的，一般为HDFS的块大小，保存了每一列的索引和数据。")])]),t._v(" "),a("li",[a("p",[t._v("stripe元数据：保存stripe的位置、每一个列的在该stripe的统计信息以及所有的stream类型和位置。")])]),t._v(" "),a("li",[a("p",[t._v("row group：索引的最小单位，一个stripe中包含多个row group，默认为10000个值组成。")])]),t._v(" "),a("li",[a("p",[t._v("stream：一个stream表示文件中一段有效的数据，包括索引和数据两类。索引stream保存每一个row group的位置和统计信息，数据stream包括多种类型的数据，具体需要哪几种是由该列类型和编码方式决定。")])])])]),t._v(" "),a("blockquote",[a("ul",[a("li",[a("strong",[t._v("ORC的优点")])])]),t._v(" "),a("p",[t._v("列式存储，存储效率非常高")]),t._v(" "),a("p",[t._v("可压缩，高效的列存取")]),t._v(" "),a("p",[t._v("查询效率较高，支持索引")]),t._v(" "),a("p",[t._v("支持矢量化查询")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("ORC的缺点**")]),t._v(" "),a("p",[t._v("加载时性能消耗较大")]),t._v(" "),a("p",[t._v("需要通过text文件转化生成")]),t._v(" "),a("p",[t._v("读取全量数据时性能较差")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("ORC的应用")])])])]),t._v(" "),a("p",[t._v("适用于Hive中大型的存储、查询")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 创建ORC数据表")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_sogou_orc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  stime string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  userid string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  keyword string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  clickorder string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  url string\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("row")]),t._v(" format delimited "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("fields")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("terminated")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\t'")]),t._v("\nstored "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" orc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h3",{attrs:{id:"存储优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#存储优化"}},[t._v("#")]),t._v(" 存储优化")]),t._v(" "),a("h5",{attrs:{id:"避免小文件生成"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#避免小文件生成"}},[t._v("#")]),t._v(" 避免小文件生成：")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\tHive的存储本质还是HDFS，HDFS是不利于小文件存储的，因为每个小文件会产生一条元数据信息，并且不利用MapReduce的处理，MapReduce中每个小文件会启动一个MapTask计算处理，导致资源的浪费，所以在使用Hive进行处理分析时，要尽量避免小文件的生成。")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 如果hive的程序，只有maptask，将MapTask产生的所有小文件进行合并")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("merge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mapfiles"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 如果hive的程序，有Map和ReduceTask,将ReduceTask产生的所有小文件进行合并")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("merge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mapredfiles"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 每一个合并的文件的大小")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("merge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("per"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("task"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("256000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 平均每个文件的大小，如果小于这个值就会进行合并")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("merge")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("smallfiles"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("avgsize"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[a("strong",[t._v("读取小文件：")])]),t._v(" "),a("p",[t._v("​\t我们总会遇到数据处理的中间结果是小文件的情况，例如每个小时的分区数据中，大多数小时的数据都比较多，但是个别几个小时，如凌晨的2点~6点等等，数据量比较小，下一步进行处理时就必须对多个小文件进行处理，那么这种场景下怎么解决呢？")]),t._v(" "),a("p",[t._v("​\t类似于MapReduce中的解决方案，Hive中也提供一种输入类CombineHiveInputFormat，用于将小文件合并以后，再进行处理。")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 设置Hive中底层MapReduce读取数据的输入类：将所有文件合并为一个大文件作为输入")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("format"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("io"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CombineHiveInputFormat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h5",{attrs:{id:"orc文件索引"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#orc文件索引"}},[t._v("#")]),t._v(" ORC文件索引：")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t在使用ORC文件时，为了加快读取ORC文件中的数据内容，ORC提供了两种索引机制：Row Group Index 和 Bloom Filter Index可以帮助提高查询ORC文件的性能，当用户写入数据时，可以指定构建索引，当用户查询数据时，可以根据索引提前对数据进行过滤，避免不必要的数据扫描。")])]),t._v(" "),a("h6",{attrs:{id:"row-group-index"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#row-group-index"}},[t._v("#")]),t._v(" ==Row Group Index:==")]),t._v(" "),a("p",[t._v("​\t\t一个ORC文件包含一个或多个stripes(groups of row data)，每个stripe中包含了每个column的min/max值的索引数据，"),a("strong",[t._v("当查询中有<,>,=的操作时，会根据min/max值，跳过扫描不包含的stripes")]),t._v("。而其中为"),a("strong",[t._v("每个stripe建立的包含min/max值的索引，就称为Row Group Index行组索引")]),t._v("，也叫min-max Index大小对比索引，或者Storage Index。")]),t._v(" "),a("p",[t._v("​\t\t在建立ORC格式表时，指定表参数’orc.create.index’=’true’之后，便会建立Row Group Index，需要注意的是，为了使Row Group Index有效利用，向表中加载数据时，"),a("strong",[t._v("必须对需要使用索引的字段进行排序")]),t._v("，否则，min/max会失去意义。另外，"),a("strong",[t._v("这种索引主要用于数值型字段的范围查询过滤优化上")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121103847578.png",alt:""}})]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("index")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filter"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 永久生效，请配置在hive-site.xml中")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_sogou_orc_index\nstored "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" orc tblproperties "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"orc.create.index"')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"true"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_sogou_source\ndistribute "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" stime\nsort "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" stime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h6",{attrs:{id:"bloom-filter-index"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bloom-filter-index"}},[t._v("#")]),t._v(" ==Bloom Filter Index:==")]),t._v(" "),a("p",[t._v("​\t\t建表时候，通过表参数”orc.bloom.filter.columns”=”columnName……”来指定为哪些字段建立BloomFilter索引，这样，在生成数据的时候，会在每个stripe中，为该字段建立BloomFilter的数据结构，当查"),a("strong",[t._v("询条件中包含对该字段的=号过滤时候")]),t._v("，先从BloomFilter中获取以下是否包含该值，如果不包含，则跳过该stripe。")]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 创建表，并指定构建索引")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" tb_sogou_orc_bloom\nstored "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" orc tblproperties \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v('-- ("orc.create.index"="true",orc.bloom.filter.columns"="stime,userid") 配置代码')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_sogou_source\ndistribute "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" stime\nsort "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" stime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- stime的范围过滤可以走row group index，userid的过滤可以走bloom filter index")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_sogou_orc_index\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" stime "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'12:00:00'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" stime "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'18:00:00'")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" userid "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'3933365481995287'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h6",{attrs:{id:"orc矢量化查询"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#orc矢量化查询"}},[t._v("#")]),t._v(" ==ORC矢量化查询:==")]),t._v(" "),a("p",[t._v("​\t\tHive的默认查询执行引擎一次处理一行，而矢量化查询执行是一种Hive针对ORC文件操作的特性，目的是按照每批1024行读取数据，并且一次性对整个记录整合（而不是对单条记录）应用操作，提升了像过滤, 联合, 聚合等等操作的性能。")]),t._v(" "),a("p",[t._v("​\t\t注意：要使用矢量化查询执行，就必须以"),a("strong",[t._v("ORC格式")]),t._v("存储数据。")]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vectorized"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("execution"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("enabled "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vectorized"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("execution"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduce"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("enabled "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h3",{attrs:{id:"计算-job-执行优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#计算-job-执行优化"}},[t._v("#")]),t._v(" 计算 Job 执行优化")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\tHiveQL是一种类SQL的语言，从编程语言规范来说是一种声明式语言，用户会根据查询需求提交声明式的HQL查询，而Hive会根据底层计算引擎将其转化成Mapreduce/Tez/Spark的 job。")]),t._v(" "),a("p",[t._v("​\t\texplain会解析HQL语句，将整个HQL语句的实现步骤、依赖关系、实现过程都会进行解析返回，可以帮助更好的了解一条HQL语句在底层是如何实现数据的查询及处理的过程，这样可以辅助用户对Hive进行优化。")]),t._v(" "),a("p",[t._v("​\t\t官网：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain"),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121104815958.png",alt:"image-20211121104815958"}})])]),t._v(" "),a("p",[t._v("==语法解释：==")]),t._v(" "),a("p",[a("code",[t._v("EXPLAIN [FORMATTED|EXTENDED|DEPENDENCY|AUTHORIZATION|] query")])]),t._v(" "),a("p",[t._v("\tFORMATTED：对执行计划进行格式化，返回JSON格式的执行计划\n\tEXTENDED：提供一些额外的信息，比如文件的路径信息\n\tDEPENDENCY：以JSON格式返回查询所依赖的表和分区的列表\n\tAUTHORIZATION：列出需要被授权的条目，包括输入与输出")]),t._v(" "),a("p",[t._v("==解析执行：==")]),t._v(" "),a("p",[t._v("解析后的执行计划一般由三个部分构成，分别是：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("The Abstract Syntax Tree for the query")])])]),t._v(" "),a("p",[t._v("抽象语法树：Hive使用Antlr解析生成器，可以自动地将HQL生成为抽象语法树")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("The dependencies between the different stages of the plan")])])]),t._v(" "),a("p",[t._v("Stage依赖关系：会列出运行查询所有的依赖以及stage的数量")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("The description of each of the stages")])])]),t._v(" "),a("p",[t._v("Stage内容：包含了非常重要的信息，比如运行时的operator和sort orders等具体的信息")]),t._v(" "),a("p",[t._v("==示例1：过滤==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("explain")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" cnt "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_emp "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" deptno "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'10'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("ul",[a("li",[t._v("组成：")])]),t._v(" "),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121105109366.png",alt:"image-20211121105109366"}}),t._v(" "),a("ul",[a("li",[t._v("解释：")])]),t._v(" "),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121105228837.png",alt:"image-20211121105228837"}}),t._v(" "),a("p",[t._v("==示例2:分组排序==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("explain")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v("\n   deptno"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" cnt\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tb_emp "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" salary "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2000")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" deptno "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("order")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" cnt "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("desc")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("ul",[a("li",[t._v("==解释：==")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121105432180.png",alt:""}})]),t._v(" "),a("h3",{attrs:{id:"mapreduce-属性优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce-属性优化"}},[t._v("#")]),t._v(" MapReduce 属性优化：")]),t._v(" "),a("h5",{attrs:{id:"jvm重用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#jvm重用"}},[t._v("#")]),t._v(" JVM重用：")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\tJVM正常指代一个Java进程，Hadoop默认使用派生的JVM来执行map-reducer，如果一个MapReduce程序中有100个Map，10个Reduce，Hadoop默认会为每个Task启动一个JVM来运行，那么就会启动100个JVM来运行MapTask，在JVM启动时内存开销大，尤其是Job大数据量情况，如果单个Task数据量比较小，也会申请JVM资源，这就导致了资源紧张及浪费的情况。")]),t._v(" "),a("p",[t._v("​\t\t为了解决上述问题，MapReduce中提供了JVM重用机制来解决，JVM重用可以使得JVM实例在同一个job中重新使用N次，当一个Task运行结束以后，JVM不会进行释放，而是继续供下一个Task运行，直到运行了N个Task以后，就会释放，N的值可以在Hadoop的mapred-site.xml文件中进行配置，通常在10-20之间。")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- Hadoop3之前的配置，在mapred-site.xml中添加以下参数")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- Hadoop3中已不再支持该选项")]),t._v("\nmapreduce"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("job"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("jvm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numtasks"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v(" \n")])])]),a("h5",{attrs:{id:"并行执行"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#并行执行"}},[t._v("#")]),t._v(" 并行执行：")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\tHive在实现HQL计算运行时，会解析为多个Stage，有时候Stage彼此之间有依赖关系，只能挨个执行，但是在一些别的场景下，很多的Stage之间是没有依赖关系的，例如Union语句，Join语句等等，这些Stage没有依赖关系，但是Hive依旧默认挨个执行每个Stage，这样会导致性能非常差，我们可以通过修改参数，开启并行执行，当多个Stage之间没有依赖关系时，允许多个Stage并行执行，提高性能。")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 开启Stage并行化，默认为false")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SET")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("exec")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parallel"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 指定并行化线程数，默认为8")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SET")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("exec")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parallel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("thread"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("number"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n")])])]),a("p",[a("strong",[t._v("-- 注意：线程数越多，程序运行速度越快，但同样更消耗CPU资源")])]),t._v(" "),a("h5",{attrs:{id:"join-优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#join-优化"}},[t._v("#")]),t._v(" Join 优化：")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\t表的Join是数据分析处理过程中必不可少的操作，Hive同样支持Join的语法，Hive Join的底层还是通过MapReduce来实现的，Hive实现Join时，为了提高MapReduce的性能，提供了多种Join方案来实现，例如适合小表Join大表的Map Join，大表Join大表的Reduce Join，以及大表Join的优化方案Bucket Join等。")])]),t._v(" "),a("h6",{attrs:{id:"map-join"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#map-join"}},[t._v("#")]),t._v(" ==Map Join:==")]),t._v(" "),a("blockquote",[a("ul",[a("li",[a("strong",[t._v("应用场景")])])]),t._v(" "),a("p",[t._v("适合于小表join大表或者小表Join小表")]),t._v(" "),a("ul",[a("li",[a("p",[a("strong",[t._v("原理")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121110059681.png",alt:""}})])])]),t._v(" "),a("p",[t._v("将小的那份数据给每个MapTask的内存都放一份完整的数据，大的数据每个部分都可以与小数据的完整数据进行join")]),t._v(" "),a("p",[t._v("底层不需要经过shuffle，需要占用内存空间存放小的数据文件")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 默认已经开启了Map Join")]),t._v("\nhive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auto"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("convert")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),t._v("\n")])])]),a("p",[a("strong",[t._v("Hive中判断哪张表是小表及限制")])]),t._v(" "),a("ul",[a("li",[a("p",[t._v("LEFT OUTER JOIN的左表必须是大表")])]),t._v(" "),a("li",[a("p",[t._v("RIGHT OUTER JOIN的右表必须是大表")])]),t._v(" "),a("li",[a("p",[t._v("INNER JOIN左表或右表均可以作为大表")])]),t._v(" "),a("li",[a("p",[t._v("FULL OUTER JOIN不能使用MAPJOIN")])]),t._v(" "),a("li",[a("p",[t._v("MAPJOIN支持小表为子查询")])]),t._v(" "),a("li",[a("p",[t._v("使用MAPJOIN时需要引用小表或是子查询时，需要引用别名")])]),t._v(" "),a("li",[a("p",[t._v("在MAPJOIN中，可以使用不等值连接或者使用OR连接多个条件")])]),t._v(" "),a("li",[a("p",[t._v("在MAPJOIN中最多支持指定6张小表，否则报语法错误")])])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 2.0版本开始由以下参数控制")]),t._v("\nhive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auto"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("convert")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("noconditionaltask"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("512000000")]),t._v("\n")])])]),a("h6",{attrs:{id:"reduce-join"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reduce-join"}},[t._v("#")]),t._v(" ==Reduce Join:==")]),t._v(" "),a("blockquote",[a("ul",[a("li",[a("strong",[t._v("应用场景")])])]),t._v(" "),a("p",[t._v("适合于大表Join大表")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("原理")])])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121110600183.png",alt:""}})]),t._v(" "),a("p",[t._v("将两张表的数据在shuffle阶段利用shuffle的分组来将数据按照关联字段进行合并")]),t._v(" "),a("p",[t._v("必须经过shuffle，利用Shuffle过程中的分组来实现关联")]),t._v(" "),a("ul",[a("li",[t._v("使用：")])]),t._v(" "),a("p",[t._v("Hive会自动判断是否满足Map Join，如果不满足Map Join，则自动执行Reduce Join")])]),t._v(" "),a("h6",{attrs:{id:"bucket-join"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bucket-join"}},[t._v("#")]),t._v(" ==Bucket Join:==")]),t._v(" "),a("blockquote",[a("ul",[a("li",[a("strong",[t._v("应用场景")])])]),t._v(" "),a("p",[t._v("适合于大表Join大表")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("原理")])])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121110809984.png",alt:""}})]),t._v(" "),a("p",[t._v("将两张表按照相同的规则将数据划分，根据对应的规则的数据进行join，减少了比较次数，提高了性能")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 开启分桶join")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bucketmapjoin "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- Sort Merge Bucket Join（SMB）：基于有序的数据Join")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 开启分桶SMB join")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bucketmapjoin "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auto"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("convert")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sortmerge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bucketmapjoin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sortedmerge "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auto"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("convert")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sortmerge"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("noconditionaltask"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n")])])]),a("p",[a("strong",[t._v("-- 分桶字段 = Join字段 = 排序字段 ，桶的个数相等或者成倍数")])]),t._v(" "),a("h5",{attrs:{id:"优化器"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#优化器"}},[t._v("#")]),t._v(" 优化器：")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\t在使用Hive的过程中经常会遇到一些特殊的问题，例如当一个程序中如果有一些操作彼此之间有关联性，是可以放在一个MapReduce中实现的，但是Hive会不智能的选择，Hive会使用两个MapReduce来完成这两个操作。")])]),t._v(" "),a("p",[t._v("==示例：==")]),t._v(" "),a("p",[t._v("当我们执行以下SQL语句：")]),t._v(" "),a("p",[a("code",[t._v("select …… from table group by id order by id desc;")])]),t._v(" "),a("p",[t._v("该SQL语句转换为MapReduce时，我们可以有两种方案来实现：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("方案一")])])]),t._v(" "),a("p",[t._v("第一个MapReduce做group by，经过shuffle阶段对id做分组")]),t._v(" "),a("p",[t._v("第二个MapReduce对第一个MapReduce的结果做order by，经过shuffle阶段对id进行排序")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("方案二")])])]),t._v(" "),a("p",[t._v("因为都是对id处理，可以使用一个MapReduce的shuffle既可以做分组也可以排序")]),t._v(" "),a("p",[t._v("在这种场景下，Hive会默认选择用第一种方案来实现，这样会导致性能相对较差，我们可以在Hive中开启关联优化，对有关联关系的操作进行解析时，可以尽量放在同一个MapReduce中实现。")]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("correlation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h5",{attrs:{id:"cbo-优化器引擎"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cbo-优化器引擎"}},[t._v("#")]),t._v(" CBO 优化器引擎：")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\t在使用MySQL或者Hive等工具时，我们经常会遇到一个问题，默认的优化器在底层解析一些聚合统计类的处理的时候，底层解析的方案有时候不是最佳的方案。")]),t._v(" "),a("p",[t._v("​\t\t例如：当前有一张表【共1000条数据】，id构建了索引，id =100值有900条，我们现在的需求是查询所有id = 100的数据，所以SQL语句为：select * from table where id = 100;")]),t._v(" "),a("p",[t._v("​\t\t由于id这一列构建了索引，索引默认的优化器引擎RBO，会选择先从索引中查询id = 100的值所在的位置，再根据索引记录位置去读取对应的数据，但是这并不是最佳的执行方案。有id=100的值有900条，占了总数据的90%，这时候是没有必要检索索引以后再检索数据的，可以直接检索数据返回，这样的效率会更高，更节省资源，这种方式就是CBO优化器引擎会选择的方案。")]),t._v(" "),a("p",[t._v("​\t\t使用Hive时，Hive中也支持RBO与CBO这两种引擎，默认使用的是RBO优化器引擎。")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("RBO")])])]),t._v(" "),a("p",[t._v("rule basic optimise：基于规则的优化器")]),t._v(" "),a("p",[t._v("根据设定好的规则来对程序进行优化")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("CBO")])])]),t._v(" "),a("p",[t._v("cost basic optimise：基于代价的优化器")]),t._v(" "),a("p",[t._v("根据不同场景所需要付出的代价来合适选择优化的方案")]),t._v(" "),a("p",[t._v("对数据的分布的信息【数值出现的次数，条数，分布】来综合判断用哪种处理的方案是最佳方案")]),t._v(" "),a("p",[a("strong",[t._v("很明显CBO引擎更加智能，所以在使用Hive时，我们可以配置底层的优化器引擎为CBO引擎。")])])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cbo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("enable")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("compute")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("query"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("using")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("fetch")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("column")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("fetch")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("partition")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("stats"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("ul",[a("li",[a("strong",[t._v("要求")])])]),t._v(" "),a("p",[t._v("要想使用CBO引擎，必须构建数据的元数据【表行数、列的信息、分区的信息……】")]),t._v(" "),a("p",[t._v("提前获取这些信息，CBO才能基于代价选择合适的处理计划")]),t._v(" "),a("p",[t._v("所以CBO引擎一般搭配analyze分析优化器一起使用")]),t._v(" "),a("h5",{attrs:{id:"analyze分析优化器"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#analyze分析优化器"}},[t._v("#")]),t._v(" Analyze分析优化器:")]),t._v(" "),a("blockquote",[a("ul",[a("li",[t._v("功能")])]),t._v(" "),a("p",[t._v("用于提前运行一个MapReduce程序将表或者分区的信息构建一些元数据 "),a("code",[t._v("表的信息、分区信息、列的信息")]),t._v("，搭配CBO引擎一起使用")])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 构建分区信息元数据")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ANALYZE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" tablename\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("PARTITION")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("partcol1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("val1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" partcol2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("val2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMPUTE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("STATISTICS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("noscan"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 构建列的元数据")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ANALYZE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" tablename\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("PARTITION")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("partcol1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("val1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" partcol2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("val2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COMPUTE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("STATISTICS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FOR")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("COLUMNS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("columns")]),t._v(" name1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("columns")]),t._v(" name2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("noscan"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 查看元数据")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DESC")]),t._v(" FORMATTED "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tablename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("columnname"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h5",{attrs:{id:"谓词下推-pdd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#谓词下推-pdd"}},[t._v("#")]),t._v(" 谓词下推 PDD :")]),t._v(" "),a("blockquote",[a("ul",[a("li",[t._v("基本思想：")])]),t._v(" "),a("p",[t._v("谓词下推 Predicate Pushdown（PPD）的思想简单点说就是在不影响最终结果的情况下，尽量将过滤条件提前执行。谓词下推后，过滤条件在map端执行，减少了map端的输出，降低了数据在集群上传输的量，降低了Reduce端的数据负载，节约了集群的资源，也提升了任务的性能。")]),t._v(" "),a("ul",[a("li",[t._v("开启参数：("),a("em",[t._v("默认自动开启谓词下推")]),t._v(")")])]),t._v(" "),a("p",[a("code",[t._v("hive.optimize.ppd=**true**;")])]),t._v(" "),a("ul",[a("li",[t._v("基本规则：")])]),t._v(" "),a("p",[t._v("不同Join场景下的Where谓词下推测试")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121112210279.png",alt:""}})]),t._v(" "),a("ul",[a("li",[t._v("试验结论：")])]),t._v(" "),a("img",{staticStyle:{zoom:"150%"},attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121112400823.png",alt:"image-20211121112400823"}}),t._v(" "),a("p",[t._v("Inner Join和Full outer Join，条件写在on后面，还是where后面，性能上面没有区别")]),t._v(" "),a("p",[t._v("Left outer Join时 ，右侧的表写在on后面，左侧的表写在where后面，性能上有提高")]),t._v(" "),a("p",[t._v("Right outer Join时，左侧的表写在on后面、右侧的表写在where后面，性能上有提高")]),t._v(" "),a("p",[t._v("如果SQL语句中出现不确定结果的函数，也无法实现下推")])]),t._v(" "),a("h3",{attrs:{id:"数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据倾斜"}},[t._v("#")]),t._v(" 数据倾斜：")]),t._v(" "),a("blockquote",[a("ul",[a("li",[a("p",[t._v("现象：")]),t._v(" "),a("p",[t._v("分布式计算中最常见的，最容易遇到的问题就是数据倾斜，数据倾斜的现象是，当我们提交运行一个程序时，我们通过监控发现，这个程序的大多数的Task都已经运行结束了，只有某一个Task一直在运行，迟迟不能结束，导致整体的进度卡在99%或者100%，这时候我们就可以判定程序出现了数据倾斜的问题。")])]),t._v(" "),a("li",[a("p",[t._v("原因：")]),t._v(" "),a("p",[t._v("第一：数据本身就是倾斜的，数据中某种数据出现的次数过多。")]),t._v(" "),a("p",[t._v("第二：分区规则导致这些相同的数据都分配给了同一个Task，导致这个Task拿到了大量的数据，而其他Task拿到的数据比较少，所以运行起来相比较于其他Task就比较慢一些。")]),t._v(" "),a("p",[a("strong",[t._v("产生数据倾斜的根本原因在于分区规则。")])])])])]),t._v(" "),a("h6",{attrs:{id:"group-by的数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#group-by的数据倾斜"}},[t._v("#")]),t._v(" ==group By的数据倾斜:==")]),t._v(" "),a("p",[t._v("​\t\t当程序中出现group by或者count（distinct）等分组聚合的场景时，如果数据本身是倾斜的根据MapReduce的Hash分区规则，肯定会出现数据倾斜的现象。根本原因是因为分区规则导致的，所以我们可以通过以下几种方案来解决group by导致的数据倾斜的问题。")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("方案一：开启Map端聚合")])])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 开启Map端聚合：Combiner")]),t._v("\nhive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aggr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[a("strong",[t._v("通过减少Reduce的输入量，避免每个Task数据差异过大导致数据倾斜")])]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("方案二：实现随机分区")])])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- SQL中避免数据倾斜，构建随机分区")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" distribute "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" rand"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("ol",[a("li",[a("p",[t._v("distribute by用于指定底层的MapReduce按照哪个字段作为Key实现分区、分组等")])]),t._v(" "),a("li",[a("p",[t._v("默认由Hive自己选择，我们可以通过distribute by自己指定，通过rank函数随机值实现随机分区，避免数据倾斜")])])]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("方案三：自动构建随机分区并自动聚合")])])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 开启随机分区，走两个MapReduce ")]),t._v("\nhive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("skewindata"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("ol",[a("li",[a("p",[t._v("开启该参数以后，当前程序会自动通过两个MapReduce来运行")])]),t._v(" "),a("li",[a("p",[t._v("第一个MapReduce自动进行随机分区，然后实现聚合")])]),t._v(" "),a("li",[a("p",[t._v("第二个MapReduce将聚合的结果再按照业务进行处理，得到结果")])])]),t._v(" "),a("h6",{attrs:{id:"join-的数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#join-的数据倾斜"}},[t._v("#")]),t._v(" ==Join 的数据倾斜:==")]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\t实际业务需求中往往需要构建两张表的Join实现，如果两张表比较大，无法实现Map Join，只能走Reduce Join，那么当关联字段中某一种值过多的时候依旧会导致数据倾斜的问题，面对Join产生的数据倾斜，我们核心的思想是尽量避免Reduce Join的产生，优先使用Map Join来实现，但往往很多的Join场景不满足Map Join的需求。")])]),t._v(" "),a("p",[t._v("那么我们可以以下几种方案来解决Join产生的数据倾斜问题：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("方案一：提前过滤，将大数据变成小数据，实现Map Join:")])])]),t._v(" "),a("p",[t._v("实现两张表的Join时，我们要尽量考虑是否可以使用Map Join来实现Join过程**。**有些场景下看起来是大表Join大表，但是我们可以通过转换将大表Join大表变成大表Join小表，来实现Map Join。")]),t._v(" "),a("p",[t._v("==例子：==现在有两张表订单表A与用户表B，需要实现查询今天所有订单的用户信息。")]),t._v(" "),a("p",[t._v("​\t\tA表：今天的订单，1000万条，字段：orderId,userId,produceId,price等")]),t._v(" "),a("p",[t._v("​\t\tB表：用户信息表，100万条，字段：userid,username,age,phone等")]),t._v(" "),a("p",[t._v("​\t\t关联字段为 userid")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 将下了订单的用户的数据过滤出来，再Join")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 获取所有下订单的用户信息")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v("\n    b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 获取所有下订单的userid")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("distinct")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userid "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" A a "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" c "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" B b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userid "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userid "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" d \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v("\n  A a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userid "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("userid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("ol",[a("li",[a("p",[t._v("100万个用户中，在今天下订单的人数可能只有一小部分，大量数据是不会Join成功的")])]),t._v(" "),a("li",[a("p",[t._v("可以提前将订单表中的userid去重，获取所有下订单的用户id")])]),t._v(" "),a("li",[a("p",[t._v("再使用所有下订单的用户id关联用户表，得到所有下订单的用户的信息")])]),t._v(" "),a("li",[a("p",[t._v("最后再使用下订单的用户信息关联订单表")])]),t._v(" "),a("li",[a("p",[t._v("通过多次Map Join来代替Reduce Join，性能更好也可以避免数据倾斜")])])]),t._v(" "),a("p",[t._v("​")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("方案二：使用Bucket Join")])])]),t._v(" "),a("ol",[a("li",[a("p",[t._v("如果使用方案一来避免Reduce Join ，有些场景下依旧无法满足，例如过滤后的数据依旧是一张大表，那么最后的Join依旧是一个Reduce Join")])]),t._v(" "),a("li",[a("p",[t._v("这种场景下，我们可以将两张表的数据构建为桶表，实现Bucket Map Join，避免数据倾斜")])])]),t._v(" "),a("ul",[a("li",[a("p",[a("strong",[t._v("方案三：使用Skew Join")])]),t._v(" "),a("blockquote",[a("p",[t._v("​\t\tSkew Join是Hive中一种专门为了避免数据倾斜而设计的特殊的Join过程，这种Join的原理是将Map Join和Reduce Join进行合并，如果某个值出现了数据倾斜，就会将产生数据倾斜的数据单独使用Map Join来实现，其他没有产生数据倾斜的数据由Reduce Join来实现，这样就避免了Reduce Join中产生数据倾斜的问题，最终将Map Join的结果和Reduce Join的结果进行Union合并。")])])])]),t._v(" "),a("p",[t._v("==代码：==")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 开启运行过程中skewjoin")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("skewjoin"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 如果这个key的出现的次数超过这个范围")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("skewjoin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("key")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 在编译时判断是否会产生数据倾斜")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("skewjoin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compiletime"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 不合并，提升性能")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("optimize")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("union")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("remove"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("-- 如果Hive的底层走的是MapReduce，必须开启这个属性，才能实现不合并")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" mapreduce"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fileinputformat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("recursive"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("==原理：==")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/ChoiNgai/ImageServer/img/image-20211121114042570.png",alt:""}})])])}),[],!1,null,null,null);s.default=n.exports}}]);